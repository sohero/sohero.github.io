<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Deterministic Policy Gradient Algorithms | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Deterministic Policy Gradient Algorithms</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2017-05-02</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/paper/">paper</a>/<a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><h2 id="why-deterministic-policy-gradients">Why Deterministic Policy Gradients?</h2>
<p><strong>Goal</strong>: continuous action RL in high dimensions</p>
<ul>
<li>Deterministic policy gradient can be estimated more efficiently
<ul>
<li>Especially in high-dimensional continuous action spaces</li>
</ul></li>
<li>Deterministic policy gradient is simple and natural
<ul>
<li>Update policy in direction of action-value gradient</li>
</ul></li>
<li>Hardware may only have deterministic controller
<ul>
<li>e.g. wind turbine sets blade pitch as function of wind speed</li>
</ul></li>
</ul>
<h2 id="stochastic-policy-gradient-theorem">Stochastic Policy Gradient Theorem</h2>
<p>Consider objective function <span class="math inline">\(J(\theta)\)</span> for state density <span class="math inline">\(\rho^\pi(s)\)</span> <span class="math display">\[
J(\theta)=\Bbb E\left[\int_a\pi_\theta(s,a) R(s,a) da\right]
\]</span> For a stochastic policy <span class="math inline">\(\pi_\theta(s,a)​\)</span>, the policy gradient is <span class="math display">\[
\begin{align}
\nabla_\theta J(\theta) &amp;=\Bbb E_s\left[\int_a\nabla_\theta\pi_\theta(s,a)Q^\pi(s,a) da\right] \\
&amp;= \Bbb E_s\left[\int_a\pi_\theta(s,a)\frac{\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)}Q^\pi(s,a)\right] \\
&amp;=\Bbb E_{s,a}\left[\nabla_\theta log\pi_\theta(s,a)Q^\pi(s,a)\right]
\end{align}
\]</span></p>
<p>Leading to the following stochastic actor-critic algorithm <span class="math display">\[
\Delta\theta=\alpha\nabla_\theta log\pi_\theta(s,a)Q^w(s,a)
\]</span></p>
<ul>
<li><span class="math inline">\(\pi_\theta(s,a)\)</span> is the actor</li>
<li><span class="math inline">\(Q^w(s,a)\approx Q^\pi(s,a)\)</span> is the critic</li>
</ul>
<h3 id="illustration-of-stochastic-policy">Illustration of Stochastic Policy</h3>
<figure>
<img src="/images/2017-05-01_230117.png">
</figure>
<h3 id="illustration-of-stochastic-policy-gradient">Illustration of Stochastic Policy Gradient</h3>
<figure>
<img src="/images/2017-05-01_225823.png">
</figure>
<h3 id="illustration-of-deterministic-policy-gradient">Illustration of Deterministic Policy Gradient</h3>
<figure>
<img src="/images/2017-05-01_232422.png">
</figure>
<h2 id="deterministic-policy-gradient-theorem">Deterministic Policy Gradient Theorem</h2>
<p>For a deterministic policy <span class="math inline">\(a=\mu_\theta(s)\)</span>, the policy gradient is <span class="math display">\[
\begin{align}
\nabla_\theta J(\theta) &amp;= \Bbb E_s\left[ \nabla_\theta Q^\mu(s,a)\mid_{a=\mu_\theta(s)} \right] \\
&amp; = \Bbb E_s\left[\nabla_\theta \mu_\theta(s)\nabla_a Q^\mu(s,a)\mid_{a=\mu_\theta(s)}\right] \\
\end{align}
\]</span> Leading to the following deterministic policy gradient update <span class="math display">\[
\Delta\theta=\nabla_\theta\mu_\theta(s)\nabla_a Q^\mu(s,a)\mid_{a=\mu_\theta(s)}
\]</span> <strong>Update policy in the direction that most improves <span class="math inline">\(Q\)</span></strong></p>
<h2 id="off-policy-deterministic-actor-critic-opdac">Off-Policy Deterministic Actor-Critic (OPDAC)</h2>
<figure>
<img src="/images/2017-05-02_002930.png">
</figure>
<h2 id="compatible-function-approximation">Compatible Function Approximation</h2>
<figure>
<img src="/images/2017-05-02_002943.png">
</figure>
<h2 id="compatible-off-policy-deterministic-actor-critic-copdac">Compatible Off-Policy Deterministic Actor-Critic (COPDAC)</h2>
<figure>
<img src="/images/2017-05-02_003000.png">
</figure>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2017/05/02/Deterministic Policy Gradient Algorithms/" data-title="Deterministic Policy Gradient Algorithms" data-url="https://www.sgq.mobi/2017/05/02/Deterministic Policy Gradient Algorithms/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>