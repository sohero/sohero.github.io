<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Summary of An overview of gradient descent optimization algorithms | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Summary of An overview of gradient descent optimization algorithms</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2017-05-02</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/deep-learning/">deep learning</a>/<a class="post-tag-link" href="/tags/machine-learning/">machine learning</a>/<a class="post-tag-link" href="/tags/paper/">paper</a></div></div></div><article><div class="container post"><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="external">Link</a></p>
<h2 id="Optimization-is-Hard"><a href="#Optimization-is-Hard" class="headerlink" title="Optimization is Hard"></a>Optimization is Hard</h2><p><img src="/images/g1.jpg" alt=""></p>
<p><img src="/images/g2.jpg" alt=""></p>
<p><img src="/images/g3.jpg" alt=""></p>
<h2 id="Gradient-descent-variants"><a href="#Gradient-descent-variants" class="headerlink" title="Gradient descent variants"></a>Gradient descent variants</h2><h3 id="Batch-gradient-descent"><a href="#Batch-gradient-descent" class="headerlink" title="Batch gradient descent"></a>Batch gradient descent</h3><p>Batch gradient descent is guaranteed to converge to the global minimum for convex error surfaces and to a local minimum for non-convex surfaces.</p>
<h3 id="Stochastic-gradient-descent"><a href="#Stochastic-gradient-descent" class="headerlink" title="Stochastic gradient descent"></a>Stochastic gradient descent</h3><p>SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily as in Image 1.</p>
<p><img src="/images/sgd_fluctuation.png" alt=""></p>
<p>SGD’s fluctuation, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep <strong>overshooting</strong>. However, it has been shown that when we <strong>slowly decrease the learning rate</strong>, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.</p>
<p>Note that we shuffle the training data at every epoch.</p>
<h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h3><p>Reduces the variance of the parameter updates, which can lead to more stable convergence. Common mini-batch sizes range between 50 and 256, but can vary for different applications.</p>
<h2 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h2><p>Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:</p>
<ul>
<li>Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.</li>
<li>Learning rate schedules try to adjust the learning rate during training by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset’s characteristics.</li>
<li>Additionally, the same learning rate applies to all parameter updates. <strong>If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.</strong></li>
<li>Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.</li>
</ul>
<h2 id="Gradient-descent-optimization-algorithms"><a href="#Gradient-descent-optimization-algorithms" class="headerlink" title="Gradient descent optimization algorithms"></a>Gradient descent optimization algorithms</h2><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum .</p>
<p>Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction $\gamma$ of the update vector of the past time step to the current update vector:<br>$$<br>v<em>t=\gamma v</em>{t-1}+\eta\nabla_\theta J(\theta) \<br>\theta = \theta - v_t<br>$$</p>
<p>The momentum term $\gamma$ is usually set to $0.9$ or a similar value.</p>
<p><strong>The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions.</strong> As a result, we gain faster convergence and reduced oscillation.</p>
<h3 id="Nesterov-accelerated-gradient"><a href="#Nesterov-accelerated-gradient" class="headerlink" title="Nesterov accelerated gradient"></a>Nesterov accelerated gradient</h3><p>However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.</p>
<p>Nesterov accelerated gradient (NAG) is a way to give our momentum term this kind of prescience.<br>$$<br>v<em>t = \gamma v</em>{t-1}+\eta\nabla<em>\theta J(\theta-\gamma v</em>{t-1}) \<br>\theta = \theta-v_t<br>$$<br>Again, we set the momentum term $\gamma$ to a value of around 0.9. While Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient, NAG first makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update.</p>
<p>Now that we are able to adapt our updates to the slope of our error function and speed up SGD in turn, we would also like to adapt our updates to each individual parameter to perform larger or smaller updates depending on their importance.</p>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p><strong>It adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequent parameters. For this reason, it is well-suited for dealing with sparse data.</strong><br>$$<br>\theta_{t+1}=\theta-\frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t<br>$$</p>
<p>$G_t\in\Bbb R^{d\times d}$ is a diagonal matrix where each diagonal element $i,i$ is the sum of the squares of the gradients w.r.t. $\theta_i$ up to time step $t$, where $\epsilon$ is a smoothing term that avoids division by zero (usually on the order of $1e-8$). Interestingly, without the square root operation, the algorithm performs much worse. $g_t$ to be the gradient of the objective function at time step $t$.</p>
<p>One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of $0.01$ and leave it at that.</p>
<p>Adagrad’s main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and <strong>eventually become infinitesimally small</strong>, at which point the algorithm is <strong>no longer able to acquire additional knowledge</strong>. The following algorithms aim to resolve this flaw.</p>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p>Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, Adadelta restricts the window of accumulated past gradients to some fixed size $w$.<br>$$<br>E[g^2]<em>t = \gamma E[g^2]</em>{t-1} + (1-\gamma)g^2_t<br>$$<br>We set $\gamma$ to a similar value as the momentum term, around $0.9$.</p>
<p>We now simply replace the diagonal matrix $G_t$ with the decaying average over past squared gradients $E[g^2]_t$:<br>$$<br>\Delta \theta_t = - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_t<br>$$<br>As the denominator is just the root mean squared (RMS) error criterion of the gradient, we can replace it with the criterion short-hand:<br>$$<br>\Delta \theta_t = - \frac{\eta}{RMS[g]_t} g_t<br>$$<br>The authors note that the units in this update (as well as in SGD, Momentum, or Adagrad) do not match, i.e. the update should have the same hypothetical units as the parameter. To realize this, they first define another exponentially decaying average, this time not of squared gradients but of squared parameter updates:<br>$$<br>E[\Delta\theta^2]<em>t = \gamma E[\Delta\theta^2]</em>{t-1} + (1-\gamma)\Delta\theta^2_t<br>$$<br>The root mean squared error of parameter updates is thus:<br>$$<br>RMS[\Delta\theta]_t = \sqrt{E[\Delta\theta^2]_t + \epsilon}<br>$$<br>Since $RMS[\Delta\theta]<em>t$ is unknown, we approximate it with the RMS of parameter updates until the previous time step. Replacing the learning rate $\eta$ in the previous update rule with $RMS[\Delta\theta]</em>{t-1}$ finally yields the Adadelta update rule:<br>$$<br>\Delta\theta<em>t=-\frac{RMS[\Delta\theta]</em>{t-1}}{RMS[g]_t}g<em>t \<br>\theta</em>{t+1} = \theta_t+\Delta\theta_t<br>$$<br>With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule.</p>
<h3 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h3><p>RMSprop and Adadelta have both been developed independently around the same time stemming from the need to resolve Adagrad’s radically diminishing learning rates. RMSprop in fact is identical to the first update vector of Adadelta that we derived above:<br>$$<br>E[g^2]<em>t = 0.9E[g^2]</em>{t-1}+0.1g^2<em>t \<br>\theta</em>{t+1}=\theta_t-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}g_t<br>$$<br>RMSprop as well divides the learning rate by an exponentially decaying average of squared gradients. Hinton suggests $\gamma$ to be set to $0.9$, while a good default value for the learning rate $\eta$ is $0.001$.</p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $v_t$ like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum:<br>$$<br>m_t=\beta<em>1 m</em>{t-1}+(1-\beta_1)g_t \<br>v_t = \beta<em>2 v</em>{t-1}+(1-\beta_2)g^2_t<br>$$<br>$m_t$ and $v_t$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_t$ and $v_t$  are initialized as vectors of 0’s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. $\beta_1$ and $\beta_2$ are close to 1).</p>
<p>They counteract these biases by computing bias-corrected first and second moment estimates:<br>$$<br>\hat m_t={m_t \over {1-\beta^t_1}} \<br>\hat v_t = {v_t \over {1-\beta^t<em>2}}<br>$$<br>They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule:<br>$$<br>\theta</em>{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat v_t}+\epsilon}\hat m_t<br>$$<br>The authors propose default values of 0.9 for $\beta_1$, 0.999 for $\beta_2$, and $10^{−8}$ for $\epsilon$. They show empirically that Adam works well in practice and compares favorably to other adaptive learning-method algorithms.</p>
<h3 id="Visualization-of-algorithms"><a href="#Visualization-of-algorithms" class="headerlink" title="Visualization of algorithms"></a>Visualization of algorithms</h3><video src="/images/g4.mp4" autoplay="autoplay" loop="loop" controls="controls"><br>your browser does not support the video tag<br></video><br><br><br><video src="/images/g5.mp4" autoplay="autoplay" loop="loop" controls="controls"><br>your browser does not support the video tag<br></video>

<h3 id="Which-optimizer-to-use"><a href="#Which-optimizer-to-use" class="headerlink" title="Which optimizer to use?"></a>Which optimizer to use?</h3><p>So, which optimizer should you now use? If your input data is sparse, then you likely achieve the best results using one of the adaptive learning-rate methods. An additional benefit is that you won’t need to tune the learning rate but likely achieve the best results with the default value.</p>
<p>In summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#fn:15" target="_blank" rel="external">15</a>] show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.</p>
<p>Interestingly, many recent papers use vanilla SGD without momentum and a simple learning rate annealing schedule. As has been shown, SGD usually achieves to find a minimum, but it might take significantly longer than with some of the optimizers, is much more reliant on a robust initialization and annealing schedule, and may get stuck in saddle points rather than local minima. Consequently, if you care about fast convergence and train a deep or complex neural network, you should choose one of the adaptive learning rate methods.</p>
<h2 id="Additional-strategies-for-optimizing-SGD"><a href="#Additional-strategies-for-optimizing-SGD" class="headerlink" title="Additional strategies for optimizing SGD"></a>Additional strategies for optimizing SGD</h2><h3 id="Shuffling-and-Curriculum-Learning"><a href="#Shuffling-and-Curriculum-Learning" class="headerlink" title="Shuffling and Curriculum Learning"></a>Shuffling and Curriculum Learning</h3><p>Generally, we want to avoid providing the training examples in a meaningful order to our model as this may bias the optimization algorithm. Consequently, it is often a good idea to shuffle the training data after every epoch.</p>
<p>On the other hand, for some cases where we aim to solve progressively harder problems, supplying the training examples in a meaningful order may actually lead to improved performance and better convergence. The method for establishing this meaningful order is called Curriculum Learning [<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#fn:16" target="_blank" rel="external">16</a>].</p>
<p>Zaremba and Sutskever [<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#fn:17" target="_blank" rel="external">17</a>] were only able to train LSTMs to evaluate simple programs using Curriculum Learning and show that a combined or mixed strategy is better than the naive one, which sorts examples by increasing difficulty.</p>
<h3 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h3><p>To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and we update parameters to different extents, we lose this normalization, which slows down training and amplifies changes as the network becomes deeper.</p>
<p>Batch normalization [<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#fn:18" target="_blank" rel="external">18</a>] reestablishes these normalizations for every mini-batch and changes are back-propagated through the operation as well. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization additionally acts as a regularizer, reducing (and sometimes even eliminating) the need for Dropout.</p>
<h3 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h3><p>You should thus always monitor error on a validation set during training and stop (with some patience) if your validation error does not improve enough.</p>
<h3 id="Gradient-noise"><a href="#Gradient-noise" class="headerlink" title="Gradient noise"></a>Gradient noise</h3><p>Neelakantan et al. [<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html#fn:21" target="_blank" rel="external">21</a>] add noise that follows a Gaussian distribution $N(0,\sigma^2<em>t)$ to each gradient update:<br>$$<br>g</em>{t,i}=g_{t,i}+N(0,\sigma^2_t)<br>$$<br>They anneal the variance according to the following schedule:<br>$$<br>\sigma^2_t={\eta \over {(1+t)^\gamma}}<br>$$<br>They show that adding this noise makes networks more robust to poor initialization and helps training particularly deep and complex networks. They suspect that the added noise gives the model more chances to escape and find new local minima, which are more frequent for deeper models.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>