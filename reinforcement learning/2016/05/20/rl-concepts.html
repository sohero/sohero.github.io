<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>RL Concepts</title>
	
	<meta name="author" content="sohero">
	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css">
	<link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/css/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">
	<link rel="shortcut icon" href="favicon.ico">
	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS_HTML"></script>
    
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/sohero">
				<i class="fa fa-github"></i>
			</a>
			
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
			<!--
            <input type="text" class="st-default-search-input navbar-toggle" style="width:100px;">
            -->
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-2 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/assets/images/bl.png" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/"></a>
    </h3>
</header>


<div id="bio" class="text-center">
	Qiang Brother Notes.
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/sohero">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
        <li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-10 col-sm-offset-2">
		<div class="page-header">
  <h1>RL Concepts </h1>
</div>
	
<article>

	<div class="col-sm-12">
	 <span class="post-date">
	 	2016-05-20 
	 </span>
	 <div class="article-outline"></div>
	  <div class="article_body">
	  <p><strong>backup</strong>: We <em>back up</em> the value of the state after each greedy move to the state before the move.</p>
<p><strong>experience</strong>: sample sequences of <em>states</em>, <em>actions</em>, and <em>rewards</em> from actual or simulated interaction with an environment.</p>
<p><strong>nonstationary</strong>: The return after taking an action in one state depends on the actions taken in later states in the same episode. Because all action selections are undergoing learning, the problem becomes nonstationary from the point view of the earlier state.</p>
<p><strong>prediction problem</strong>: The computation of <span class="math inline">\(v_\pi\)</span> and <span class="math inline">\(q_\pi\)</span> for a fixed solution policy <span class="math inline">\(\pi\)</span>.</p>
<p><strong>bootstrapping</strong>: All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea <em>bootstrapping</em>. <code>Updating an estimate from an estimate, a guess from a guess.</code></p>
<p><strong>exploring starts</strong>: For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes <em>start in a state-action pair</em>, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of <em>exploring starts</em>.</p>
<p><strong><span class="math inline">\(\varepsilon\)</span>-greedy</strong>: most of the time they choose an action that maximal estimated action value, but with probability <span class="math inline">\(\varepsilon\)</span> they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, <span class="math inline">\({\epsilon \over |\mathcal A(\mathcal s)|}\)</span>, and the remaining bulk of the probability, <span class="math inline">\(1-\varepsilon +{\epsilon \over |\mathcal A(\mathcal s)|}\)</span>, is given to the greedy action. The <span class="math inline">\(\varepsilon\)</span>-greedy policies are examples of <span class="math inline">\(\varepsilon\)</span>-soft policies, defined as policies for which <span class="math inline">\(\pi(a|s)\ge {\epsilon \over |\mathcal A(\mathcal s)|}\)</span> for all states and actions, for some <span class="math inline">\(\varepsilon &gt;0\)</span>. Among <span class="math inline">\(\varepsilon\)</span>-soft policies, $-greedy policies are in some sense those that are closest to greedy.</p>
<p><strong>target policy</strong>: The policy being learned about is called the <em>target policy</em>.</p>
<p><strong>behavior policy</strong>: The policy used to generate behavior is called the <em>behavior policy</em>.</p>
<p><strong>off-policy learning</strong>: Off-policy learning is learning about the value of <code>target policy</code> other than the <code>behavior policy</code> being used to generate the trajectory.</p>
<p><strong>on-policy learning</strong>: on-policy learning is when the two policies are the same.</p>
<blockquote>
<p>On-policy methods typically <code>perform better</code> than off-policy methods, but <code>find poorer policies</code>.</p>
</blockquote>
<p><strong>importance sampling</strong>: a general technique for estimating expected values under one distribution given samples from another.</p>
<p><strong>importance-sampling ratio</strong>: We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the <em>importance-sampling ratio</em>. <span class="math display">\[
\rho_t^T=\frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}\mu(A_k|S_k)p(S_{k+1}|S_k,A_k)}
=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}
\]</span></p>
<p><strong>ordinary importance sampling</strong>: Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy <span class="math inline">\(\mu\)</span> to estimate <span class="math inline">\(v_\pi(\mathcal{s})\)</span>. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time <span class="math inline">\(t=101\)</span>. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state <span class="math inline">\(\mathcal{s}\)</span> is visited, denoted <span class="math inline">\(\mathcal{T}(\mathcal{s})\)</span>. This is for an every-visit method; for a first-visit method, <span class="math inline">\(\mathcal{T}(\mathcal{s})\)</span> would only include time steps that were first visits to <span class="math inline">\(\mathcal{s}\)</span> within their episodes. Also, let <span class="math inline">\(T(t)\)</span> denote the first time of termination following time <span class="math inline">\(t\)</span>, and <span class="math inline">\(G_t\)</span> denote the return after <span class="math inline">\(t\)</span> up through <span class="math inline">\(T(t)\)</span>. Then <span class="math inline">\(\{G_t\}_{t\in{\mathcal{T}(\mathcal{s})}}\)</span> are the returns that pertain to state <span class="math inline">\(\mathcal{s}\)</span>, and <span class="math inline">\(\{\rho_t^{T(t)}\}_{t\in{\mathcal{T}(\mathcal{s})}}\)</span> are the corresponding importance-sampling ratios. To estimate <span class="math inline">\(v_\pi(\mathcal{s})\)</span>, we simply scale the returns by the ratios and average the results: <span class="math display">\[
V(\mathcal{s})=\frac{\sum_{t\in{\mathcal{T}(\mathcal{s})}}\rho_t^{T(t)}G_t}{|\mathcal{T}(\mathcal{s})|}
\]</span> When importance sampling is done as a simple average in this way it is called <em>ordinary importance sampling</em>.</p>
<p><strong>weighted importance sampling</strong>: <span class="math display">\[
V(\mathcal{s})=\frac{\sum_{t\in\mathcal{T}(\mathcal{s})} \rho_t^{T(t)}G_t}{\sum_{t\in\mathcal{T}(\mathcal{s})}\rho_t^{T(t)}}
\]</span></p>
<p><strong>Score Function</strong>: The score function <span class="math inline">\(u(\theta)\)</span> is the partial derivative of the log-likelihood function <span class="math inline">\(F(\theta)=\text{ln}L(\theta)\)</span>, where <span class="math inline">\(L(\theta)\)</span> is the standard likelihood function. <span class="math display">\[
u(\theta)=\frac{\partial}{\partial\theta}F(\theta)
\]</span> Using formulatino of <span class="math inline">\(u\)</span>, one can easily compute various statistical measurements associated with <span class="math inline">\(u\)</span>. For example, the mean <span class="math inline">\(E(u(\theta))\)</span> can be shown to equal zero while the variance is precisely the Fisher information matrix.<a href="http://mathworld.wolfram.com/ScoreFunction.html">[Link]</a></p>

	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="/categories.html#reinforcement learning-ref">
					reinforcement learning <span>(11)</span>
					
				</a></li>
			
		  
		</ul>
		  

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#reinforcement learning-ref">
					reinforcement learning <span>(11)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-12">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=RL Concepts"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/reinforcement%20learning/2016/05/19/temporal-difference-learning.html" title="Temporal-Difference Learning">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="/reinforcement%20learning/2016/06/11/model-free-control.html" title="Model free Control">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>




		<footer>
			<hr/>
			<p>
				&copy; 2016 <a href="http://guoqiang.gq">sohero</a>.
                <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258527085'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258527085%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
			</p>
		</footer>
	</div>

	<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
	<script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



