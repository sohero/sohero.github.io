<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Dynamic Programming</title>
	
	<meta name="author" content="sohero">
	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css">
	<link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/css/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">
	<link rel="shortcut icon" href="favicon.ico">
	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS_HTML"></script>
    
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/sohero">
				<i class="fa fa-github"></i>
			</a>
			
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
			<!--
            <input type="text" class="st-default-search-input navbar-toggle" style="width:100px;">
            -->
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-2 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/assets/images/bl.png" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/"></a>
    </h3>
</header>


<div id="bio" class="text-center">
	Qiang Brother Notes.
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/sohero">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
        <li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-10 col-sm-offset-2">
		<div class="page-header">
  <h1>Dynamic Programming </h1>
</div>
	
<article>

	<div class="col-sm-12">
	 <span class="post-date">
	 	2016-05-17 
	 </span>
	 <div class="article-outline"></div>
	  <div class="article_body">
	  <p>A method for solving complex problems by breaking them down into subproblems:</p>
<ul>
<li>Slove the subproblems</li>
<li>Combine solutions to subproblems</li>
</ul>
<p>Dynamic programming assumes full knowledge of the MDP. It is used for <em>planning</em> in an MDP.</p>
<p><strong>For prediction:</strong></p>
<p>Input: MDP <span class="math inline">\(\left&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&gt;\)</span> and policy <span class="math inline">\(\pi\)</span> or MRP <span class="math inline">\(\left&lt;\mathcal S,\mathcal P^\pi,\mathcal R^\pi,\gamma \right&gt;\)</span></p>
<p>Output: value function <span class="math inline">\(v_\pi\)</span></p>
<p><strong>For control:</strong></p>
<p>Input: MDP <span class="math inline">\(\left&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&gt;\)</span></p>
<p>Output: optimal value function <span class="math inline">\(v_*\)</span> and optimal policy <span class="math inline">\(\pi_*\)</span></p>
<h2 id="iterative-policy-evaluation">Iterative Policy Evaluation</h2>
<p><strong>Problem</strong>: evaluate a given policy <span class="math inline">\(\pi\)</span></p>
<p><strong>Solution</strong>: iterative application of Bellman expectation backup <span class="math display">\[
v_1\to v_2 \to ... \to v_\pi
\]</span> Using <em>synchronous</em> backups,</p>
<ul>
<li>At each iteration <span class="math inline">\(k+1\)</span></li>
<li>For all state <span class="math inline">\(\mathcal s \in \mathcal S\)</span></li>
<li>Update <span class="math inline">\(v_{k+1}(\mathcal s)\)</span> from <span class="math inline">\(v_k(\mathcal s&#39;)\)</span> where <span class="math inline">\(\mathcal s&#39;\)</span> is a successor state of <span class="math inline">\(\mathcal s\)</span></li>
</ul>
<p><span class="math display">\[
v_{k+1}(\mathcal s)=\sum_{a\in\mathcal A} \pi(a|\mathcal s)\left(\mathcal R_{\mathcal s}^a+\gamma\sum_{\mathcal s&#39;\in\mathcal S}\mathcal P^a_{\mathcal s\mathcal s&#39;}v_k(\mathcal s&#39;)\right)
\]</span> <span class="math display">\[
\mathbf{v^{k+1}}=\mathbf{\mathcal R^\pi} + \gamma\mathbf{\mathcal P^\pi v^k}
\]</span></p>
<h2 id="how-to-improve-a-policy">How to Improve a Policy</h2>
<p>Given a policy <span class="math inline">\(\pi\)</span>, <strong>evaluate</strong> the policy <span class="math inline">\(\pi\)</span>: <span class="math display">\[
v_\pi(s)=\Bbb E[R_{t+1}+\gamma R_{t+2}+...|S_t=s]
\]</span> <strong>Improve</strong> the policy by acting greedily with respect to <span class="math inline">\(v_\pi\)</span>: <span class="math display">\[
\pi &#39;=greedy(v_\pi)
\]</span></p>
<h2 id="policy-iteration">Policy Iteration</h2>
<figure>
<img src="/assets/images/41.png" alt="" />
</figure>
<h2 id="principle-of-optimality">Principle of Optimality</h2>
<p>Any optimal policy can be subdivided into two components:</p>
<ul>
<li>An optimal first Action <span class="math inline">\(A_*\)</span></li>
<li>Followed by an optimal policy from successor state <span class="math inline">\(\mathcal{S&#39;}\)</span></li>
</ul>
<p>A policy <span class="math inline">\(\pi(a|s)\)</span> achieves the optimal value from state <span class="math inline">\(s\)</span>, <span class="math inline">\(v_\pi(s)=v_*(s)\)</span>, if and only if <strong>For any state <span class="math inline">\(s&#39;\)</span> reachable from <span class="math inline">\(s\)</span>, <span class="math inline">\(\pi\)</span> achieves the optimal value from state <span class="math inline">\(s&#39;\)</span>, <span class="math inline">\(v_\pi(s&#39;)=v_*(s&#39;)\)</span></strong></p>
<h2 id="deterministic-value-iteration">Deterministic Value Iteration</h2>
<p>If we know the solution to subproblems <span class="math inline">\(v_*(s&#39;)\)</span>. Then solution <span class="math inline">\(v_*(s)\)</span> can be found by one-step lookahead: <span class="math display">\[
v_*(s)\leftarrow\max_{a\in\mathcal A}\mathcal R_s^a+\gamma\sum_{s&#39;\in\mathcal S}v_*(s&#39;)
\]</span> The idea of value iteration is to apply these updates iteratively.</p>
<p><strong>Intuition</strong>: start with final rewards and word backwards. <span class="math display">\[
v_{k+1}(s)=\max_{a\in\mathcal A}\left(\mathcal R_s^a+\gamma\sum_{s&#39;\in\mathcal S}\mathcal R_{ss&#39;}^av_k(s&#39;)\right)
\]</span> <span class="math display">\[
v_{k+1}=\max_{a\in\mathcal A}\mathcal R^a+\gamma\mathcal P^av_k
\]</span></p>
<h2 id="synchronous-dynamic-programming-algorithms">Synchronous Dynamic Programming Algorithms</h2>
<table style="width:50%;">
<colgroup>
<col style="width: 11%" />
<col style="width: 23%" />
<col style="width: 15%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Problem</th>
<th style="text-align: left;">Bellman Equation</th>
<th style="text-align: left;">Algorithm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Prediction</td>
<td style="text-align: left;">Bellman Expectation Equation</td>
<td style="text-align: left;">Iterative Policy Evaluation</td>
</tr>
<tr class="even">
<td style="text-align: left;">Control</td>
<td style="text-align: left;">Bellman Expectation Equation + Greedy Policy Improvement</td>
<td style="text-align: left;">Policy Iteration</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Control</td>
<td style="text-align: left;">Bellman Optimally Equation</td>
<td style="text-align: left;">Value Iteration</td>
</tr>
</tbody>
</table>
<ul>
<li>Algorithms are based on state-value function <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(v_*(s)\)</span>. Complexity <span class="math inline">\(O(mn^2)\)</span> per iteration for <span class="math inline">\(m\)</span> actions and <span class="math inline">\(n\)</span> states.</li>
<li>Could also apply to action-value function <span class="math inline">\(q_\pi(s,a)\)</span> or <span class="math inline">\(q_*(s,a)\)</span>. Complexity <span class="math inline">\(O(m^2n^2)\)</span> per iteration.</li>
</ul>
<h2 id="asynchronous-dynamic-programming">Asynchronous Dynamic Programming</h2>
<p>Asynchronous DP backs up states individually in any order. For each selected state, apply the appropriate backup. Can significantly reduce computation. Guaranteed to converge if all states continue to be selected.</p>
<h3 id="in-place-dynamic-programming">In-Place Dynamic Programming</h3>
<p><strong>Synchronous value iteration</strong> stores two copies of value function for all <span class="math inline">\(s\)</span> in <span class="math inline">\(\mathcal{S}\)</span> <span class="math display">\[
\begin{align}
v_{new}(s) &amp;\leftarrow \max_{a\in{\mathcal{A}}}\left(\mathcal{R}_s^a+
\gamma\sum_{s&#39;\in{\mathcal{S}}}\mathcal{P}^a_{ss&#39;}v_{old}(s&#39;)
\right) \\
v_{old} &amp;\leftarrow v_{new}
\end{align}
\]</span> <strong>In-place value iteration</strong> only stores one copy of value function for all <span class="math inline">\(s\)</span> in <span class="math inline">\(\mathcal{S}\)</span> <span class="math display">\[
v(s) \leftarrow \max_{a\in\mathcal{A}}\left(
\mathcal{R}_s^a+\gamma\sum_{s&#39;\in\mathcal{S}}\mathcal{P}_{ss&#39;}^av(s&#39;)
\right)
\]</span></p>
<h3 id="prioritised-sweeping">Prioritised Sweeping</h3>
<p>Use magnitude of Bellman error to guide state selection, e.g. <span class="math display">\[
\left|
\max_{a\in\mathcal{A}}
\left(
\mathcal{R}_s^a + \gamma\sum_{s&#39;\in\mathcal{S}}\mathcal{P}_{ss&#39;}^av(s&#39;)
\right) - v(s)
\right|
\]</span></p>
<ul>
<li>Backup the state with the largest remaining Bellman error</li>
<li>Update Bellman error of affected states after each backup</li>
<li>Requires knowledge of reverse dynamics (predecessor states)</li>
<li>Can be implemented efficiently by maintaining a priority queue</li>
</ul>
<h3 id="real-time-dynamic-programming">Real-Time Dynamic Programming</h3>
<p><strong>Idea</strong>: only states that are relevant to agent. Use agent’s experience to guide the selection of states. After each time-step <span class="math inline">\(S_t, A_t, R_{t+1}\)</span>, backup the state <span class="math inline">\(S_t\)</span>: <span class="math display">\[
v(S_t) \leftarrow \max_{a\in\mathcal{A}} \left(
\mathcal{R}_{S_t}^a + \gamma\sum_{s&#39;\in\mathcal{S}}
\mathcal{P}_{S_ts&#39;}^av(s&#39;)
\right)
\]</span></p>
<h2 id="full-width-and-sample-backups">Full-width and sample backups</h2>
<h3 id="full-width-backups">Full-Width Backups</h3>
<p>DP uses <em>full-width</em> backups. For each backup (sync or async):</p>
<ul>
<li>Every successor state and action is considered</li>
<li>Using knowledge of the MDP transitions and reward function</li>
</ul>
<p>DP is effective for medium-sized problems (millions of states). For large problems DP suffers Bellman’s <em>curse of dimensionality</em> (Number of states <span class="math inline">\(n=|\mathcal{S}|\)</span> grows exponentially with number of state variables). Even one backup can be too expensive.</p>
<h3 id="sample-backups">Sample Backups</h3>
<p>Using sample rewards and sample transitions <span class="math inline">\(\left&lt;\mathcal{S,A,R,S&#39;}\right&gt;\)</span> instead of reward function <span class="math inline">\(\mathcal{R}\)</span> and transition dynamics <span class="math inline">\(\mathcal{P}\)</span>.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Model-free: no advance knowledge of MDP required</li>
<li>Breaks the curse of dimensionality through sampling</li>
<li>Cost of backup is constant, independent of <span class="math inline">\(n=|\mathcal{S}|\)</span></li>
</ul>

	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="/categories.html#reinforcement learning-ref">
					reinforcement learning <span>(11)</span>
					
				</a></li>
			
		  
		</ul>
		  

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#reinforcement learning-ref">
					reinforcement learning <span>(11)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-12">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=Dynamic Programming"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/deep%20learning/rnn/2016/05/03/rnn-em.html" title="Recurrent Neural Networks with External Memory">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="/reinforcement%20learning/2016/05/19/monte-carlo-reinforcement-learning.html" title="Monte-Carlo Reinforcement Learning">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>




		<footer>
			<hr/>
			<p>
				&copy; 2016 <a href="http://guoqiang.gq">sohero</a>.
                <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258527085'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258527085%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
			</p>
		</footer>
	</div>

	<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
	<script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



