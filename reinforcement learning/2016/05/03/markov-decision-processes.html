<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Markov Decision Processes</title>
	
	<meta name="author" content="sohero">
	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css">
	<link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/css/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">
	<link rel="shortcut icon" href="favicon.ico">
	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS_HTML"></script>
    
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/sohero">
				<i class="fa fa-github"></i>
			</a>
			
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
			<!--
            <input type="text" class="st-default-search-input navbar-toggle" style="width:100px;">
            -->
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-2 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/assets/images/bl.png" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/"></a>
    </h3>
</header>


<div id="bio" class="text-center">
	Qiang Brother Notes.
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/sohero">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
        <li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-10 col-sm-offset-2">
		<div class="page-header">
  <h1>Markov Decision Processes </h1>
</div>
	
<article>

	<div class="col-sm-12">
	 <span class="post-date">
	 	2016-05-03 
	 </span>
	 <div class="article-outline"></div>
	  <div class="article_body">
	  <h2 id="rewards">Rewards</h2>
<ul>
<li>A <code>reward</code> <span class="math inline">\(R_t\)</span> is a scalar feedback signal</li>
<li>Indecates how well agent is doing at step <span class="math inline">\(t\)</span></li>
<li>The agent’s job is to maximise cumulative reward</li>
</ul>
<p>Reinforcement learning is based on the <code>reward hypothesis</code>:</p>
<blockquote>
<p>All goals can be described by the maximisation of expected cumulative reward</p>
</blockquote>
<h2 id="information-statea.k.a-markov-state">Information State(a.k.a Markov state)</h2>
<p>A state <span class="math inline">\(S_t\)</span> is <code>Markov</code> if and only if <span class="math display">\[
\Bbb P[S_{t+1}|S_t]=\Bbb P[S_{t+1}|S_1,...,S_t]
\]</span></p>
<ul>
<li>The future is independent of the past given the present</li>
<li>Once the state is known, the history may be thrown away</li>
</ul>
<h2 id="markov-decision-processes">Markov Decision Processes</h2>
<h3 id="markov-processes">Markov Processes</h3>
<h4 id="state-transition-matrix">State Transition Matrix</h4>
<p>For a Markov state <span class="math inline">\(\mathcal s\)</span> and successor state <span class="math inline">\(\mathcal s&#39;\)</span>, the <em>state transition probability</em> is defined by <span class="math display">\[
\mathcal P_{\mathcal s\mathcal s&#39;}=\Bbb P[\mathcal S_{t+1}=\mathcal s&#39;|\mathcal S_t=\mathcal s]
\]</span> State transition matrix <span class="math inline">\(\mathcal P\)</span> defines transition probabilities from all states <span class="math inline">\(s\)</span> to all successor states <span class="math inline">\(\mathcal s&#39;\)</span>, <span class="math display">\[
\mathcal P=\text{from}
\begin{array}{c}
\text{to} \\
\begin{bmatrix}
\mathcal P_{11} &amp; \cdots &amp; \mathcal P_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
\mathcal P_{n1} &amp; \cdots &amp; \mathcal P_{nn}
\end{bmatrix}
\end{array}
\]</span> where each row of the matrix sums to 1.</p>
<h4 id="markov-process">Markov Process</h4>
<p>A Markov process is a memoryless random process, i.e. a sequence of random states <span class="math inline">\(\mathcal S_1,\mathcal S_2,...\)</span> with the Markov property.</p>
<p>A <em>Markov Process(or Markov Chain)</em> is a tuple <span class="math inline">\(\left&lt;\mathcal S,\mathcal P\right&gt;\)</span></p>
<ul>
<li><span class="math inline">\(\mathcal S\)</span> is a (finite) set of states</li>
<li><span class="math inline">\(\mathcal P\)</span> is state transition probability matrix</li>
</ul>
<h3 id="markov-reward-process">Markov Reward Process</h3>
<p>A Markov reward process is a Markov chain with values.</p>
<p>A <em>Markov Reward Process</em> is a tuple <span class="math inline">\(\left&lt;\mathcal S,\mathcal P,\mathcal R,\gamma\right&gt;\)</span></p>
<ul>
<li><span class="math inline">\(\mathcal S\)</span> is a finite set of states</li>
<li><span class="math inline">\(\mathcal P\)</span> is a state transition probability matrix</li>
<li><span class="math inline">\(\mathcal R\)</span> is a reward function, <span class="math inline">\(\mathcal R_s=\Bbb E[R_{t+1}|\mathcal S_t=\mathcal s]\)</span></li>
<li><span class="math inline">\(\gamma\)</span> is a discount factor, <span class="math inline">\(\gamma \in [0,1]\)</span></li>
</ul>
<h4 id="return">Return</h4>
<p>The return <span class="math inline">\(G_t\)</span>(Goal) is the total discounted reward from time-step <span class="math inline">\(t\)</span>. <span class="math display">\[
G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^kR_{t+k+1}
\]</span> The discount <span class="math inline">\(\gamma\in[0,1]\)</span> is the present value of future rewords.</p>
<h4 id="value-function">Value Function</h4>
<p>The value function <span class="math inline">\(v(s)\)</span> gives the long-term value of state <span class="math inline">\(\mathcal s\)</span>.</p>
<p>The <em>state value function</em> <span class="math inline">\(v(s)\)</span> of an MRP is the expected <code>Return</code> starting from state <span class="math inline">\(\mathcal s\)</span> <span class="math display">\[
v(s)=\Bbb E[G_t|\mathcal S_t=\mathcal s]
\]</span></p>
<figure>
<img src="/assets/images/39.png" alt="" />
</figure>
<figure>
<img src="/assets/images/38.png" alt="" />
</figure>
<blockquote>
<p>The reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.</p>
</blockquote>
<h4 id="bellman-equation-for-mrps">Bellman Equation for MRPs</h4>
<p>The value function can be decomposed into two parts:</p>
<ul>
<li>immediate reward <span class="math inline">\(R_{t+1}\)</span></li>
<li>discounted value of successor state <span class="math inline">\(\gamma v(S_{t+1})\)</span></li>
</ul>
<p><span class="math display">\[
\begin{align}
v(s) &amp;= \Bbb E[G_t|S_t=s] \\
&amp;= \Bbb E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...|S_t=s] \\
&amp;= \Bbb E[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+...)|S_t=s] \\
&amp;= \Bbb E[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
&amp;= \Bbb E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]
\end{align}
\]</span> <span class="math display">\[
v(s)=\mathcal R_s+\gamma \sum_{s&#39;\in\mathcal S} \mathcal P_{ss&#39;}v(s&#39;)
\]</span></p>
<h4 id="bellman-equation-in-matrix-form">Bellman Equation in Matrix Form</h4>
<p>The Bellman equation can be expressed concisely using matrices, <span class="math display">\[
v=\mathcal R+\gamma \mathcal Pv
\]</span> where <span class="math inline">\(v\)</span> is a column vector with one entry per state <span class="math display">\[
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
=
\begin{bmatrix}
\mathcal R_1 \\
\vdots \\
\mathcal R_n
\end{bmatrix}
+\gamma \begin{bmatrix}
\mathcal P_{11} &amp; \cdots &amp; \mathcal P_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
\mathcal P_{n1} &amp; \cdots &amp; \mathcal P_{nn}
\end{bmatrix}
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
\]</span></p>
<h4 id="sloving-the-bellman-equation">Sloving the Bellman Equation</h4>
<ul>
<li>The Bellman equation is a linear equation</li>
<li>It can be solved directly: <span class="math display">\[
\begin{align}
v &amp;= \mathcal R+\gamma\mathcal Pv \\
(I-\gamma\mathcal P)v &amp;=\mathcal R \\
v &amp;= (I-\gamma\mathcal P)^{-1}\mathcal R
\end{align}
\]</span></li>
<li>Computational complexity is <span class="math inline">\(O(n^3)\)</span> for <span class="math inline">\(n\)</span> states</li>
<li>Direct solution only possible for small MRPs</li>
</ul>
<h3 id="markov-decision-process">Markov Decision Process</h3>
<p>A Markov decision process(MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.</p>
<p>A Markov Decision Process is a tuple <span class="math inline">\(\left&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma\right&gt;\)</span></p>
<ul>
<li><span class="math inline">\(\mathcal A\)</span> is a finite set of actions</li>
</ul>
<figure>
<img src="/assets/images/40.png" alt="" />
</figure>
<h4 id="policies">Policies</h4>
<p>A policy <span class="math inline">\(\pi\)</span> is a distribution over actions given states <span class="math display">\[
\pi(a|s)=\Bbb P[A_t=a|S_t=s]
\]</span></p>
<ul>
<li>A policy fully defines the behaviour of an agent</li>
<li>MDP policies depend on the current state (not the history)</li>
</ul>
<h4 id="value-function-1">Value Function</h4>
<p>The <em>state-value function</em> <span class="math inline">\(v_\pi(s)\)</span> of an MDP is the expected return starting from state <span class="math inline">\(s\)</span>, and then following policy <span class="math inline">\(\pi\)</span> <span class="math display">\[
v_\pi(s)=\Bbb E_\pi[G_t|S_t=s]
\]</span></p>
<p>The <em>action-value function</em> <span class="math inline">\(q_\pi(s,a)\)</span> is the expected return starting from state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span>, and then following policy <span class="math inline">\(\pi\)</span> <span class="math display">\[
q_\pi(s,a)=\Bbb E_\pi[G_t|S_t=s,A_t=a]
\]</span></p>
<h4 id="bellman-expectation-equation">Bellman Expectation Equation</h4>
<p>The state-value function can again be decomposed into immediate reward plus discounted value of successor state <span class="math display">\[
\begin{align}
v_\pi(s) &amp;= \Bbb E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s] \\
&amp;=\sum_{a\in\mathcal A} \pi(a|s)q_\pi(s,a)
\end{align}
\]</span></p>
<p>The action-value function can similarly be decomposed, <span class="math display">\[
\begin{align}
q_\pi(s,a) &amp;= \Bbb E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a] \\
&amp;= \mathcal R_s^a + \gamma\sum_{s&#39;\in\mathcal S} \mathcal P_{ss&#39;}^a v_\pi(s&#39;)
\end{align}
\]</span></p>
<h4 id="bellman-expectation-equation-matrix-form">Bellman Expectation Equation (Matrix Form)</h4>
<p>The Bellman expectation equation can be expressed concisely using the induced MRP, <span class="math display">\[
v_\pi=\mathcal R^\pi + \gamma \mathcal P^\pi v_\pi
\]</span> with direct solution <span class="math display">\[
v_\pi=(I-\gamma\mathcal P^\pi)^{-1}\mathcal R^\pi
\]</span></p>
<h4 id="optimal-value-function">Optimal Value Function</h4>
<p>The <em>optimal state-value function</em> <span class="math inline">\(v_*(s)\)</span> is the maximum value function over all policies <span class="math display">\[
v_*(s)=\max_\pi v_\pi(s)
\]</span></p>
<p>The <em>optimal action-value function</em> <span class="math inline">\(q_*(s,a)\)</span> is the maximum action-value function over all policies <span class="math display">\[
q_*(s,a)=\max_\pi q_\pi(s,a)
\]</span></p>
<ul>
<li>The optimal value function specifies the best possible performance in the MDP.</li>
<li>An MDP is “solved” when we know the optimal value fn.</li>
</ul>
<h3 id="partially-observable-mdps-pomdps">Partially Observable MDPs (POMDPs)</h3>
<p>A Partially Observable Markov Decision Process is an MDP with hidden states. It is a hidden Markov model with actions.</p>
<p>A POMDP is a tuple <span class="math inline">\(\left&lt;\mathcal S,\mathcal A,\mathcal O,\mathcal P,\mathcal R,\mathcal Z,\gamma\right&gt;\)</span></p>
<ul>
<li><span class="math inline">\(\mathcal O\)</span> is a finite set of observations</li>
<li><span class="math inline">\(\mathcal Z\)</span> is an observation function, <span class="math display">\[
\mathcal Z_{s&#39;o}^a=\Bbb P[O_{t+1}=o|S_{t+1}=s&#39;,A_t=a]
\]</span></li>
</ul>

	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="/categories.html#reinforcement learning-ref">
					reinforcement learning <span>(12)</span>
					
				</a></li>
			
		  
		</ul>
		  

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#reinforcement learning-ref">
					reinforcement learning <span>(12)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-12">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=Markov Decision Processes"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/deep%20learning/tools/2016/04/13/a-full-hardware-guide-to-deep-learning.html" title="A Full Hardware Guide to Deep Learning">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="/deep%20learning/rnn/2016/05/03/rnn-em.html" title="Recurrent Neural Networks with External Memory">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>




		<footer>
			<hr/>
			<p>
				&copy; 2016 <a href="http://guoqiang.gq">sohero</a>.
                <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258527085'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258527085%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
			</p>
		</footer>
	</div>

	<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
	<script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



