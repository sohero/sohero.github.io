<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Model free Control</title>
	
	<meta name="author" content="sohero">
	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css">
	<link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/css/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">
	<link rel="shortcut icon" href="favicon.ico">
	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS_HTML"></script>
    
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/sohero">
				<i class="fa fa-github"></i>
			</a>
			
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
			<!--
            <input type="text" class="st-default-search-input navbar-toggle" style="width:100px;">
            -->
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-2 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/assets/images/bl.png" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/"></a>
    </h3>
</header>


<div id="bio" class="text-center">
	Qiang Brother Notes.
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/sohero">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
        <li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-10 col-sm-offset-2">
		<div class="page-header">
  <h1>Model free Control </h1>
</div>
	
<article>

	<div class="col-sm-12">
	 <span class="post-date">
	 	2016-06-11 
	 </span>
	 <div class="article-outline"></div>
	  <div class="article_body">
	  <p><strong>Model-free prediction</strong>: <em>Estimate</em> the value function of an <em>unknown</em> MDP.</p>
<p><strong>Model-free Control</strong>: <em>Optimise</em> the value function of an <em>unknown</em> MDP.</p>
<h2 id="on-and-off-policy-learning">On and Off-Policy Learning</h2>
<ul>
<li>On-policy learning
<ul>
<li>“Learn on the job”</li>
<li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\pi\)</span></li>
</ul></li>
<li>Off-policy learning
<ul>
<li>“Look over someone’s shoulder”</li>
<li>Learn about policy <span class="math inline">\(\pi\)</span> from experience sampled from <span class="math inline">\(\mu\)</span>.</li>
</ul></li>
</ul>
<h2 id="glie">GLIE</h2>
<p><em>Greedy in the Limit with Infinite Exploration</em> (GLIE).</p>
<ul>
<li>All state-action pairs are explored infinitely many times, <span class="math inline">\(\lim\limits_{k\to\infty}N_k(s,a)=\infty\)</span></li>
<li>The policy converges on a greedy policy, <span class="math display">\[
\lim\limits_{k\to\infty}\pi_k(a|s)=1(a=\mathop{\text{argmax}}\limits_{a&#39;\in\mathcal{A}}Q_k(s,a&#39;))
\]</span></li>
</ul>
<p>For example, <span class="math inline">\(\epsilon\)</span>-greedy is GLIE if <span class="math inline">\(\epsilon\)</span> reduces to zero at <span class="math inline">\(\epsilon_k={1\over k}\)</span>.</p>
<h2 id="glie-monte-carlo-control">GLIE Monte-Carlo Control</h2>
<p>Sample <span class="math inline">\(k\)</span>th episode using <span class="math inline">\(\pi\)</span>:<span class="math inline">\(\{\mathcal{S_1,A_1,R_2,...,S_T}\}\sim\pi\)</span>, for each state <span class="math inline">\(\mathcal S_t\)</span> and action <span class="math inline">\(\mathcal A_t\)</span> in the episode <span class="math display">\[
\begin{align}
N(\mathcal{S_t,A_t}) &amp;\leftarrow N(\mathcal{S_t,A_t})+1 \\
Q(\mathcal{S_t,A_t}) &amp;\leftarrow Q(\mathcal{S_t,A_t}) + {1\over N(\mathcal{S_t,A_t})}\left(G_t-Q(\mathcal{S_t,A_t})\right)
\end{align}
\]</span></p>
<p>Improve policy based on new action-value function <span class="math display">\[
\begin{align}
\epsilon &amp;\leftarrow {1\over k} \\
\pi &amp;\leftarrow \epsilon\text{-greedy}(Q)
\end{align}
\]</span></p>
<h2 id="saralambda">Sara(<span class="math inline">\(\lambda\)</span>)</h2>
<p><span class="math display">\[
Q(\mathcal{S,A})\leftarrow Q(\mathcal{S,A})+\alpha(R+\gamma Q(\mathcal{S&#39;,A&#39;})-Q(\mathcal{S,A}))
\]</span></p>
<h3 id="n-step-sarsa"><span class="math inline">\(n\)</span>-Step Sarsa</h3>
<p>Define the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[
q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n})
\]</span> <span class="math inline">\(n\)</span>-step Sarsa updates <span class="math inline">\(Q(s,a)\)</span> towards the <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-return <span class="math display">\[
Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha\left(q_t^{(n)}-Q(S_t,A_t)\right)
\]</span></p>
<h3 id="forward-view-sarsalambda">Forward View Sarsa(<span class="math inline">\(\lambda\)</span>)</h3>
<p>The <span class="math inline">\(q^\lambda\)</span> return combines all <span class="math inline">\(n\)</span>-step <span class="math inline">\(Q\)</span>-returns <span class="math inline">\(q_t^{(n)}\)</span>, using weight <span class="math inline">\((1-\lambda)\lambda^{n-1}\)</span> <span class="math display">\[
q_t^\lambda=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)}
\]</span> Forward-view Sarsa(<span class="math inline">\(\lambda\)</span>) <span class="math display">\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left(q_t^\lambda-Q(S_t,A_t)\right)
\]</span></p>
<h3 id="backward-view-sarsalambda">Backward View Sarsa(<span class="math inline">\(\lambda\)</span>)</h3>
<p>Just like TD(<span class="math inline">\(\lambda\)</span>), we use <code>eligibility traces</code> in an online algorithm. But Sarsa(<span class="math inline">\(\lambda\)</span>) has one eligibility trace for each state-action pair <span class="math display">\[
\begin{align}
E_0(s,a) &amp;= 0 \\
E_t(s,a) &amp;= \gamma\lambda E_{t-1}(s,a)+1(S_t=s,A_t=a)
\end{align}
\]</span> <span class="math inline">\(Q(s,a)\)</span> is updated for every state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span>. Inproportion to TD-error <span class="math inline">\(\delta_t\)</span> and eligibility trace <span class="math inline">\(E_t(s,a)\)</span> <span class="math display">\[
\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})-Q(S_t,A_t)
\]</span> <span class="math display">\[
Q(s,a)\leftarrow Q(s,a)+\alpha\delta_t E_t(s,a)
\]</span></p>
<h2 id="off-policy-learning">Off-Policy Learning</h2>
<p>Evaluate target policy <span class="math inline">\(\pi(a|s)\)</span> to compute <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(q_\pi(s,a)\)</span>, while following behaviour policy <span class="math inline">\(\mu(a|s)\)</span> <span class="math display">\[
\{\mathcal{S_1,A_1,R_2,...,S_t}\}\sim \mu
\]</span></p>
<h3 id="importance-sampling-for-off-policy-monte-carlo">Importance Sampling for Off-Policy Monte-Carlo</h3>
<p>Use returns generated from <span class="math inline">\(\mu\)</span> to evaluate <span class="math inline">\(\pi\)</span>. Weight return <span class="math inline">\(G_t\)</span> according to similarity between policies. Multiply importance sampling corrections along whole episode <span class="math display">\[
G_t^{\pi / \mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}
\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}
...
\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}
G_t
\]</span> Update value towards <em>corrected</em> return <span class="math display">\[
V(S_t) \leftarrow V(S_t)+\alpha\left(
\color{red}{G_t^{\pi / \mu}}
-V(S_t)\right)
\]</span> Cannot use if <span class="math inline">\(\mu\)</span> is zero when <span class="math inline">\(\pi\)</span> is non-zero. Importance sampling can dramatically increase variance.</p>
<h3 id="importance-sampling-for-off-policy-td">Importance Sampling for Off-Policy TD</h3>
<p><span class="math display">\[
V(S_t)\leftarrow V(S_t)+\alpha\left(
\color{red}{\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}}
(R_{t+1}+\gamma V(S_{t+1}))-V(s_t)
\right)
\]</span> Much lower variance than Monte-Carlo importance sampling. Policies only need to be similar over a single step.</p>
<h3 id="q-learning">Q-Learning</h3>
<p>Next action is chosen using behaviour policy <span class="math inline">\(A_{t+1}\sim\mu(\cdot |S_t)\)</span>, but we consider alternative successor action <span class="math inline">\(A&#39;\sim\pi(\cdot |S_t)\)</span>, and update <span class="math inline">\(Q(S_t,A_t)\)</span> towards value of alternative action <span class="math display">\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left(
\color{red}{R_{t+1}+\gamma Q(S_{t+1},A&#39;)}
-Q(S_t,A_t)
\right)
\]</span></p>
<h3 id="off-policy-control-with-q-learning">Off-Policy Control with Q-Learning</h3>
<p>We now allow both behaviour and target policies to improve. The target policy <span class="math inline">\(\pi\)</span> is greedy w.r.t. <span class="math inline">\(Q(s,a)\)</span> <span class="math display">\[
\pi(S_{t+1})=\mathop{\text{argmax}} \limits_{a&#39;}Q(S_{t+1},a&#39;)
\]</span> The behaviour policy <span class="math inline">\(\mu\)</span> is e.g. <span class="math inline">\(\epsilon\)</span>-greedy w.r.t. <span class="math inline">\(Q(s,a)\)</span>. The Q-learning target then simplifies: <span class="math display">\[
\begin{align}
R_{t+1} &amp;+ \gamma Q(S_{t+1},A&#39;) \\
=R_{t+1} &amp;+ \gamma Q(S_{t+1}, \mathop{\text{argmax}} \limits_{a&#39;} Q(S_{t+1},a&#39;)) \\
=R_{t+1} &amp;+ \max_{a&#39;} \gamma Q(S_{t+1},a&#39;)
\end{align}
\]</span></p>
<h3 id="q-learning-control-algorithm">Q-Learning Control Algorithm</h3>
<p><span class="math display">\[
Q(S,A)\leftarrow Q(S,A)+\alpha\left(R+\gamma\max_{a&#39;}Q(S&#39;,a&#39;)-Q(S,A)\right)
\]</span></p>

	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="/categories.html#reinforcement learning-ref">
					reinforcement learning <span>(10)</span>
					
				</a></li>
			
		  
		</ul>
		  

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#reinforcement learning-ref">
					reinforcement learning <span>(10)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-12">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=Model free Control"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/reinforcement%20learning/2016/05/20/rl-concepts.html" title="RL Concepts">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="/reinforcement%20learning/2016/06/19/value-function-approximation.html" title="Value Function Approximation">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>




		<footer>
			<hr/>
			<p>
				&copy; 2016 <a href="http://guoqiang.gq">sohero</a>.
                <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258527085'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258527085%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
			</p>
		</footer>
	</div>

	<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
	<script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



