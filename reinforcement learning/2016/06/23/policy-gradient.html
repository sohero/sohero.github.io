<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Policy Gradient</title>
	
	<meta name="author" content="sohero">
	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css">
	<link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/css/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">
	<link rel="shortcut icon" href="favicon.ico">
	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS_HTML"></script>
    
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/sohero">
				<i class="fa fa-github"></i>
			</a>
			
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
            <input type="text" class="st-default-search-input navbar-toggle" style="width:100px;">
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-3 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/assets/images/bl.png" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/"></a>
    </h3>
</header>


<div id="bio" class="text-center">
	Qiang Brother Notes.
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/sohero">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
        <li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-9 col-sm-offset-3">
		<div class="page-header">
  <h1>Policy Gradient </h1>
</div>
	
<article>

	<div class="col-sm-12">
	 <span class="post-date">
	 	2016-06-23 
	 </span>
	 <div class="article-outline"></div>
	  <div class="article_body">
	  <p>Directly parametrise the <strong>policy</strong>: <span class="math display">\[
\pi_\theta(s,a)=\Bbb P[a|s,\theta]
\]</span></p>
<h2 id="advantages-of-policy-based-rl">Advantages of Policy-Based RL</h2>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Better convergence properties</li>
<li>Effective in high-dimensional or continuous action spaces</li>
<li>Can learn stochastic policies</li>
</ul>
<p><strong>Disadvantages</strong>:</p>
<ul>
<li>Typically converge to a local rather than global optimum</li>
<li>Evaluting a policy is typically inefficient and high variance</li>
</ul>
<h2 id="policy-objective-functions">Policy Objective Functions</h2>
<p><strong>Goal</strong>: given policy <span class="math inline">\(\pi_\theta(s,a)\)</span> with parameters <span class="math inline">\(\theta\)</span>, find best <span class="math inline">\(\theta\)</span>.</p>
<p>Policy based reinforcement learning is an <code>optimisation</code> problem, find <span class="math inline">\(\theta\)</span> that <strong>maximises</strong> <span class="math inline">\(J(\theta)\)</span>.</p>
<h2 id="policy-gradient">Policy Gradient</h2>
<p>Let <span class="math inline">\(J(\theta)\)</span> be any policy objective function. Policy gradient algorithms search for a <em>local</em> maximum in <span class="math inline">\(J(\theta)\)</span> by ascending the gradient of the policy, w.r.t. parameters <span class="math inline">\(\theta\)</span> <span class="math display">\[
\Delta\theta=\alpha\nabla_\theta J(\theta)
\]</span></p>
<h2 id="score-function">Score Function</h2>
<p>Assume policy <span class="math inline">\(\pi_\theta\)</span> is differentiable whenever it is non-zero and we know the gradient <span class="math inline">\(\nabla_\theta \pi_\theta(s,a)\)</span>. <code>Likelihood ratios</code> exploit the following identity <span class="math display">\[
\begin{align}
\nabla_\theta\pi_\theta(s,a) &amp;= \pi_\theta(s,a) \frac{\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)} \\
&amp;= \pi_\theta(s,a) \nabla_\theta log \pi_\theta(s,a)
\end{align}
\]</span> The <code>score function</code> is <span class="math inline">\(\nabla_\theta log \pi_\theta(s,a)\)</span>.</p>
<h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2>
<h3 id="one-step-mdps">One-Step MDPs</h3>
<p>Consider a simple class of <em>one-step</em> MDPs: Starting in state <span class="math inline">\(s\sim d(s)\)</span>; Terminating after one time-step with reward <span class="math inline">\(r=\mathcal R_{s,a}\)</span>.</p>
<p>Use likelihood ratios to compute the policy gradient <span class="math display">\[
\begin{align}
J(\theta) &amp;= \Bbb E_{\pi_\theta}[r] \\
&amp;= \sum_{\mathcal{s\in S}} d(s) \sum_{a\in\mathcal A} \pi_\theta(s,a)\mathcal R_{s,a} \\
\nabla_\theta J(\theta) &amp;= \sum_{s\in\mathcal S}d(s)\sum_{a\in\mathcal A}\pi_\theta(s,a)\nabla_\theta log \pi_\theta(s,a)\mathcal R_{s,a} \\
&amp;=\Bbb E_{\pi_\theta}\left[\nabla_\theta log \pi_\theta(s,a)r\right]
\end{align}
\]</span></p>
<blockquote>
<p>For any differentiable policy <span class="math inline">\(\pi_\theta(s,a)\)</span>, for any of the policy objective functions <span class="math inline">\(J\)</span>, the policy gradient is <span class="math display">\[
\nabla_\theta J(\theta)=\Bbb E_{\pi_\theta}
\left[
\nabla_\theta log\pi_\theta(s,a)Q^{\pi_\theta}(s,a)
\right]
\]</span></p>
</blockquote>
<h3 id="monte-carlo-policy-gradient">Monte-Carlo Policy Gradient</h3>
<ul>
<li>Update parameters by stochastic gradient ascent</li>
<li>Using policy gradient theorem</li>
<li>Using return <span class="math inline">\(v_t\)</span> as an unbiased sample of <span class="math inline">\(Q^{\pi_\theta}(s_t,a_t)\)</span> <span class="math display">\[
\Delta\theta_t=\alpha\nabla_\theta log \pi_\theta(s_t,a_t)v_t
\]</span></li>
</ul>
<h2 id="actor-critic-policy-gradient">Actor-Critic Policy Gradient</h2>
<h3 id="reducing-variance-using-a-critic">Reducing Variance Using a Critic</h3>
<p>Monte-Carlo policy gradient still has high variance. We use a <code>critic</code> to estimate the action-value function <span class="math display">\[
Q_w(s,a)\approx Q^{\pi_\theta}(s,a)
\]</span> Actor-Critic algorithms maintain <em>two</em> sets of parameters</p>
<ul>
<li><strong>Critic</strong> Updates action-value function parameters <span class="math inline">\(w\)</span></li>
<li><strong>Actor</strong> Updates policy parameters <span class="math inline">\(\theta\)</span>, in direction suggested by critic</li>
</ul>
<p>Actor-critic algorithms follow an <em>approximate</em> policy gradient <span class="math display">\[
\begin{align}
\nabla_\theta J(\theta) &amp;\approx \Bbb E_{\pi_\theta}\left[
\nabla_\theta log \pi_\theta(s,a) Q_w(s,a)
\right] \\
\Delta \theta &amp;= \alpha\nabla_\theta log \pi_\theta(s,a) Q_w(s,a)
\end{align}
\]</span></p>
<h3 id="action-value-actor-critic">Action-Value Actor-Critic</h3>
<p>Simple actor-critic algorithm based on action-critic. Using linear value fn approx. <span class="math inline">\(Q_w(s,a)=\phi (s,a)^Tw\)</span></p>
<ul>
<li><strong>Critic</strong> Updates <span class="math inline">\(w\)</span> by linear TD(0)</li>
<li><strong>Actor</strong> Updates <span class="math inline">\(\theta\)</span> by policy gradient</li>
</ul>
<h3 id="compatible-function-approximation">Compatible Function Approximation</h3>
<p>If the following two conditions are satisfied:</p>
<ul>
<li>Value function approximator is compatible to the policy <span class="math display">\[
 \nabla_w Q_w(s,a)=\nabla_\theta log \pi_\theta(s,a)
 \]</span></li>
<li>Value function parameters <span class="math inline">\(w\)</span> minimise the mean-squared error <span class="math display">\[
 \varepsilon=\Bbb E_{\pi_\theta}\left[
(Q^{\pi_\theta}(s,a)-Q_w(s,a))^2
 \right]
 \]</span> Then the policy gradient is exist <span class="math display">\[
\nabla_\theta J(\theta)=\Bbb E_{\pi_\theta}
\left[
\nabla_\theta log \pi_\theta(s,a) Q_w(s,a)
\right]
\]</span></li>
</ul>
<h3 id="reducing-variance-using-a-baseline">Reducing Variance Using a Baseline</h3>
<p>We subtract a baseline function <span class="math inline">\(B(s)\)</span> from the policy gradient. This can reduce variance, without changing expectation <span class="math display">\[
\begin{align}
\Bbb E_{\pi_\theta}[\nabla_\theta log \pi_\theta(s,a)B(s)] &amp;= 
\sum_{s\in\mathcal S} d^{\pi_\theta}(s) \sum_a \nabla_\theta \pi_\theta(s,a)B(s) \\
&amp;=\sum_{s\in\mathcal S} d^{\pi_\theta}B(s)\nabla_\theta \sum_{a\in\mathcal A} \pi_\theta(s,a) \\
&amp;=0
\end{align}
\]</span> A good baseline is the state value function <span class="math inline">\(B(s)=V^{\pi_\theta}(s)\)</span>. So we can rewrite the policy gradient using the <code>advantage function</code> <span class="math inline">\(A^{\pi_\theta}(s,a)\)</span> <span class="math display">\[
\begin{align}
A^{\pi_\theta}(s,a) &amp;= Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s) \\
\nabla_\theta J(\theta) &amp;= \color{red}{
    \Bbb E_{\pi_\theta}[\nabla_\theta log \pi_\theta(s,a)A^{\pi_\theta}(s,a)]
}
\end{align}
\]</span></p>
<h2 id="summary-of-policy-gradient-algorithms">Summary of Policy Gradient Algorithms</h2>
<p>The <em>policy gradient</em> has many equivalent forms <span class="math display">\[
\begin{align}
\nabla_\theta J(\theta) &amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{v_t}] \tag{REINFORCE} \\
&amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{Q^w(s,a)}] \tag{Q Actor-Critic} \\
&amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{A^w(s,a)}] \tag{Advantage Actor-Critic} \\
&amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{\delta}] \tag{TD Actor-Critic} \\
&amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{\delta e}] \tag{TD($\lambda$) Actor-Critic} \\
G_\theta^{-1}\nabla_\theta J(\theta) &amp;= w \tag{Natural Actor-Critic}
\end{align}
\]</span> Each leads a stochastic gradient ascent algorithm. Critic uses <code>policy evaluation</code> (e.g. MC or TD learning) to estimate <span class="math inline">\(Q^\pi(s,a)\)</span>, <span class="math inline">\(A^\pi(s,a)\)</span> or <span class="math inline">\(V^\pi(s)\)</span>.</p>

	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="/categories.html#reinforcement learning-ref">
					reinforcement learning <span>(8)</span>
					
				</a></li>
			
		  
		</ul>
		  

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#reinforcement learning-ref">
					reinforcement learning <span>(8)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-12">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=Policy Gradient"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/reinforcement%20learning/2016/06/19/value-function-approximation.html" title="Value Function Approximation">&larr; Previous</a></li>
		  
		  
			<li class="next disabled"><a>Next &rarr;</a>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>




		<footer>
			<hr/>
			<p>
				&copy; 2016 <a href="http://guoqiang.gq">sohero</a>.
                <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258527085'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258527085%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
			</p>
		</footer>
	</div>

	<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
	<script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



