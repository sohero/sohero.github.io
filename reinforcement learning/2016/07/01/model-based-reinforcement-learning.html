<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Model-Based Reinforcement Learning</title>
	
	<meta name="author" content="sohero">
	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->
    <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css">
	<link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/css/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">
	<link rel="shortcut icon" href="favicon.ico">
	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.6.1/MathJax.js?config=TeX-AMS_HTML"></script>
    
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/sohero">
				<i class="fa fa-github"></i>
			</a>
			
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
			<!--
            <input type="text" class="st-default-search-input navbar-toggle" style="width:100px;">
            -->
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-2 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/assets/images/bl.png" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/"></a>
    </h3>
</header>


<div id="bio" class="text-center">
	Qiang Brother Notes.
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/sohero">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
        <li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-10 col-sm-offset-2">
		<div class="page-header">
  <h1>Model-Based Reinforcement Learning </h1>
</div>
	
<article>

	<div class="col-sm-12">
	 <span class="post-date">
	 	2016-07-01 
	 </span>
	 <div class="article-outline"></div>
	  <div class="article_body">
	  <h2 id="model-based-and-model-free-rl">Model-Based and Model-Free RL</h2>
<ul>
<li>Model-Free RL
<ul>
<li>No model</li>
<li><code>Learn</code> value function (and/or policy) from experience</li>
</ul></li>
<li>Model-Based RL
<ul>
<li>Learn a model from experience</li>
<li><code>Plan</code> value function (and/or policy) from model</li>
</ul></li>
<li>Dyna
<ul>
<li>Learn a model from real experience</li>
<li><code>Learn and plan</code> value function (and/or policy) from real and simulated experience</li>
</ul></li>
</ul>
<h2 id="model-based-reinforcement-learning">Model-Based Reinforcement Learning</h2>
<h3 id="advantages-of-model-based-rl">Advantages of Model-Based RL</h3>
<ul>
<li>Advantages:
<ul>
<li>Can efficiently learn model by supervised learning methods</li>
<li>Can reason about model uncertainty</li>
</ul></li>
<li>Disadvantages:
<ul>
<li>First learn a model, then construct a value function (two sources of approximation error)</li>
</ul></li>
</ul>
<h3 id="what-is-a-model">What is a Model?</h3>
<p>A model <span class="math inline">\(\mathcal M\)</span> is a representation of and MDP <span class="math inline">\(\left&lt;\mathcal{S,A,P,R}\right&gt;\)</span>, parametrized by <span class="math inline">\(\eta\)</span>. We will assume state space <span class="math inline">\(\mathcal S\)</span> and action space <span class="math inline">\(\mathcal A\)</span> are known. So a model <span class="math inline">\(\mathcal{M=\left&lt;P_\eta,R_\eta\right&gt;}\)</span> represents state transitions <span class="math inline">\(\mathcal{P_\eta\approx P}\)</span> and rewards <span class="math inline">\(\mathcal{R_\eta\approx R}\)</span> <span class="math display">\[
\begin{align}
S_{t+1} &amp;\sim \mathcal P_\eta(S_{t+1}|S_t,A_t) \\
R_{t+1} &amp;= \mathcal R_\eta(R_{t+1}|S_t,A_t)
\end{align}
\]</span> Typically assume conditional independence between state transitions and rewards. <span class="math display">\[
\Bbb P[S_{t+1},R_{t+1}|S_t,A_t]=\Bbb P[S_{t+1}|S_t,A_t]\Bbb P[R_{t+1}|S_t,A_t]
\]</span></p>
<h3 id="model-learning">Model Learning</h3>
<p><strong>Goal</strong>: estimate model <span class="math inline">\(\mathcal M_\eta\)</span> from experience <span class="math inline">\(\{S_1,A_1,R_2,...,S_T\}\)</span>. This is a supervised learning problem <span class="math display">\[
S_1,A_1 \to R_2,S_2 \\
S_2,A_2 \to R_3,S_3
\]</span> Learning <span class="math inline">\(s,a\to r\)</span> is a <em>regression problem</em>.</p>
<p>Learning <span class="math inline">\(s,a\to s&#39;\)</span> is a <em>density estimation problem</em>.</p>
<h3 id="examples-of-models">Examples of Models</h3>
<ul>
<li>Table Lookup Model</li>
<li>Linear Expectation Model</li>
<li>Linear Gaussian Model</li>
<li>Gaussian Process Model</li>
<li>Deep Belief Network Model</li>
<li>…</li>
</ul>
<h3 id="planning-with-a-model">Planning with a Model</h3>
<p>Given a model <span class="math inline">\(\mathcal{M_\eta=\left&lt;P_\eta,R_\eta\right&gt;}\)</span>. Solve the MDP <span class="math inline">\(\left&lt;\mathcal{S,A,P_\eta,R_\eta}\right&gt;\)</span> using favourite planning algorithm</p>
<ul>
<li>Value iteration</li>
<li>Policy iteration</li>
<li>Tree search</li>
<li>…</li>
</ul>
<h2 id="integrated-architectures-dyna">Integrated Architectures: Dyna</h2>
<h3 id="dyna-architecture">Dyna Architecture</h3>
<figure>
<img src="/assets/images/53.png" alt="" />
</figure>
<h3 id="dyna-q-algorithm">Dyna-Q Algorithm</h3>
<figure>
<img src="/assets/images/54.png" alt="" />
</figure>
<h2 id="monte-carlo-search">Monte-Carlo Search</h2>
<h3 id="simple-monte-carlo-search">Simple Monte-Carlo Search</h3>
<p>Given a model <span class="math inline">\(\mathcal M_v\)</span> and a <code>simulation policy</code> <span class="math inline">\(\pi\)</span>. For each action <span class="math inline">\(a\in\mathcal A\)</span>, simulate <span class="math inline">\(K\)</span> episodes from current (real) state <span class="math inline">\(s_t\)</span> <span class="math display">\[
\{\color{red}{s_t,a,}R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,...,S_T^k\}_{k=1}^K\sim \mathcal M_v,\pi
\]</span> Evaluate actions by mean return (<code>Monte-Carlo evaluation</code>) <span class="math display">\[
Q(\color{red}{s_t,a})={1\over K}\sum_{k=1}^K G_t 
\overset{P}\to
q_\pi(s_t,a)
\]</span> Select current (real) action with maximum value <span class="math display">\[
a_t=\mathop{argmax}\limits_{a\in\mathcal A} Q(s_t,a) \tag{*}\label{*}
\]</span></p>
<h3 id="monte-carlo-tree-search-evaluation">Monte-Carlo Tree Search (Evaluation)</h3>
<p>Given a model <span class="math inline">\(\mathcal M_v\)</span>. Simulate <span class="math inline">\(K\)</span> episodes from current state <span class="math inline">\(s_t\)</span> using current simulation policy <span class="math inline">\(\pi\)</span>. Build a search tree containing visited states and actions. <code>Evaluate</code> states <span class="math inline">\(Q(s,a)\)</span> by mean return of episodes from <span class="math inline">\(s,a\)</span> <span class="math display">\[
Q(\color{red}{s,a})={1\over N(s,a)}\sum_{k=1}^K\sum_{u=t}^T
1(S_u,A_u=s,a)G_u\overset P\to q_\pi(s,a)
\]</span> After search is finished, select current (real) action with maximum value in search tree (<span class="math inline">\(\ref{*}\)</span>).</p>
<h3 id="monte-carlo-tree-search-simulation">Monte-Carlo Tree Search (Simulation)</h3>
<p>In MCTS, the simulation policy <span class="math inline">\(\pi\)</span> <code>improves</code>. Each simulation consists of two phases (in-tree, out-of-tree)</p>
<ul>
<li><code>Tree policy</code> (improves): pick actions to maximise <span class="math inline">\(Q(S,A)\)</span></li>
<li><code>Default policy</code> (fixed): pick actions randomly</li>
</ul>
<p>Repeat (each simulation)</p>
<ul>
<li><code>Evaluate</code> states <span class="math inline">\(Q(S,A)\)</span> by Monte-Carlo evaluation</li>
<li><code>Improve</code> tree policy, e.g. by <span class="math inline">\(\epsilon\)</span>-greedy(Q)</li>
</ul>
<p><code>Monte-Carlo control</code> applied to <code>simulated experience</code>. Converges on the optimal search tree, <span class="math inline">\(Q(S,A)\to q_*(S,A)\)</span>.</p>
<h2 id="td-search">TD Search</h2>
<p>Simulate episodes from the current (real) state <span class="math inline">\(s_t\)</span>. Estimate action-value function <span class="math inline">\(Q(s,a)\)</span>. For each step of simulation, update action-values by Sarsa <span class="math display">\[
\Delta Q(S,A)=\alpha(R+\gamma Q(S&#39;,A&#39;)-Q(S,A))
\]</span> Select actions based on action-values <span class="math inline">\(Q(s,a)\)</span>, e.g. <span class="math inline">\(\epsilon\)</span>-greedy. May also use function approximate for <span class="math inline">\(Q\)</span>.</p>

	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="/categories.html#reinforcement learning-ref">
					reinforcement learning <span>(12)</span>
					
				</a></li>
			
		  
		</ul>
		  

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#reinforcement learning-ref">
					reinforcement learning <span>(12)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-12">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=Model-Based Reinforcement Learning"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/reinforcement%20learning/2016/06/23/policy-gradient.html" title="Policy Gradient">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="/reinforcement%20learning/2016/07/04/exploration-and-exploitation.html" title="Exploration and Exploitation">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>




		<footer>
			<hr/>
			<p>
				&copy; 2016 <a href="http://guoqiang.gq">sohero</a>.
                <script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_1258527085'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258527085%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
			</p>
		</footer>
	</div>

	<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
	<script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



