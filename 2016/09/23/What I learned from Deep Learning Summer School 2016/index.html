<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>What I learned from Deep Learning Summer School 2016 | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>What I learned from Deep Learning Summer School 2016</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-09-23</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/deep-learning/">deep learning</a></div></div></div><article><div class="container post"><p><a href="https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi" target="_blank" rel="external">原文链接</a></p>
<ul>
<li><strong>Essence of regularization</strong> Usually in practice, using a large model with regularization (e.g., injecting noise) works better than using a small fully parametric model without regularization.</li>
<li><strong>What non-linearity to choose for neurons?</strong> The rule of thumb to select non-linearity is to always start with ReLU (Rectified Linear Unit).</li>
<li><strong>Practical tips to train a neural network</strong>
<ul>
<li>Initialization: To break symmetry we use random initialization, for example see [Glorot &amp; Bengio, 2010].</li>
<li>Hyper-parameter selection: (a): Using grid search, i.e., trying all possible configurations of hyper-parameters. This is computationally expensive. (b): Using random search [Bergstra &amp; Bengio, 2012], i.e., specify a distribution over the values of each hyper-parameter and then sampling from each of them independently. (c): Bayesian optimization [Snoek, et al, NIPS 2012] which requires less number of guesses to get hyper-parameters.</li>
<li>Early stopping: Since it has zero cost, it is better to always do it.</li>
<li>Validation set choice: This can become very important. The validation set size should be large enough so that the model does not overfit on the validation set. This type of overfitting also depends on how many validation tests we run on the validation set.</li>
<li>Normalization: For real valued data, normalization speeds up the training.</li>
<li>Learning rate: Starting with a large learning rate and then decaying it or using methods with adaptive learning rates like Adagrad, RMSprop or Adam.</li>
<li>Gradient check: Very helpful for debugging the implementation of backprop. We simply compare the gradient with a finite difference approximation of it. Question: Can the finite difference approximation of the gradient replace backprop? No, because it is less numerically stable.</li>
<li>Always make sure the model overfits on a small dataset.</li>
<li>What to do if training is hard?: First, make sure backpropagation implementation is not buggy and the learning rate is not too large. Then, If it is underfitting, use better optimization methods, larger models, etc . If it is overfitting, use better regularization, e.g., unsupervised initialization, dropout, etc.</li>
<li>Batch Normalization [Loffe &amp; Szegedy, JMLR 2015]: Very helpful technique, which shows that the normalization at higher layers further improves the performance. It can be done in 4 steps: (a): Doing normalization for each hidden layer _before_applying non-linearity. (b): During training, mean and standard deviation are computed <em>for each minibatch</em>. (c): During backpropagation, we should take into account the normalization during forward pass. In other words, a scale and shift operation should be performed during backpropagation. Scale and shift parameters should also be learned because derivative with respect to hidden layers will also depend on them. (d): At the test time, global mean and standard deviation is used_NOT_the ones calculated for each minibatch.</li>
</ul></li>
</ul>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/09/23/What I learned from Deep Learning Summer School 2016/" data-title="What I learned from Deep Learning Summer School 2016" data-url="http://www.sgq.mobi/2016/09/23/What I learned from Deep Learning Summer School 2016/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>