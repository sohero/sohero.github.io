<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Deep Reinforcement Learning- Pong from Pixels | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Deep Reinforcement Learning- Pong from Pixels</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-08-26</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><p><strong>Policy Gradients (PG)</strong>, our favorite default choice for attacking RL problems at the moment. If you’re from outside of RL you might be curious why I’m not presenting DQN instead, which is an alternative and better-known RL algorithm, widely popularized by the ATARI game playing paper. It turns out that Q-Learning is not a great algorithm (you could say that DQN is so 2013 (okay I’m 50% joking)). In fact most people prefer to use Policy Gradients, including the authors of the original DQN paper who have shown Policy Gradients to work better than Q Learning when tuned well. PG is preferred because it is <strong>end-to-end</strong>: there’s an explicit policy and a principled approach that directly optimizes the expected reward.</p>
<h2 id="more-general-advantage-functions">More general advantage functions</h2>
<p>For example, suppose we compute <span class="math inline">\(R_t\)</span> for all of the 20,000 actions in the batch of 100 Pong game rollouts above. One good idea is to “standardize” these returns (e.g. subtract mean, divide by standard deviation) before we plug them into backprop. This way we’re always encouraging and discouraging roughly half of the performed actions. Mathematically you can also interpret these tricks as a way of controlling the variance of the policy gradient estimator. A more in-depth exploration can be found <a href="http://arxiv.org/abs/1506.02438" target="_blank" rel="external">here</a>.</p>
<h2 id="deriving-policy-gradients">Deriving Policy Gradients</h2>
<p>I’d like to also give a sketch of where Policy Gradients come from mathmatically. Policy Gradients are a special case of a more general <em>score function gradient estimator</em>. The general case is that when we have an expression of the form <span class="math inline">\(E_{x \sim p(x \mid \theta)} [f(x)]\)</span> - i.e. the expectation of some scalar valued score function <span class="math inline">\(f(x)\)</span> under some probability distribution <span class="math inline">\(p(x;\theta)\)</span> parameterized by some <span class="math inline">\(\theta\)</span>. Hint hint, <span class="math inline">\(f(x)\)</span> will become our reward function (or advantage function more generally) and <span class="math inline">\(p(x)\)</span> will be our policy network, which is really a model for <span class="math inline">\(p(a|I)\)</span>, giving a distribution over actions for any image <span class="math inline">\(I\)</span>. Then we are interested in finding how we should shift the distribution (through its parameters <span class="math inline">\(\theta\)</span>) to increase the scores of its samples, as judged by <span class="math inline">\(f\)</span> (i.e. how do we change the network’s parameters so that action samples get higher rewards). We have that: <span class="math display">\[
\begin{align}
\nabla_{\theta} E_x[f(x)] &amp;= \nabla_{\theta} \sum_x p(x) f(x) &amp; \text{definition of expectation} \\
&amp; = \sum_x \nabla_{\theta} p(x) f(x) &amp; \text{swap sum and gradient} \\
&amp; = \sum_x p(x) \frac{\nabla_{\theta} p(x)}{p(x)} f(x) &amp; \text{both multiply and divide by } p(x) \\
&amp; = \sum_x p(x) \nabla_{\theta} \log p(x) f(x) &amp; \text{use the fact that } \nabla_{\theta} \log(z) = \frac{1}{z} \nabla_{\theta} z \\
&amp; = E_x[f(x) \nabla_{\theta} \log p(x) ] &amp; \text{definition of expectation}
\end{align}
\]</span> To put this in English, we have some distribution <span class="math inline">\(p(x;\theta)\)</span> (I used shorthand <span class="math inline">\(p(x)\)</span> to reduce clutter) that we can sample from (e.g. this could be a gaussian). For each sample we can also evaluate the score function <span class="math inline">\(f\)</span> which takes the sample and gives us some scalar-valued score. This equation is telling us how we should shift the distribution (through its parameters <span class="math inline">\(\theta\)</span>) if we wanted its samples to achieve higher scores, as judged by <span class="math inline">\(f\)</span>. In particular, it says that look: draw some samples <span class="math inline">\(x\)</span>, evaluate their scores <span class="math inline">\(f(x)\)</span>, and for each <span class="math inline">\(x\)</span> also evaluate the second term <span class="math inline">\(\nabla_{\theta} \log p(x;\theta)\)</span>. What is this second term? It’s a vector - the gradient that’s giving us the direction in the parameter space that would lead to increase of the probability assigned to an <span class="math inline">\(x\)</span>. In other words if we were to nudge <span class="math inline">\(\theta\)</span> in the direction of <span class="math inline">\(\nabla_{\theta} \log p(x;\theta)\)</span> we would see the new probability assigned to some <span class="math inline">\(x\)</span> slightly increase. If you look back at the formula, it’s telling us that we should take this direction and muliply onto it the scalar-valued score <span class="math inline">\(f(x)\)</span>. This will make it so that samples that have a higher score will “tug” on the probability density stronger than the samples that have lower score, so if we were to do an update based on several samples from <span class="math inline">\(p\)</span> the probability density would shift around in the direction of higher scores, making highly-socring samples more likely.</p>
<figure>
<img src="/images/2016-08-26_104141.png">
</figure>
<h2 id="what-isnt-happening">What isn’t happening</h2>
<p>I’d like to also emphasize the point that, conversely, there are many games where Policy Gradients would quite easily defeat a human. In particular, anything with frequent reward signals that requires precise play, fast reflexes, and not too much long-term planning would be ideal, as these short-term correlations between rewards and actions can be easily “noticed” by the approach, and the execution meticulously perfected by the policy.</p>
<h2 id="conclusions">Conclusions</h2>
<h3 id="on-advancing-ai">On advancing AI</h3>
<p>We saw that the algorithm works through a brute-force search where you jitter around randomly at first and must accidentally stumble into rewarding situations at least once, and ideally often and repeatedly before the policy distribution shifts its parameters to repeat the responsible actions. We also saw that humans approach these problems very differently, in what feels more like rapid abstract model building - something we have barely even scratched the surface of in research (although many people are trying). Since these abstract models are very difficult (if not impossible) to explicitly annotate, this is also why there is so much interest recently in (unsupervised) generative models and program induction.</p>
<h3 id="on-use-in-complex-robotics-settings">On use in complex robotics settings</h3>
<p>The algorithm does not scale naively to settings where huge amounts of exploration are difficult to obtain. For instance, in robotic settings one might have a single (or few) robots, interacting with the world in real time. This prohibits naive applications of the algorithm as I presented it in this post. One related line of work intended to mitigate this problem is <a href="http://jmlr.org/proceedings/papers/v32/silver14.pdf" target="_blank" rel="external">deterministic policy gradients</a> - instead of requiring samples from a stochastic policy and encouraging the ones that get higher scores, the approach uses a deterministic policy and gets the gradient information directly from a second network (called a <em>critic</em>) that models the score function. This approach can in principle be much more efficient in settings with very high-dimensional actions where sampling actions provides poor coverage, but so far seems empirically slightly finicky to get working. Another related approach is to scale up robotics, as we’re starting to see with <a href="http://googleresearch.blogspot.com/2016/03/deep-learning-for-robots-learning-from.html" target="_blank" rel="external">Google’s robot arm farm</a>, or perhaps even <a href="http://qz.com/694520/tesla-has-780-million-miles-of-driving-data-and-adds-another-million-every-10-hours/" target="_blank" rel="external">Tesla’s Model S + Autopilot</a>.</p>
<p>There is also a line of work that tries to make the search process less hopeless by adding additional supervision. In many practical cases, for instance, one can obtain expert trajectories from a human. For example <a href="https://deepmind.com/alpha-go" target="_blank" rel="external">AlphaGo</a>first uses supervised learning to predict human moves from expert Go games and the resulting human mimicking policy is later finetuned with policy gradients on the “real” objective of winning the game. In some cases one might have fewer expert trajectories (e.g. from <a href="https://www.youtube.com/watch?v=kZlg0QvKkQQ" target="_blank" rel="external">robot teleoperation</a>) and there are techniques for taking advantage of this data under the umbrella of <a href="http://ai.stanford.edu/~pabbeel//thesis/thesis.pdf" target="_blank" rel="external">apprenticeship learning</a>. Finally, if no supervised data is provided by humans it can also be in some cases computed with expensive optimization techniques, e.g. by <a href="http://people.eecs.berkeley.edu/~igor.mordatch/policy/index.html" target="_blank" rel="external">trajectory optimization</a> in a known dynamics model (such as <span class="math inline">\(F=ma\)</span> in a physical simulator), or in cases where one learns an approximate local dynamics model (as seen in very promising framework of <a href="http://arxiv.org/abs/1504.00702" target="_blank" rel="external">Guided Policy Search</a>).</p>
<h3 id="on-using-pg-in-practice">On using PG in practice</h3>
<p>As a last note, I’d like to do something I wish I had done in my RNN blog post. I think I may have given the impression that RNNs are magic and automatically do arbitrary sequential problems. The truth is that getting these models to work can be tricky, requires care and expertise, and in many cases could also be an overkill, where simpler methods could get you 90%+ of the way there. The same goes for Policy Gradients. They are not automatic: You need a lot of samples, it trains forever, it is difficult to debug when it doesn’t work. One should always try a BB gun before reaching for the Bazooka. In the case of Reinforcement Learning for example, one strong baseline that should always be tried first is the <a href="https://en.wikipedia.org/wiki/Cross-entropy_method" target="_blank" rel="external">cross-entropy method (CEM)</a>, a simple stochastic hill-climbing “guess and check” approach inspired loosely by evolution. And if you insist on trying out Policy Gradients for your problem make sure you pay close attention to the <em>tricks</em> section in papers, start simple first, and use a variation of PG called <a href="https://arxiv.org/abs/1502.05477" target="_blank" rel="external">TRPO</a>, which almost always works better and more consistently than vanilla PG <a href="http://arxiv.org/abs/1604.06778" target="_blank" rel="external">in practice</a>. The core idea is to avoid parameter updates that change your policy too much, as enforced by a constraint on the KL divergence between the distributions predicted by the old and the new policy on a batch of data (instead of conjugate gradients the simplest instantiation of this idea could be implemented by doing a line search and checking the KL along the way).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div></pre></td><td class="code"><pre><div class="line"><span class="string">""" Trains an agent with (stochastic) Policy Gradients on Pong. Uses OpenAI Gym. """</span></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> cPickle <span class="keyword">as</span> pickle</div><div class="line"><span class="keyword">import</span> gym</div><div class="line"></div><div class="line"><span class="comment"># hyperparameters</span></div><div class="line">H = <span class="number">200</span> <span class="comment"># number of hidden layer neurons</span></div><div class="line">batch_size = <span class="number">10</span> <span class="comment"># every how many episodes to do a param update?</span></div><div class="line">learning_rate = <span class="number">1e-4</span></div><div class="line">gamma = <span class="number">0.99</span> <span class="comment"># discount factor for reward</span></div><div class="line">decay_rate = <span class="number">0.99</span> <span class="comment"># decay factor for RMSProp leaky sum of grad^2</span></div><div class="line">resume = <span class="keyword">False</span> <span class="comment"># resume from previous checkpoint?</span></div><div class="line">render = <span class="keyword">False</span></div><div class="line"></div><div class="line"><span class="comment"># model initialization</span></div><div class="line">D = <span class="number">80</span> * <span class="number">80</span> <span class="comment"># input dimensionality: 80x80 grid</span></div><div class="line"><span class="keyword">if</span> resume:</div><div class="line">  model = pickle.load(open(<span class="string">'save.p'</span>, <span class="string">'rb'</span>))</div><div class="line"><span class="keyword">else</span>:</div><div class="line">  model = &#123;&#125;</div><div class="line">  model[<span class="string">'W1'</span>] = np.random.randn(H,D) / np.sqrt(D) <span class="comment"># "Xavier" initialization</span></div><div class="line">  model[<span class="string">'W2'</span>] = np.random.randn(H) / np.sqrt(H)</div><div class="line"></div><div class="line">grad_buffer = &#123; k : np.zeros_like(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> model.iteritems() &#125; <span class="comment"># update buffers that add up gradients over a batch</span></div><div class="line">rmsprop_cache = &#123; k : np.zeros_like(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> model.iteritems() &#125; <span class="comment"># rmsprop memory</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span> </div><div class="line">  <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-x)) <span class="comment"># sigmoid "squashing" function to interval [0,1]</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepro</span><span class="params">(I)</span>:</span></div><div class="line">  <span class="string">""" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector """</span></div><div class="line">  I = I[<span class="number">35</span>:<span class="number">195</span>] <span class="comment"># crop</span></div><div class="line">  I = I[::<span class="number">2</span>,::<span class="number">2</span>,<span class="number">0</span>] <span class="comment"># downsample by factor of 2</span></div><div class="line">  I[I == <span class="number">144</span>] = <span class="number">0</span> <span class="comment"># erase background (background type 1)</span></div><div class="line">  I[I == <span class="number">109</span>] = <span class="number">0</span> <span class="comment"># erase background (background type 2)</span></div><div class="line">  I[I != <span class="number">0</span>] = <span class="number">1</span> <span class="comment"># everything else (paddles, ball) just set to 1</span></div><div class="line">  <span class="keyword">return</span> I.astype(np.float).ravel() <span class="comment"># flattened array</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">discount_rewards</span><span class="params">(r)</span>:</span></div><div class="line">  <span class="string">""" take 1D float array of rewards and compute discounted reward """</span></div><div class="line">  discounted_r = np.zeros_like(r)</div><div class="line">  running_add = <span class="number">0</span></div><div class="line">  <span class="keyword">for</span> t <span class="keyword">in</span> reversed(xrange(<span class="number">0</span>, r.size)):</div><div class="line">    <span class="keyword">if</span> r[t] != <span class="number">0</span>: running_add = <span class="number">0</span> <span class="comment"># reset the sum, since this was a game boundary (pong specific!)</span></div><div class="line">    running_add = running_add * gamma + r[t]</div><div class="line">    discounted_r[t] = running_add</div><div class="line">  <span class="keyword">return</span> discounted_r</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_forward</span><span class="params">(x)</span>:</span></div><div class="line">  h = np.dot(model[<span class="string">'W1'</span>], x)</div><div class="line">  h[h&lt;<span class="number">0</span>] = <span class="number">0</span> <span class="comment"># ReLU nonlinearity</span></div><div class="line">  logp = np.dot(model[<span class="string">'W2'</span>], h)</div><div class="line">  p = sigmoid(logp)</div><div class="line">  <span class="keyword">return</span> p, h <span class="comment"># return probability of taking action 2, and hidden state</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">policy_backward</span><span class="params">(eph, epdlogp)</span>:</span></div><div class="line">  <span class="string">""" backward pass. (eph is array of intermediate hidden states) """</span></div><div class="line">  dW2 = np.dot(eph.T, epdlogp).ravel()</div><div class="line">  dh = np.outer(epdlogp, model[<span class="string">'W2'</span>])</div><div class="line">  dh[eph &lt;= <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># backpro prelu</span></div><div class="line">  dW1 = np.dot(dh.T, epx)</div><div class="line">  <span class="keyword">return</span> &#123;<span class="string">'W1'</span>:dW1, <span class="string">'W2'</span>:dW2&#125;</div><div class="line"></div><div class="line">env = gym.make(<span class="string">"Pong-v0"</span>)</div><div class="line">observation = env.reset()</div><div class="line">prev_x = <span class="keyword">None</span> <span class="comment"># used in computing the difference frame</span></div><div class="line">xs,hs,dlogps,drs = [],[],[],[]</div><div class="line">running_reward = <span class="keyword">None</span></div><div class="line">reward_sum = <span class="number">0</span></div><div class="line">episode_number = <span class="number">0</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">  <span class="keyword">if</span> render: env.render()</div><div class="line"></div><div class="line">  <span class="comment"># preprocess the observation, set input to network to be difference image</span></div><div class="line">  cur_x = prepro(observation)</div><div class="line">  x = cur_x - prev_x <span class="keyword">if</span> prev_x <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span> <span class="keyword">else</span> np.zeros(D)</div><div class="line">  prev_x = cur_x</div><div class="line"></div><div class="line">  <span class="comment"># forward the policy network and sample an action from the returned probability</span></div><div class="line">  aprob, h = policy_forward(x)</div><div class="line">  action = <span class="number">2</span> <span class="keyword">if</span> np.random.uniform() &lt; aprob <span class="keyword">else</span> <span class="number">3</span> <span class="comment"># roll the dice!</span></div><div class="line"></div><div class="line">  <span class="comment"># record various intermediates (needed later for backprop)</span></div><div class="line">  xs.append(x) <span class="comment"># observation</span></div><div class="line">  hs.append(h) <span class="comment"># hidden state</span></div><div class="line">  y = <span class="number">1</span> <span class="keyword">if</span> action == <span class="number">2</span> <span class="keyword">else</span> <span class="number">0</span> <span class="comment"># a "fake label"</span></div><div class="line">  dlogps.append(y - aprob) <span class="comment"># grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)</span></div><div class="line"></div><div class="line">  <span class="comment"># step the environment and get new measurements</span></div><div class="line">  observation, reward, done, info = env.step(action)</div><div class="line">  reward_sum += reward</div><div class="line"></div><div class="line">  drs.append(reward) <span class="comment"># record reward (has to be done after we call step() to get reward for previous action)</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span> done: <span class="comment"># an episode finished</span></div><div class="line">    episode_number += <span class="number">1</span></div><div class="line"></div><div class="line">    <span class="comment"># stack together all inputs, hidden states, action gradients, and rewards for this episode</span></div><div class="line">    epx = np.vstack(xs)</div><div class="line">    eph = np.vstack(hs)</div><div class="line">    epdlogp = np.vstack(dlogps)</div><div class="line">    epr = np.vstack(drs)</div><div class="line">    xs,hs,dlogps,drs = [],[],[],[] <span class="comment"># reset array memory</span></div><div class="line"></div><div class="line">    <span class="comment"># compute the discounted reward backwards through time</span></div><div class="line">    discounted_epr = discount_rewards(epr)</div><div class="line">    <span class="comment"># standardize the rewards to be unit normal (helps control the gradient estimator variance)</span></div><div class="line">    discounted_epr -= np.mean(discounted_epr)</div><div class="line">    discounted_epr /= np.std(discounted_epr)</div><div class="line"></div><div class="line">    epdlogp *= discounted_epr <span class="comment"># modulate the gradient with advantage (PG magic happens right here.)</span></div><div class="line">    grad = policy_backward(eph, epdlogp)</div><div class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> model: grad_buffer[k] += grad[k] <span class="comment"># accumulate grad over batch</span></div><div class="line"></div><div class="line">    <span class="comment"># perform rmsprop parameter update every batch_size episodes</span></div><div class="line">    <span class="keyword">if</span> episode_number % batch_size == <span class="number">0</span>:</div><div class="line">      <span class="keyword">for</span> k,v <span class="keyword">in</span> model.iteritems():</div><div class="line">        g = grad_buffer[k] <span class="comment"># gradient</span></div><div class="line">        rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (<span class="number">1</span> - decay_rate) * g**<span class="number">2</span></div><div class="line">        model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + <span class="number">1e-5</span>)</div><div class="line">        grad_buffer[k] = np.zeros_like(v) <span class="comment"># reset batch gradient buffer</span></div><div class="line"></div><div class="line">    <span class="comment"># boring book-keeping</span></div><div class="line">    running_reward = reward_sum <span class="keyword">if</span> running_reward <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> running_reward * <span class="number">0.99</span> + reward_sum * <span class="number">0.01</span></div><div class="line">    <span class="keyword">print</span> <span class="string">'resetting env. episode reward total was %f. running mean: %f'</span> % (reward_sum, running_reward)</div><div class="line">    <span class="keyword">if</span> episode_number % <span class="number">100</span> == <span class="number">0</span>: pickle.dump(model, open(<span class="string">'save.p'</span>, <span class="string">'wb'</span>))</div><div class="line">    reward_sum = <span class="number">0</span></div><div class="line">    observation = env.reset() <span class="comment"># reset env</span></div><div class="line">    prev_x = <span class="keyword">None</span></div><div class="line"></div><div class="line">  <span class="keyword">if</span> reward != <span class="number">0</span>: <span class="comment"># Pong has either +1 or -1 reward exactly when game ends.</span></div><div class="line">    <span class="keyword">print</span> (<span class="string">'ep %d: game finished, reward: %f'</span> % (episode_number, reward)) + (<span class="string">''</span> <span class="keyword">if</span> reward == <span class="number">-1</span> <span class="keyword">else</span> <span class="string">' !!!!!!!!'</span>)</div></pre></td></tr></table></figure>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/08/26/Deep Reinforcement Learning- Pong from Pixels/" data-title="Deep Reinforcement Learning- Pong from Pixels" data-url="http://www.sgq.mobi/2016/08/26/Deep Reinforcement Learning- Pong from Pixels/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>