<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Introduction to Reinforcement Learning with Function Approximation by Rich Sutton | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Introduction to Reinforcement Learning with Function Approximation by Rich Sutton</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><h2 id="the-problem-of-instability">The problem of instability</h2>
<p>The risk of divergence arises whenever we combine three things:</p>
<ul>
<li><code>1. Function approximation</code>
<ul>
<li>significantly generalizing from large numbers of examples</li>
</ul></li>
<li><code>2. Bootstrapping</code>
<ul>
<li>learning value estimates from other value estimates, as in dynamic programming and temporal-difference learning</li>
</ul></li>
<li><code>3. Off-policy learning</code>
<ul>
<li>learning about a policy from data not due to that policy, as in Q-learning, where we learn about the greedy policy from data with a necessarily more exploratory policy</li>
</ul></li>
</ul>
<p><strong>Any two without the third is ok.</strong></p>
<h3 id="can-we-do-without-bootstrapping">Can we do without bootstrapping?</h3>
<ul>
<li>Bootstrapping is critical to the <code>computational efficienty of DP</code></li>
<li>Bootstrapping is critical to the <code>data efficiency of TD</code> methods</li>
<li>On the other hand, bootstrapping <code>introduces bias</code>, which harms the asymptotic performance of approximate methods</li>
<li>The <code>degree of bootstrapping</code> can be finely controlled via the <span class="math inline">\(\lambda\)</span> parameter, from <span class="math inline">\(\lambda=0\)</span> (full bootstrapping) to <span class="math inline">\(\lambda=1\)</span> (no bootstrapping)</li>
</ul>
<blockquote>
<p>4 examples of the effect of bootstrapping suggest that <span class="math inline">\(\lambda=1\)</span> (no bootstrapping) is a very poor choice.</p>
</blockquote>
<h3 id="other-ways-to-survive-the-deadly-triad">Other ways to survive the deadly triad</h3>
<ul>
<li>Use high <span class="math inline">\(\lambda\)</span>. Use good features</li>
<li>Recent results suggest that <code>replay</code> and <code>more stable targets</code> (e.g., <code>Double Q-learning</code>, van Hasselt 2010) may help, but it is too soon to be sure</li>
<li>Use <code>least-squares methods</code> like <code>off-policy LSTD(λ)</code> (Yu 2010, Mahmood et al. 2015). Such methods (Bradtke &amp; Barto 1996, Boyan 2000) easily survive the triad, but their computational costs scale with the square of the number of parameters</li>
<li>Try the <code>new true-gradient RL methods</code> (<code>Gradient-TD</code> and <code>proximal-gradientTD</code>) developed by Maei (2011) and Mahadevan (2015) et al. These seem to me to be the best attempts to make TD methods with the robust convergence properties of stochastic gradient descent. <code>Residual gradient methods</code> (Baird 1999) are also true gradient methods, but optimize a poor objective, or can’t learn purely from data (double sampling). These and other methods based on the Bellman error/residual are not recommended</li>
<li>Try the even newer <code>Emphatic-TD methods</code> (Sutton, White &amp; Mahmood 2015, Yu 2015). These semi-gradient methods attain stability through an extension of the early on-policy theorems</li>
</ul>
<h2 id="why-approximate-policies-rather-than-values">Why approximate policies rather than values?</h2>
<ul>
<li>In many problems, the policy is simpler than the value function</li>
<li>In many problems, the optimal policy is stochastic</li>
<li>To enable smoother change in policies</li>
<li>To avoid a search on every step (the max)</li>
<li>To better relate to biology</li>
</ul>
<h2 id="we-should-never-discount-when-optimizing-approximate-policies">We should never discount when optimizing approximate policies!</h2>
<ul>
<li>It breaks the definition of an optimal policy</li>
<li>With approximation, <em>the optimal policy</em> is no longer representable</li>
<li>There is no way to rank the remaining policies</li>
<li>Different policies will be best in different states</li>
<li>Instead, you must say what states you care about or else use average reward (which you should probably do anyway)</li>
</ul>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/07/14/Introduction to Reinforcement Learning with Function Approximation by Rich Sutton/" data-title="Introduction to Reinforcement Learning with Function Approximation by Rich Sutton" data-url="http://www.sgq.mobi/2016/07/14/Introduction to Reinforcement Learning with Function Approximation by Rich Sutton/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>