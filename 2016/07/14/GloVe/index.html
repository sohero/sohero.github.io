<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>GloVe | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>GloVe</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/deep-learning/">deep learning</a>/<a class="post-tag-link" href="/tags/nlp/">nlp</a></div></div></div><article><div class="container post"><blockquote>
<p>Glove uses the matrix of word-word co-occurrence to produce the word vector, while word2vec using the local window context.</p>
</blockquote>
<p>The <strong>statistics of word occurrences</strong> in a corpus is the <strong>primary source</strong> of information available to all unsupervised methods for <strong>learning word representations</strong>, and although many such methods now exist, the question still remains as to <strong>how meaning is generated from these statistics</strong>, and <strong>how the resulting word vectors might represent that meaning</strong>.</p>
<p><span class="math inline">\(X\)</span> denote the matrix of word-word co-occurrence counts, the entries <span class="math inline">\(X_{ij}\)</span> tabulate the number of times word <span class="math inline">\(j\)</span> occurs in the context of word <span class="math inline">\(i\)</span>. Let <span class="math inline">\(X_i=\sum_k X_{ik}\)</span> be the nubmer of times any word appears in the context of word <span class="math inline">\(i\)</span>. Let <span class="math inline">\(P_{ij}=P(j|i)={X_{ij} \over X_i}\)</span> be the probability that word <span class="math inline">\(j\)</span> appear in the context of word <span class="math inline">\(i\)</span>.</p>
<p>We take <span class="math inline">\(i=ice\)</span> and <span class="math inline">\(j=steam\)</span>, for words <span class="math inline">\(k\)</span> related to <span class="math inline">\(i\)</span> but not <span class="math inline">\(j\)</span>, say <span class="math inline">\(k=solid\)</span>, we expect the ratio <span class="math inline">\(P_{ik} \over P_{jk}\)</span> will be large. Similarly, say <span class="math inline">\(k=gas\)</span>, the ratio should be small. For words <span class="math inline">\(k\)</span> like <span class="math inline">\(water\)</span> or <span class="math inline">\(fashion\)</span>, that are either related to both <span class="math inline">\(ice\)</span> and <span class="math inline">\(steam\)</span>, or to neither, the ratio should be close to <strong>one</strong>.</p>
<p>Noting that the ratio <span class="math inline">\(P_{ik} \over P_{jk}\)</span> depends on three words <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span>, the most general model takes the form: <span class="math display">\[
F(w_i,w_j,\tilde w_k)={P_{ik} \over P_{jk}}\tag{1}
\]</span> where <span class="math inline">\(w\in\Bbb R^d\)</span> are word vectors and <span class="math inline">\(\tilde w\in\Bbb R^d\)</span> are separate context word vectors. <span class="math inline">\(F\)</span> may depend on some as-of-yet unspecified parameters, and the number of possibilites for <span class="math inline">\(F\)</span> is vast.</p>
<p>Since <strong>vector spaces are inherently linear structures</strong>, the most natural way to do this is with vector differences. With this aim, we can restrict our consideration to those functions <span class="math inline">\(F\)</span> that depend only on the difference of the two target words, modifying Eqn.(1) to: <span class="math display">\[
F(w_i-w_j,\tilde w_k)={P_{ik}\over P_{jk}}\tag{2}
\]</span> note that the arguments of <span class="math inline">\(F\)</span> are vectors while the right-hand side is a scalar. While <span class="math inline">\(F\)</span> could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. To avoid this issue, we can first take the dot product of the arguments: <span class="math display">\[
F((w_i-w_j)^T\tilde w_k)={P_{ik}\over P_{jk}}\tag{3}
\]</span> which prevents <span class="math inline">\(F\)</span> from mixing the vector dimensions in undesirable ways.</p>
<p>Note that for word-word co-occurrence matrices, the distinction between a word a context word is arbitrary and that we are free to exchange the two roles. To do so consistently, we must not only exchange <span class="math inline">\(w\leftrightarrow \tilde w\)</span> but also <span class="math inline">\(X\leftrightarrow X^T\)</span>. Our final model should be invariant under this relabeling, but Eqn.(3) is not. However, the <strong>symmetry</strong> can be restored in two steps. First, we require that <span class="math inline">\(F\)</span> be a homomorphism between the group <span class="math inline">\((\Bbb R, +)\)</span> and <span class="math inline">\((\Bbb R_{&gt;0}, \times)\)</span>, i.e., <span class="math display">\[
F((w_i-w_j)^T\tilde w_k)={F(w^T_i\tilde w_k)\over F(w^T_j\tilde w_k)}\tag{4}
\]</span> which, by Eqn.(3), is solved by: <span class="math display">\[
F(w^T_i\tilde w_k)=P_{ik}={X_{ik}\over X_i}\tag{5}
\]</span> The solution to Eqn.(4) is <span class="math inline">\(F=exp\)</span>:</p>
<p><span class="math display">\[
e^{(w_i-w_j)^T\tilde w_k}={e^{w^T_i\tilde w_k}\over e^{w^T_j\tilde w_k}}
\]</span></p>
<p>or:</p>
<p><span class="math display">\[
w^T_i\tilde w_k=log(P_{ik})=log(X_{ik})-log(X_i)\tag{6}
\]</span> Note that Eqn.(6) would exhibit the exchange symmetry if not for the <span class="math inline">\(log(X_i)\)</span> on the right-hand side. However, this term is independent for <span class="math inline">\(k\)</span> so it can be absorbed into a bias <span class="math inline">\(b_i\)</span> for <span class="math inline">\(w_i\)</span>. Finally, adding an additional bias <span class="math inline">\(\tilde b_k\)</span> for <span class="math inline">\(\tilde w_k\)</span> restores the symmetry: <span class="math display">\[
w^T_i\tilde w_k+b_i+\tilde b_k=log(X_{ik})\tag{7}
\]</span> Eqn.(7) is a drastic simplification over Eqn.(1), but it is actually ill-defined since the logarithm diverges whenever its argument is zero. One resolution to this issue is to include an additive shift in the logarithm, <span class="math inline">\(log(X_{ik})\rightarrow log(1+X_{ik})\)</span>, which maintains the sparsity of <span class="math inline">\(X\)</span> while avoiding the divergences.</p>
<p>A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never. Such rare co-occurrences are noisy and carry less information than the more frequent ones —— yet even just the zero entries account for 75-95% of the data in <span class="math inline">\(X\)</span>, depending on the vocabulary size and corpus.</p>
<p>They propose a new weighted least squares regression model that addresses these problems. Casting Eqn.(7) as a <strong>least squares problem</strong> and introducing a weighting function <span class="math inline">\(f(X_{ij})\)</span> into the cost function gives us the model: <span class="math display">\[
J=\sum^V_{i,j=1} f(X_{ij})(w^T_i\tilde w_j+b_i+\tilde b_j-logX_{ij})^2\tag{8}
\]</span> where <span class="math inline">\(V\)</span> is the size of the vocabulary. The weighting function should obey the following properties:</p>
<ul>
<li><span class="math inline">\(f(0)=0\)</span>. If <span class="math inline">\(f\)</span> is viewed as a continuous function, it should vanish as <span class="math inline">\(x\rightarrow 0\)</span> fast enough that the <span class="math inline">\(\lim_{x\to 0}f(x)log^2x\)</span> is finite.</li>
<li><span class="math inline">\(f(x)\)</span> should be non-decreasing so that rare co-occurrences are not overweighted.</li>
<li><span class="math inline">\(f(x)\)</span> should be relatively small for large values of <span class="math inline">\(x\)</span>, so that frequent co-occurrences are not overweighted.</li>
</ul>
<p><span class="math display">\[
f(x)=\begin{cases}
(x/x_{max})^\alpha &amp; \text{if $x&lt;x_{max}$} \\
1 &amp; \text{otherwise}
\end{cases}\tag{9}
\]</span></p>
<figure>
<img src="/images/35.png">
</figure>
<p>The performance of the model depends weakly on the cutoff, which we fix to <span class="math inline">\(x_{max}=100\)</span> for all our experiments. We found that <span class="math inline">\(\alpha=3/4\)</span> gives a modest improvement over a linear version with <span class="math inline">\(\alpha=1\)</span>. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolove et al., 2013a).</p>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/07/14/GloVe/" data-title="GloVe" data-url="http://www.sgq.mobi/2016/07/14/GloVe/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>