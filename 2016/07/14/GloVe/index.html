<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>GloVe | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>GloVe</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/deep-learning/">deep learning</a>/<a class="post-tag-link" href="/tags/nlp/">nlp</a></div></div></div><article><div class="container post"><blockquote>
<p>Glove uses the matrix of word-word co-occurrence to produce the word vector, while word2vec using the local window context.</p>
</blockquote>
<p>The <strong>statistics of word occurrences</strong> in a corpus is the <strong>primary source</strong> of information available to all unsupervised methods for <strong>learning word representations</strong>, and although many such methods now exist, the question still remains as to <strong>how meaning is generated from these statistics</strong>, and <strong>how the resulting word vectors might represent that meaning</strong>.</p>
<p>$X$ denote the matrix of word-word co-occurrence counts, the entries $X_{ij}$ tabulate the number of times word $j$ occurs in the context of word $i$. Let $X_i=\sum<em>k X</em>{ik}$ be the nubmer of times any word appears in the context of word $i$. Let $P<em>{ij}=P(j|i)={X</em>{ij} \over X_i}$ be the probability that word $j$ appear in the context of word $i$.</p>
<p>We take $i=ice$ and $j=steam$, for words $k$ related to $i$ but not $j$, say $k=solid$, we expect the ratio $P<em>{ik} \over P</em>{jk}$ will be large. Similarly, say $k=gas$, the ratio should be small. For words $k$ like<br>$water$ or $fashion$, that are either related to both $ice$ and $steam$, or to neither, the ratio should be close to <strong>one</strong>.</p>
<p>Noting that the ratio $P<em>{ik} \over P</em>{jk}$ depends on three words $i$, $j$ and $k$, the most general model takes the form:<br>$$<br>F(w_i,w_j,\tilde w<em>k)={P</em>{ik} \over P_{jk}}\tag{1}<br>$$<br>where $w\in\Bbb R^d$ are word vectors and $\tilde w\in\Bbb R^d$ are separate context word vectors. $F$ may depend on some as-of-yet unspecified parameters, and the number of possibilites for $F$ is vast.</p>
<p>Since <strong>vector spaces are inherently linear structures</strong>, the most natural way to do this is with vector differences. With this aim, we can restrict our consideration to those functions $F$ that depend only on the difference of the two target words, modifying Eqn.(1) to:<br>$$<br>F(w_i-w_j,\tilde w<em>k)={P</em>{ik}\over P_{jk}}\tag{2}<br>$$<br>note that the arguments of $F$ are vectors while the right-hand side is a scalar. While $F$ could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. To avoid this issue, we can first take the dot product of the arguments:<br>$$<br>F((w_i-w_j)^T\tilde w<em>k)={P</em>{ik}\over P_{jk}}\tag{3}<br>$$<br>which prevents $F$ from mixing the vector dimensions in undesirable ways.</p>
<p>Note that for word-word co-occurrence matrices, the distinction between a word a context word is arbitrary and that we are free to exchange the two roles. To do so consistently, we must not only exchange $w\leftrightarrow \tilde w$ but also $X\leftrightarrow X^T$. Our final model should be invariant under this relabeling, but Eqn.(3) is not. However, the <strong>symmetry</strong> can be restored in two steps. First, we require that $F$ be a homomorphism between the group $(\Bbb R, +)$ and $(\Bbb R_{&gt;0}, \times)$, i.e.,<br>$$<br>F((w_i-w_j)^T\tilde w_k)={F(w^T_i\tilde w_k)\over F(w^T_j\tilde w_k)}\tag{4}<br>$$<br>which, by Eqn.(3), is solved by:<br>$$<br>F(w^T_i\tilde w<em>k)=P</em>{ik}={X_{ik}\over X_i}\tag{5}<br>$$<br>The solution to Eqn.(4) is $F=exp$:</p>
<p>$$<br>e^{(w_i-w_j)^T\tilde w_k}={e^{w^T_i\tilde w_k}\over e^{w^T_j\tilde w_k}}<br>$$</p>
<p>or:</p>
<p>$$<br>w^T_i\tilde w<em>k=log(P</em>{ik})=log(X_{ik})-log(X_i)\tag{6}<br>$$<br>Note that Eqn.(6) would exhibit the exchange symmetry if not for the $log(X_i)$ on the right-hand side. However, this term is independent for $k$ so it can be absorbed into a bias $b_i$ for $w_i$. Finally, adding an additional bias $\tilde b_k$ for $\tilde w_k$ restores the symmetry:<br>$$<br>w^T_i\tilde w_k+b_i+\tilde b<em>k=log(X</em>{ik})\tag{7}<br>$$<br>Eqn.(7) is a drastic simplification over Eqn.(1), but it is actually ill-defined since the logarithm diverges whenever its argument is zero. One resolution to this issue is to include an additive shift in the logarithm, $log(X<em>{ik})\rightarrow log(1+X</em>{ik})$, which maintains the sparsity of $X$ while avoiding the divergences.</p>
<p>A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never. Such rare co-occurrences are noisy and carry less information than the more frequent ones —— yet even just the zero entries account for 75-95% of the data in $X$, depending on the vocabulary size and corpus.</p>
<p>They propose a new weighted least squares regression model that addresses these problems. Casting Eqn.(7) as a <strong>least squares problem</strong> and introducing a weighting function $f(X<em>{ij})$ into the cost function gives us the model:<br>$$<br>J=\sum^V</em>{i,j=1} f(X_{ij})(w^T_i\tilde w_j+b_i+\tilde b<em>j-logX</em>{ij})^2\tag{8}<br>$$<br>where $V$ is the size of the vocabulary. The weighting function should obey the following properties:</p>
<ul>
<li>$f(0)=0$. If $f$ is viewed as a continuous function, it should vanish as $x\rightarrow 0$ fast enough that the $\lim_{x\to 0}f(x)log^2x$ is finite.</li>
<li>$f(x)$ should be non-decreasing so that rare co-occurrences are not overweighted.</li>
<li>$f(x)$ should be relatively small for large values of $x$, so that frequent co-occurrences are not overweighted.</li>
</ul>
<p>$$<br>f(x)=\begin{cases}<br>(x/x<em>{max})^\alpha &amp; \text{if $x&lt;x</em>{max}$} \<br>1 &amp; \text{otherwise}<br>\end{cases}\tag{9}<br>$$</p>
<p><img src="/images/35.png" alt=""></p>
<p>The performance of the model depends weakly on the cutoff, which we fix to $x_{max}=100$ for all our experiments. We found that $\alpha=3/4$ gives a modest improvement over a linear version with $\alpha=1$. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolove et al., 2013a).</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>