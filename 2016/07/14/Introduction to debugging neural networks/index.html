<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Introduction to debugging neural networks | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Introduction to debugging neural networks</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/deep-learning/">deep learning</a>/<a class="post-tag-link" href="/tags/tricks/">tricks</a></div></div></div><article><div class="container post"><p><a class="btn btn-default" href="http://russellsstewart.com/notes/0.html" target="_blank">Link</a></p>
<p>The following advice is targeted at beginners to neural networks, and is based<br>on my experience giving advice to neural net newcomers in industry and at<br>Stanford. Neural nets are fundamentally harder to debug than most programs,<br>because most neural net bugs don’t result in type errors or runtime errors.<br>They just cause poor convergence. Especially when you’re new, this can be very<br>frustrating! But an experienced neural net trainer will be able to<br>systematically overcome the difficulty in spite of the ubiquitous and<br>seemingly ambiguous error message:</p>
<blockquote>
<p>Performance Error: your neural net did not train well.</p>
</blockquote>
<p>To the uninitiated, the message is daunting. But to the experienced, this is a<br>great error. It means the boilerplate coding is out of the way, and it’s time<br>to dig in!</p>
<h2 id="How-to-deal-with-NaNs"><a href="#How-to-deal-with-NaNs" class="headerlink" title="How to deal with NaNs"></a>How to deal with NaNs</h2><p>By far the most common first question I get from students is, “Why am I<br>getting NaNs.” Occasionally, this has a complicated answer. But most often,<br>the NaNs come in the first 100 iterations, and the answer is simple: your<br>learning rate is too high. When the learning rate is very high, you will get<br>NaNs in the first 100 iterations of training. Try reducing the learning rate<br>by a factor of 3 until you no longer get NaNs in the first 100 iterations. As<br>soon as this works, you’ll have a pretty good learning rate to get started<br>with. In my experience, the best heavily validated learning rates are 1-10x<br>below the range where you get NaNs.</p>
<p>If you are getting NaNs beyond the first 100 iterations, there are 2 further<br>common causes. 1) If you are using RNNs, make sure that you are using “gradient<br>clipping”, which caps the global L2 norm of the gradients. RNNs tend to<br>produce gradients early in training where 10% or fewer of the batches have<br>learning spikes, where the gradient magnitude is very high. Without clipping,<br>these spikes can cause NaNs. 2) If you have written any custom layers<br>yourself, there is a good chance your own custom layer is causing the problems<br>in a division by zero scenario. Another notoriously NaN producing layer is<br>the softmax layer. The softmax computation involves an exp(x) term in both the<br>numerator and denominator, which can divide Inf by Inf and produce NaNs. Make<br>sure you are using a stabilized softmax implementation.</p>
<h2 id="What-to-do-when-your-neural-net-isn’t-learning-anything"><a href="#What-to-do-when-your-neural-net-isn’t-learning-anything" class="headerlink" title="What to do when your neural net isn’t learning anything"></a>What to do when your neural net isn’t learning anything</h2><p>Once you stop getting NaNs, you are often rewarded with a neural net that runs<br>smoothly for many thousand iterations, but never reduces the training loss<br>after the initial fidgeting of the first few hundred iterations. When you’re<br>first constructing your code base, waiting for more than 2000 iterations is<br>rarely the answer. This is not because all networks can start learning in<br>under 2000 iterations. Rather, the chance you’ve introduced a bug when coding<br>up a network from scratch is so high that you’ll want to go into a special<br>early debugging mode before waiting on high iteration counts. The name of the<br>game here is to reduce the scope of the problem over and over again until you<br>have a network that trains in less than 2000 iterations. Fortunately, there<br>are always 2 good dimensions to reduce complexity.</p>
<p>1) Reduce the size of the training set to 10 instances. Working neural nets<br>can usually overfit to 10 instances within just a few hundred iterations. Many<br>coding bugs will prevent this from happening. If you’re network is not able to<br>overfit to 10 instances of the training set, make sure your data and labels<br>are hooked up correctly. Try reducing the batch size to 1 to check for batch<br>computation errors. Add print statements throughout the code to make sure<br>things look like you expect. Usually, you’ll be able to find these bugs<br>through sheer brute force. Once you can train on 10 instances, try training on</p>
<ol>
<li>If this works okay, but not great, you’re ready for the next step.</li>
</ol>
<p>2) Solve the simplest version of the problem that you’re interested in. If<br>you’re translating sentences, try to build a language model for the target<br>language first. Once that works, try to predict the first word of the<br>translation given only the first 3 words of the source. If you’re trying to<br>detect objects in images, try classifying the number of objects in each image<br>before training a regression network. There is a trade-off between getting<br>a good sub-problem you’re sure the network can solve, and spending the<br>least amount of time plumbing the code to hook up the appropriate data.<br>Creativity will help here.</p>
<p>The trick to scaling up a neural net for a new idea is to slowly relax the<br>simplifications made in the above two steps. This is a form of coordinate<br>ascent, and it works great. First, you show that the neural net can at least<br>memorize a few examples. Then you show that it’s able to really generalize to<br>the validation set on a dumbed down version of the problem. You slowly up the<br>difficulty while making steady progress. It’s not as fun as hotshotting it<br>the first time Karpathy style, but at least it works. At some point, you’ll<br>find the problem is difficult enough that it can no longer be learned in 2000<br>iterations. That’s great! But it should rarely take more than 10 times the<br>iterations of the previous complexity level of the problem. If you’re finding<br>that to be the case, try to search for an intermediate level of complexity.</p>
<h2 id="Tuning-hyperparameters"><a href="#Tuning-hyperparameters" class="headerlink" title="Tuning hyperparameters"></a>Tuning hyperparameters</h2><p>Now that your networks is learning things, you’re probably in pretty good<br>shape. But you may find that your network is just not capable of solving the<br>most difficult versions of your problem. Hyperparameter tuning will be key<br>here. Some people who just download a CNN package and ran it on their dataset<br>will tell you hyperparameter tuning didn’t make a difference. Realize that<br>they’re solving an existing problem with an existing architecture. If you’re<br>solving a new problem that demands a new architecture, hyperparameter tuning<br>to get within the ballpark of a good setting is a must. You’re best bet is<br>to read a hyperparameter tutorial for your specific problem, but I’ll list<br>a few basic ideas here for completeness.</p>
<ul>
<li>Visualization is key. Don’t be afraid to take the time to write<br>yourself nice visualization tools throughout training. If your method<br>of visualization is watching the loss bump around from the terminal,<br>consider an upgrade.</li>
<li>Weight initializations are important. Generally, larger magnitude initial<br>weights are a good idea, but too large will get you NaNs. Thus, weight<br>initialization will need to be simultaneously tuned with the learning rate.</li>
<li>Make sure the weights look “healthy”. To learn what this means, I<br>recommend opening weights from existing networks in an ipython notebook.<br>Take some time to get used to what weight histograms should look<br>like for your components in mature nets trained on standard datasets<br>like ImageNet or the Penn Tree Bank.</li>
<li>Neural nets are not scale invariant w.r.t. inputs, especially<br>when trained with SGD rather than second order methods, as SGD is<br>not a scale-invariant method.  Take the time to scale your input<br>data and output labels in the same way that others before you have<br>scaled them.</li>
<li>Decreasing your learning rate towards the end of training will<br>almost always give you a boost. The best decay schedules usually take the<br>form: after k epochs, divide the learning rate by 1.5 every n epochs,<br>where $k &gt; n$.</li>
<li><p>Use hyperparameter config files, although it’s okay to put<br>hyperparameters in the code until you start trying out different values.<br>I use json files that I load in with a command line argument as in<br><a href="https://github.com/Russell91/tensorbox" target="_blank" rel="external">https://github.com/Russell91/tensorbox</a>, but the exact format is not<br>important. Avoid the urge to refactor your code as it becomes a<br>hyperparameter loading mess! Refactors introduce bugs that cost you<br>training cycles, and can be avoided until after you have a network<br>you like.</p>
</li>
<li><p>Randomize your hyperparameter search if you can afford it. Random<br>search generates hyperparmeter combinations you wouldn’t have thought<br>of and removes a great deal of effort once your intuition is already<br>trained on how to think about the impact of a given hyperparameter.</p>
</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Debugging neural nets can be more laborious than traditional programs because<br>almost all errors get projected onto the single dimension of overall network<br>performance. Nonetheless, binary search is still your friend. By alternately<br>1) changing the difficulty of your problem, and 2) using a small number of<br>training examples, you can quickly work through the initial bugs.<br>Hyperparameter tuning and long periods of diligent waiting will get you the<br>rest of the way.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>