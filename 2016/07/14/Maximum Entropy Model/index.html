<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Maximum Entropy Model | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Maximum Entropy Model</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/machine-learning/">machine learning</a>/<a class="post-tag-link" href="/tags/nlp/">nlp</a></div></div></div><article><div class="container post"><p><a href="http://www.zhizhihu.com/html/y2011/3500.html" target="_blank" rel="external">原文链接</a></p>
<p>设随机变量<span class="math inline">\(\xi\)</span>，他有<span class="math inline">\(A_1,A_2...A_n\)</span>共<span class="math inline">\(n\)</span>个不同的取值，每个取值出现的概率为<span class="math inline">\(p_1,p_2...p_n\)</span>，那么<span class="math inline">\(\xi\)</span>的不确定度，即信息熵为： <span class="math display">\[
H(\xi)=\sum_{i=1}^n p_i log_2{1 \over p_i}=-\sum_{i=1}^n p_i log_2 p_i
\]</span> 熵越大，越不确定。熵为0，事件是确定的，例如抛硬币，每次事件发生的概率都是1/2的话，那么熵为1，即<span class="math inline">\(H(X)=-(0.5log_2 0.5+0.5log_2 0.5)=1\)</span>。</p>
<blockquote>
<p>最大熵原理指出，当我们需要对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。因为这时概率分布的信息熵最大，所以人们称这种模型叫“最大熵模型”。我们常说，不要把所有的鸡蛋放在一个篮子里，其实就是最大熵原理的一个朴素的说法，因为当我们遇到不确定性时，就要保留各种可能性。说白了，就是要保留全部的不确定性，将风险降到最小。—-摘自《Google黑板报》作者：吴军</p>
</blockquote>
<p><strong>自然语言处理的例子：</strong></p>
<p>“学习”这个词可能是动词，也可能是名词。可以可以被标为主语、谓语、宾语、定语…… 令<span class="math inline">\(x_1\)</span>表示“学习”被标为名词，<span class="math inline">\(x_2\)</span>表示“学习”被标为动词。令<span class="math inline">\(y_1\)</span>表示“学习”被标为主语，<span class="math inline">\(y_2\)</span>表示被标为谓语，<span class="math inline">\(y_3\)</span>表示宾语，<span class="math inline">\(y_4\)</span>表示定语。得到下面的表示： <span class="math display">\[
p(x_1)+p(x_2)=1
\]</span><span class="math display">\[
\sum_{i=1}^4 p(y_i)=1
\]</span> 如果没有其他的知识，根据信息熵的理论，概率趋向于均匀。所以有： <span class="math display">\[
p(x_1)=p(x_2)=0.5
\]</span><span class="math display">\[
p(y_1)=p(y_2)=p(y_3)=p(y_4)=0.25
\]</span> 但是在实际情况中，“学习”被标为定语的可能性很小，只有0.05。我们引入这个新的知识：<span class="math inline">\(p(y_4)=0.05\)</span>，在满足了这个约束的情况下，其他的事件我们尽可能的让他们符合自然，符合均匀分布： <span class="math display">\[
p(x_1)=p(x_2)=0.5
\]</span><span class="math display">\[
p(y_1)=p(y_2)=p(y_3)=0.953
\]</span> 嗯，如果再加入一个知识，当“学习”被标作动词的时候，它被标作谓语的概率为0.95，这个其实是很自然的事情。都已经是动词了，那么是谓语的可能性就很大了： <span class="math display">\[
p(y_2|x_2)=0.95
\]</span> 已经有了两个知识了，第二个还是条件概率的知识，那么其他的我们尽可能的让他们不受约束，不受影响，分布的均匀一些，现在应该怎么让他们符合尽可能的均匀分布呢？</p>
<p>其实就是使熵尽可能的大就行了。也就是有个分布<span class="math inline">\(p\)</span>，他尽可能的把训练集中的知识表示出来，损失最小，并且还能够保证<span class="math inline">\(p\)</span>的熵最大： <span class="math display">\[
p^*=\mathop{argmax}\limits_p H(p)
\]</span></p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:herosgq@gmail.com" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>