<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Dynamic Programming | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Dynamic Programming</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><p>A method for solving complex problems by breaking them down into subproblems:</p>
<ul>
<li>Slove the subproblems</li>
<li>Combine solutions to subproblems</li>
</ul>
<p>Dynamic programming assumes full knowledge of the MDP. It is used for <em>planning</em> in an MDP.</p>
<h2 id="for-prediction">For prediction</h2>
<p><strong>Input</strong>: MDP <span class="math inline">\(\left&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&gt;\)</span> and policy <span class="math inline">\(\pi\)</span> or MRP <span class="math inline">\(\left&lt;\mathcal S,\mathcal P^\pi,\mathcal R^\pi,\gamma \right&gt;\)</span></p>
<p><strong>Output</strong>: value function <span class="math inline">\(v_\pi\)</span></p>
<h2 id="for-control">For control</h2>
<p><strong>Input</strong>: MDP <span class="math inline">\(\left&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&gt;\)</span></p>
<p><strong>Output</strong>: optimal value function $ v_* $ and optimal policy $ _* $</p>
<h2 id="iterative-policy-evaluation-code">Iterative Policy Evaluation <a href="https://github.com/sohero/reinforcement-learning/blob/master/DP/Policy%20Evaluation%20Solution.ipynb" title="[code]" target="_blank" rel="external">[code]</a></h2>
<p><strong>Problem</strong>: evaluate a given policy <span class="math inline">\(\pi\)</span></p>
<p><strong>Solution</strong>: iterative application of Bellman expectation backup <span class="math display">\[
v_1\to v_2 \to ... \to v_\pi
\]</span> Using <em>synchronous</em> backups,</p>
<ul>
<li>At each <strong>iteration</strong> <span class="math inline">\(k+1\)</span></li>
<li>For all state <span class="math inline">\(\mathcal s \in \mathcal S\)</span></li>
<li>Update <span class="math inline">\(v_{k+1}(\mathcal s)\)</span> from <span class="math inline">\(v_k(\mathcal s&#39;)\)</span> where <span class="math inline">\(\mathcal s&#39;\)</span> is a successor state of <span class="math inline">\(\mathcal s\)</span></li>
</ul>
<p><span class="math display">\[
v_{k+1}(\mathcal s)=\sum_{a\in\mathcal A} \pi(a|\mathcal s)\left(\mathcal R_{\mathcal s}^a+\gamma\sum_{\mathcal s&#39;\in\mathcal S}\mathcal P^a_{\mathcal s\mathcal s&#39;}v_k(\mathcal s&#39;)\right)
\]</span> <span class="math display">\[
\mathbf{v^{k+1}}=\mathbf{\mathcal R^\pi} + \gamma\mathbf{\mathcal P^\pi v^k}
\]</span></p>
<h2 id="how-to-improve-a-policy-code">How to Improve a Policy <a href="https://github.com/sohero/reinforcement-learning/blob/master/DP/Policy%20Iteration%20Solution.ipynb" title="[code]" target="_blank" rel="external">[code]</a></h2>
<p>Given a policy <span class="math inline">\(\pi\)</span>, <strong>evaluate</strong> the policy <span class="math inline">\(\pi\)</span>: <span class="math display">\[
v_\pi(s)=\Bbb E[R_{t+1}+\gamma R_{t+2}+...|S_t=s]
\]</span> <strong>Improve</strong> the policy by acting greedily with respect to <span class="math inline">\(v_\pi\)</span>: <span class="math display">\[
\pi &#39;=greedy(v_\pi)
\]</span></p>
<h2 id="policy-iteration">Policy Iteration</h2>
<figure>
<img src="/images/437f004bfc3d7e31ade589f421f09267.png">
</figure>
<h2 id="principle-of-optimality">Principle of Optimality</h2>
<p>Any optimal policy can be subdivided into two components:</p>
<ul>
<li>An optimal first Action <span class="math inline">\(A_*\)</span></li>
<li>Followed by an optimal policy from successor state <span class="math inline">\(\mathcal{S&#39;}\)</span></li>
</ul>
<p>A policy <span class="math inline">\(\pi(a|s)\)</span> achieves the optimal value from state <span class="math inline">\(s\)</span>, <span class="math inline">\(v_\pi(s)=v_* (s)\)</span>, if and only if <strong>For any state <span class="math inline">\(s&#39;\)</span> reachable from <span class="math inline">\(s\)</span>, <span class="math inline">\(\pi\)</span> achieves the optimal value from state <span class="math inline">\(s&#39;\)</span>, <span class="math inline">\(v_\pi(s&#39;)=v_* (s&#39;)\)</span></strong></p>
<h2 id="deterministic-value-iteration-code">Deterministic Value Iteration <a href="https://github.com/sohero/reinforcement-learning/blob/master/DP/Value%20Iteration%20Solution.ipynb" title="[code]" target="_blank" rel="external">[code]</a></h2>
<p>If we know the solution to subproblems <span class="math inline">\(v_* (s&#39;)\)</span>. Then solution <span class="math inline">\(v_* (s)\)</span> can be found by one-step lookahead: <span class="math display">\[
v_*(s)\leftarrow\max_{a\in\mathcal A}\mathcal R_s^a+\gamma\sum_{s&#39;\in\mathcal S}v_*(s&#39;)
\]</span> The idea of value iteration is to apply these updates iteratively.</p>
<p><strong>Intuition</strong>: start with final rewards and word backwards. <span class="math display">\[
v_{k+1}(s)=\max_{a\in\mathcal A}\left(\mathcal R_s^a+\gamma\sum_{s&#39;\in\mathcal S}\mathcal R_{ss&#39;}^av_k(s&#39;)\right)
\]</span> <span class="math display">\[
v_{k+1}=\max_{a\in\mathcal A}\mathcal R^a+\gamma\mathcal P^av_k
\]</span></p>
<h2 id="synchronous-dynamic-programming-algorithms">Synchronous Dynamic Programming Algorithms</h2>
<table style="width:100%;">
<colgroup>
<col style="width: 13%">
<col style="width: 51%">
<col style="width: 35%">
</colgroup>
<thead>
<tr class="header">
<th>Problem</th>
<th>Bellman Equation</th>
<th>Algorithm</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Prediction</td>
<td>Bellman Expectation Equation</td>
<td>Iterative Policy Evaluation</td>
</tr>
<tr class="even">
<td>Control</td>
<td>Bellman Expectation Equation + Greedy Policy Improvement</td>
<td>Policy Iteration</td>
</tr>
<tr class="odd">
<td>Control</td>
<td>Bellman Optimally Equation</td>
<td>Value Iteration</td>
</tr>
</tbody>
</table>
<ul>
<li>Algorithms are based on state-value function <span class="math inline">\(v_\pi(s)\)</span> or <span class="math inline">\(v_*(s)\)</span>. Complexity <span class="math inline">\(O(mn^2)\)</span> per iteration for <span class="math inline">\(m\)</span> actions and <span class="math inline">\(n\)</span> states.</li>
<li>Could also apply to action-value function <span class="math inline">\(q_\pi(s,a)\)</span> or <span class="math inline">\(q_*(s,a)\)</span>. Complexity <span class="math inline">\(O(m^2n^2)\)</span> per iteration.</li>
</ul>
<h2 id="asynchronous-dynamic-programming">Asynchronous Dynamic Programming</h2>
<p>Asynchronous DP backs up states individually in any order. For each selected state, apply the appropriate backup. Can significantly reduce computation. Guaranteed to converge if all states continue to be selected.</p>
<h3 id="in-place-dynamic-programming">In-Place Dynamic Programming</h3>
<p><strong>Synchronous value iteration</strong> stores two copies of value function for all <span class="math inline">\(s\)</span> in <span class="math inline">\(\mathcal{S}\)</span> <span class="math display">\[
\begin{align}
v_{new}(s) &amp;\leftarrow \max_{a\in{\mathcal{A}}}\left(\mathcal{R}_s^a+
\gamma\sum_{s&#39;\in{\mathcal{S}}}\mathcal{P}^a_{ss&#39;}v_{old}(s&#39;)
\right) \\
v_{old} &amp;\leftarrow v_{new}
\end{align}
\]</span> <strong>In-place value iteration</strong> only stores one copy of value function for all <span class="math inline">\(s\)</span> in <span class="math inline">\(\mathcal{S}\)</span> <span class="math display">\[
v(s) \leftarrow \max_{a\in\mathcal{A}}\left(
\mathcal{R}_s^a+\gamma\sum_{s&#39;\in\mathcal{S}}\mathcal{P}_{ss&#39;}^av(s&#39;)
\right)
\]</span></p>
<h3 id="prioritised-sweeping">Prioritised Sweeping</h3>
<p>Use magnitude of Bellman error to guide state selection, e.g. <span class="math display">\[
\left|
\max_{a\in\mathcal{A}}
\left(
\mathcal{R}_s^a + \gamma\sum_{s&#39;\in\mathcal{S}}\mathcal{P}_{ss&#39;}^av(s&#39;)
\right) - v(s)
\right|
\]</span></p>
<ul>
<li>Backup the state with the largest remaining Bellman error</li>
<li>Update Bellman error of affected states after each backup</li>
<li>Requires knowledge of reverse dynamics (predecessor states)</li>
<li>Can be implemented efficiently by maintaining a priority queue</li>
</ul>
<h3 id="real-time-dynamic-programming">Real-Time Dynamic Programming</h3>
<p><strong>Idea</strong>: only states that are relevant to agent. Use agent’s experience to guide the selection of states. After each time-step <span class="math inline">\(S_t, A_t, R_{t+1}\)</span>, backup the state <span class="math inline">\(S_t\)</span>: <span class="math display">\[
v(S_t) \leftarrow \max_{a\in\mathcal{A}} \left(
\mathcal{R}_{S_t}^a + \gamma\sum_{s&#39;\in\mathcal{S}}
\mathcal{P}_{S_ts&#39;}^av(s&#39;)
\right)
\]</span></p>
<h2 id="full-width-and-sample-backups">Full-width and sample backups</h2>
<h3 id="full-width-backups">Full-Width Backups</h3>
<p>DP uses <em>full-width</em> backups. For each backup (sync or async):</p>
<ul>
<li>Every successor state and action is considered</li>
<li>Using knowledge of the MDP transitions and reward function</li>
</ul>
<p>DP is effective for medium-sized problems (millions of states). For large problems DP suffers Bellman’s <em>curse of dimensionality</em> (Number of states <span class="math inline">\(n=|\mathcal{S}|\)</span> grows exponentially with number of state variables). Even one backup can be too expensive.</p>
<h3 id="sample-backups">Sample Backups</h3>
<p>Using sample rewards and sample transitions <span class="math inline">\(\left&lt;\mathcal{S,A,R,S&#39;}\right&gt;\)</span> instead of reward function <span class="math inline">\(\mathcal{R}\)</span> and transition dynamics <span class="math inline">\(\mathcal{P}\)</span>.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Model-free: no advance knowledge of MDP required</li>
<li>Breaks the curse of dimensionality through sampling</li>
<li>Cost of backup is constant, independent of <span class="math inline">\(n=|\mathcal{S}|\)</span></li>
</ul>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/07/14/Dynamic Programming/" data-title="Dynamic Programming" data-url="https://www.sgq.mobi/2016/07/14/Dynamic Programming/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>