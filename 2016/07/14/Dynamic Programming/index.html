<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Dynamic Programming | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Dynamic Programming</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><p>A method for solving complex problems by breaking them down into subproblems:</p>
<ul>
<li>Slove the subproblems</li>
<li>Combine solutions to subproblems</li>
</ul>
<p>Dynamic programming assumes full knowledge of the MDP. It is used for <em>planning</em> in an MDP.</p>
<h2 id="For-prediction"><a href="#For-prediction" class="headerlink" title="For prediction"></a>For prediction</h2><p><strong>Input</strong>: MDP $\left&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&gt;$ and policy $\pi$ or MRP $\left&lt;\mathcal S,\mathcal P^\pi,\mathcal R^\pi,\gamma \right&gt;$</p>
<p><strong>Output</strong>: value function $v_\pi$</p>
<h2 id="For-control"><a href="#For-control" class="headerlink" title="For control"></a>For control</h2><p><strong>Input</strong>: MDP $\left&lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&gt;$</p>
<p><strong>Output</strong>: optimal value function $ v<em>* $ and optimal policy $ \pi</em>* $</p>
<p>##Iterative Policy Evaluation <a href="https://github.com/sohero/reinforcement-learning/blob/master/DP/Policy%20Evaluation%20Solution.ipynb" title="[code]" target="_blank" rel="external">[code]</a><br><strong>Problem</strong>: evaluate a given policy $\pi$</p>
<p><strong>Solution</strong>: iterative application of Bellman expectation backup<br>$$<br>v_1\to v<em>2 \to … \to v</em>\pi<br>$$<br>Using <em>synchronous</em> backups,</p>
<ul>
<li>At each <strong>iteration</strong> $k+1$</li>
<li>For all state $\mathcal s \in \mathcal S$</li>
<li>Update $v_{k+1}(\mathcal s)$ from $v_k(\mathcal s’)$ where $\mathcal s’$ is a successor state of $\mathcal s$</li>
</ul>
<p>$$<br>v<em>{k+1}(\mathcal s)=\sum</em>{a\in\mathcal A} \pi(a|\mathcal s)\left(\mathcal R<em>{\mathcal s}^a+\gamma\sum</em>{\mathcal s’\in\mathcal S}\mathcal P^a_{\mathcal s\mathcal s’}v_k(\mathcal s’)\right)<br>$$<br>$$<br>\mathbf{v^{k+1}}=\mathbf{\mathcal R^\pi} + \gamma\mathbf{\mathcal P^\pi v^k}<br>$$</p>
<p>##How to Improve a Policy <a href="https://github.com/sohero/reinforcement-learning/blob/master/DP/Policy%20Iteration%20Solution.ipynb" title="[code]" target="_blank" rel="external">[code]</a><br>Given a policy $\pi$, <strong>evaluate</strong> the policy $\pi$:<br>$$<br>v<em>\pi(s)=\Bbb E[R</em>{t+1}+\gamma R_{t+2}+…|S<em>t=s]<br>$$<br><strong>Improve</strong> the policy by acting greedily with respect to $v</em>\pi$:<br>$$<br>\pi ‘=greedy(v_\pi)<br>$$</p>
<p>##Policy Iteration<br><img src="/images/437f004bfc3d7e31ade589f421f09267.png" alt=""></p>
<p>##Principle of Optimality<br>Any optimal policy can be subdivided into two components:</p>
<ul>
<li>An optimal first Action $A_*$</li>
<li>Followed by an optimal policy from successor state $\mathcal{S’}$</li>
</ul>
<p>A policy $\pi(a|s)$ achieves the optimal value from state $s$, $v<em>\pi(s)=v</em><em> (s)$, if and only if **For any state $s’$ reachable from $s$, $\pi$ achieves the optimal value from state $s’$, $v<em>\pi(s’)=v</em></em> (s’)$**</p>
<p>##Deterministic Value Iteration <a href="https://github.com/sohero/reinforcement-learning/blob/master/DP/Value%20Iteration%20Solution.ipynb" title="[code]" target="_blank" rel="external">[code]</a><br>If we know the solution to subproblems $v<em>* (s’)$. Then solution $v</em><em> (s)$ can be found by one-step lookahead:<br>$$<br>v_</em>(s)\leftarrow\max_{a\in\mathcal A}\mathcal R<em>s^a+\gamma\sum</em>{s’\in\mathcal S}v_*(s’)<br>$$<br>The idea of value iteration is to apply these updates iteratively.</p>
<p><strong>Intuition</strong>: start with final rewards and word backwards.<br>$$<br>v<em>{k+1}(s)=\max</em>{a\in\mathcal A}\left(\mathcal R<em>s^a+\gamma\sum</em>{s’\in\mathcal S}\mathcal R_{ss’}^av<em>k(s’)\right)<br>$$<br>$$<br>v</em>{k+1}=\max_{a\in\mathcal A}\mathcal R^a+\gamma\mathcal P^av_k<br>$$</p>
<p>##Synchronous Dynamic Programming Algorithms</p>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Bellman Equation</th>
<th>Algorithm</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prediction</td>
<td>Bellman Expectation Equation</td>
<td>Iterative Policy Evaluation</td>
</tr>
<tr>
<td>Control</td>
<td>Bellman Expectation Equation + Greedy Policy Improvement</td>
<td>Policy Iteration</td>
</tr>
<tr>
<td>Control</td>
<td>Bellman Optimally Equation</td>
<td>Value Iteration</td>
</tr>
</tbody>
</table>
<ul>
<li>Algorithms are based on state-value function $v<em>\pi(s)$ or $v</em>*(s)$. Complexity $O(mn^2)$ per iteration for $m$ actions and $n$ states.</li>
<li>Could also apply to action-value function $q<em>\pi(s,a)$ or $q</em>*(s,a)$. Complexity $O(m^2n^2)$ per iteration.</li>
</ul>
<p>##Asynchronous Dynamic Programming<br>Asynchronous DP backs up states individually in any order. For each selected state, apply the appropriate backup. Can significantly reduce computation. Guaranteed to converge if all states continue to be selected.</p>
<p>###In-Place Dynamic Programming<br><strong>Synchronous value iteration</strong> stores two copies of value function for all $s$ in $\mathcal{S}$<br>$$<br>\begin{align}<br>v<em>{new}(s) &amp;\leftarrow \max</em>{a\in{\mathcal{A}}}\left(\mathcal{R}<em>s^a+<br>\gamma\sum</em>{s’\in{\mathcal{S}}}\mathcal{P}^a<em>{ss’}v</em>{old}(s’)<br>\right) \<br>v<em>{old} &amp;\leftarrow v</em>{new}<br>\end{align}<br>$$<br><strong>In-place value iteration</strong> only stores one copy of value function for all $s$ in $\mathcal{S}$<br>$$<br>v(s) \leftarrow \max_{a\in\mathcal{A}}\left(<br>\mathcal{R}<em>s^a+\gamma\sum</em>{s’\in\mathcal{S}}\mathcal{P}_{ss’}^av(s’)<br>\right)<br>$$</p>
<p>###Prioritised Sweeping<br>Use magnitude of Bellman error to guide state selection, e.g.<br>$$<br>\left|<br>\max_{a\in\mathcal{A}}<br>\left(<br>\mathcal{R}<em>s^a + \gamma\sum</em>{s’\in\mathcal{S}}\mathcal{P}_{ss’}^av(s’)<br>\right) - v(s)<br>\right|<br>$$</p>
<ul>
<li>Backup the state with the largest remaining Bellman error</li>
<li>Update Bellman error of affected states after each backup</li>
<li>Requires knowledge of reverse dynamics (predecessor states)</li>
<li>Can be implemented efficiently by maintaining a priority queue</li>
</ul>
<p>###Real-Time Dynamic Programming<br><strong>Idea</strong>: only states that are relevant to agent. Use agent’s experience to guide the selection of states. After each time-step $S_t, A<em>t, R</em>{t+1}$, backup the state $S_t$:<br>$$<br>v(S<em>t) \leftarrow \max</em>{a\in\mathcal{A}} \left(<br>\mathcal{R}_{S<em>t}^a + \gamma\sum</em>{s’\in\mathcal{S}}<br>\mathcal{P}_{S_ts’}^av(s’)<br>\right)<br>$$</p>
<p>##Full-width and sample backups</p>
<p>###Full-Width Backups<br>DP uses <em>full-width</em> backups. For each backup (sync or async):</p>
<ul>
<li>Every successor state and action is considered</li>
<li>Using knowledge of the MDP transitions and reward function</li>
</ul>
<p>DP is effective for medium-sized problems (millions of states). For large problems DP suffers Bellman’s <em>curse of dimensionality</em> (Number of states $n=|\mathcal{S}|$ grows exponentially with number of state variables). Even one backup can be too expensive.</p>
<p>###Sample Backups<br>Using sample rewards and sample transitions $\left&lt;\mathcal{S,A,R,S’}\right&gt;$ instead of reward function $\mathcal{R}$ and transition dynamics $\mathcal{P}$.</p>
<p><strong>Advantages</strong>:</p>
<ul>
<li>Model-free: no advance knowledge of MDP required</li>
<li>Breaks the curse of dimensionality through sampling</li>
<li>Cost of backup is constant, independent of $n=|\mathcal{S}|$</li>
</ul>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>