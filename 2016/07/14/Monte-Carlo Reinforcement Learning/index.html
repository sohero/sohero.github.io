<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Monte-Carlo Reinforcement Learning | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Monte-Carlo Reinforcement Learning</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><ul>
<li>MC methods learn directly from episodes of experience</li>
<li>MC is <em>model-free</em>: no knowledge of MDP transitions / rewards</li>
<li>MC learns from <em>complete</em> episodes: no bootstrapping</li>
<li>MC uses the simplest possible idea: value = mean return</li>
<li>Caveat: can only apply MC to <em>episodic</em> MDPs. All episodes must terminate.</li>
</ul>
<h2 id="monte-carlo-policy-evaluation">Monte-Carlo Policy Evaluation</h2>
<p><strong>Goal</strong>: learn <span class="math inline">\(v_\pi\)</span> from episodes of experience under policy <span class="math inline">\(\pi\)</span>.</p>
<p>Monte-Carlo policy evaluation uses <code>empirical mean return</code> instead of <em>expected return</em>.</p>
<h2 id="incremental-mean">Incremental Mean</h2>
<p>The mean <span class="math inline">\(\mu_1,\mu_2,...\)</span> of a sequence <span class="math inline">\(x_1,x_2,...\)</span> can be computed incrementally, <span class="math display">\[
\begin{align}
\mu_k &amp;= {1 \over k}\sum_{j=1}^k x_j \\
&amp;= {1 \over k}\left(x_k+\sum_{j=1}^{k-1} x_j\right) \\
&amp;= {1 \over k}\left(x_k+(k-1)\mu_{k-1} \right) \\
&amp;= \mu_{k-1} + {1 \over k}(x_k - \mu_{k-1})
\end{align}
\]</span></p>
<h2 id="incremental-monte-carlo-updates">Incremental Monte-Carlo Updates</h2>
<p>Update <span class="math inline">\(V(s)\)</span> incrementally after episode <span class="math inline">\(S_1,A_1,R_2,...,S_T\)</span>.</p>
<blockquote>
<p>The form of an episode is an array of (state, action, reward) tuples, that is no next state, so no bootstrapping.</p>
</blockquote>
<p>For each state <span class="math inline">\(S_t\)</span> with return <span class="math inline">\(G_t\)</span> <span class="math display">\[
\begin{align}
N(S_t) &amp;\leftarrow N(S_t) + 1 \\
V(S_t) &amp;\leftarrow V(S_t) + {1 \over N(S_t)}(G_t-V(S_t))
\end{align}
\]</span></p>
<p>In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes. <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]</span></p>
<h2 id="off-policy-monte-carlo-control">Off-Policy Monte Carlo Control</h2>
<p>Recall that the distinguishing feature of on-policy methods is that they estimate the value of a policy while using it for control. In off-policy methods these two functions are separated. The policy used to generate behavior, called the <em>behavior</em> policy, may in fact be unrelated to the policy that is evaluated and improved, called the <em>target</em> policy. An advantage of this separation is that the target policy may be deterministic (e.g., greedy), while the behavior policy can continue to sample all possible actions.</p>
<p>Off-policy Monte Carlo control methods follow the behavior policy while learning about and improving the target policy. These techniques require that the behavior policy has a nonzero probability of selecting all actions that might be selected by the target policy (coverage). To explore all possibilities, we require that the behavior policy be soft (i.e., that it select all actions in all states with nonzero probability).</p>
<figure>
<img src="/images/2016-11-15_085750.png">
</figure>
<h2 id="weighted-importance-sampling-incremental-algorithm">weighted importance sampling incremental algorithm</h2>
<p>Suppose we have a sequence of returns <span class="math inline">\(G_1,G_2,...,G_{n-1}\)</span>, all starting in the same state and each with a corresponding random weight <span class="math inline">\(W_i\)</span> (e.g., <span class="math inline">\(W_i=\rho_t^{T(t)}\)</span>). We wish to form the estimate <span class="math display">\[
V_n=\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k},n\ge2
\]</span> and keep it up-to-date as we obtain a single additional return <span class="math inline">\(G_n\)</span>. In addtiton to keeping track of <span class="math inline">\(V_n\)</span>, we must maintain for each state the cumulative sum <span class="math inline">\(C_n\)</span> of the weights given to the first <span class="math inline">\(n\)</span> returns. The update rule for <span class="math inline">\(V_n\)</span> is <span class="math display">\[
V_{n+1}=V_n+\frac{W_n}{C_n}\left[G_n-V_n\right], n\ge 1
\]</span> and <span class="math display">\[
C_{n+1}=C_n+W_{n+1}
\]</span></p>
<figure>
<img src="/images/2016-11-15_084402.png">
</figure>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>