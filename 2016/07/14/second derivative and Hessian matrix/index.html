<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>second derivative and Hessian matrix | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>second derivative and Hessian matrix</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/math/">math</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/math/">math</a></div></div></div><article><div class="container post"><p>We are also sometimes interested in a derivative of a derivative. This is known as a <em>second derivative</em>. For example, <span class="math inline">\(\frac{\partial ^2}{\partial x_i \partial x_j}f\)</span> is the derivative with respect to <span class="math inline">\(x_i\)</span> of the derivative of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(x_j\)</span>. Note that the order of derivativation can be swapped, so that <span class="math inline">\(\frac{\partial ^2}{\partial x_i \partial x_j}f = \frac{\partial ^2}{\partial x_j \partial x_i}f\)</span>. In a single dimension, we can denote <span class="math inline">\(\frac{d^2}{dx^2}f\)</span> by <span class="math inline">\(f&#39;&#39;(x)\)</span>.</p>
<p>The second derivative tells us how the first derivative will change as we vary the input. This means it can be useful for determining whether a critical point is a local maximum , a local minimum, or saddle point. Recall that on a critical point, <span class="math inline">\(f&#39;(x)=0\)</span>. When <span class="math inline">\(f&#39;&#39;(x)&gt;0\)</span>, this means that <span class="math inline">\(f&#39;(x)\)</span> increases as we move to the right, and <span class="math inline">\(f&#39;(x)\)</span> decreases as we move to the left. This means <span class="math inline">\(f&#39;(x-\epsilon)&lt;0\)</span> and <span class="math inline">\(f&#39;(x+\epsilon)&gt;0\)</span> for small enough <span class="math inline">\(\epsilon\)</span>. In other words, as we move right, the slope begins to point uphill to the right, and as we move left, the slope begins to point uphill to the left. Thus, when <span class="math inline">\(f&#39;(x)=0\)</span> and <span class="math inline">\(f&#39;&#39;(x)&gt;0\)</span>, we can conclude that <span class="math inline">\(x\)</span> is a <strong>local minimum</strong>. Similarly, when <span class="math inline">\(f&#39;(x)=0\)</span> and <span class="math inline">\(f&#39;&#39;(x)&lt;0\)</span>, we can conclude that <span class="math inline">\(x\)</span> is a local <strong>local maximum</strong>. This is known as the <em>second derivative test</em>. Unfortunately, when <span class="math inline">\(f&#39;&#39;(x)=0\)</span>, the test is inconclusive. In this case <span class="math inline">\(x\)</span> may be a saddle point, or a part of a flat region.</p>
<p>In multiple dimensions, we need to examine all of the second derivatives of the function. These derivatives can be collected together into a matrix called the <em>Hessian matrix</em>. The Hessian matrix <span class="math inline">\(H(f)(x)\)</span> is defined such that <span class="math display">\[
H(f)(x)_{i,j} = \frac{\partial ^2}{\partial x_i \partial x_j}f(x).
\]</span> Equivalently, the Hessian is the Jacobian of the gradient.</p>
<p>Anywhere that the second partial derivatives are continuous, the differential operators are commutative: <span class="math display">\[
\frac{\partial ^2}{\partial x_i \partial x_j}f(x)=\frac{\partial ^2}{\partial x_j \partial x_i}f(x)
\]</span> This implies that <span class="math inline">\(h_{i,j}=h_{j,i}\)</span>, so the Hessian matrix is symmetric at such points (which includes nearly all inputs to nearly all functions we encounter in deep learning). Because the Hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis of eigenvectors. Using the eigendecomposition Hessian matrix, we can generalize the second derivative test to multiple dimensions. At a critical point, where <span class="math inline">\(\nabla_xf(x)=0\)</span>, we can examine the eigenvalues of the Hessian to determine whether the critical point is a local maximum, local minimum, or saddle point. When the Hessian is positive definite (all its eigenvalues are positive), the point is a <em>local minimum</em>. This can be seen by observing that the directional second derivative in any direction must be positive, and making reference to the univariate second derivative test. Likewise, when the Hessian is negative definite (all its eigenvalues are negative), the point is a <em>local maximum</em>. In multiple dimensions, it is actually possible to find positive evidence of saddle points in some cases. When at least one eigenvalue is positive and at least on eigenvalue is negative, we known that <span class="math inline">\(x\)</span> is a local maximum on one across section of <span class="math inline">\(f\)</span> but a local minimum on another cross section. Finally, the multidimensional second derivative test can be inconclusive, just like the univariate version. The test is inconclusive whenever all of the non-zero eigenvalues have the same sign, but at least one eigenvalue is zero. This is because the univariate second derivative test is inconclusive in the cross section corresponding to the zero eigenvalue.</p>
<p>The Hessian can also be useful for understanding the performance of gradient descent. When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly. Gradient descent is unaware of this change in the derivative so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:herosgq@gmail.com" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">Â© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>