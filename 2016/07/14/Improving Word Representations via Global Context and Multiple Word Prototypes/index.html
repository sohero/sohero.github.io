<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Improving Word Representations via Global Context and Multiple Word Prototypes | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Improving Word Representations via Global Context and Multiple Word Prototypes</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/deep-learning/">deep learning</a>/<a class="post-tag-link" href="/tags/nlp/">nlp</a>/<a class="post-tag-link" href="/tags/paper/">paper</a></div></div></div><article><div class="container post"><h2 id="global-context-aware-neural-language-model">Global Context-Aware Neural Language Model</h2>
<h3 id="training-objective">Training Objective</h3>
<p>Given a word sequence <span class="math inline">\(s\)</span> and document <span class="math inline">\(d\)</span> in which the sequence occurs, our goal is to discriminate the correct last word in <span class="math inline">\(s\)</span> from other random words. We compute scores <span class="math inline">\(g(s,d)\)</span> and <span class="math inline">\(g(s^w,d)\)</span> where <span class="math inline">\(s^w\)</span> is <span class="math inline">\(s\)</span> with the last word replaced by word <span class="math inline">\(w\)</span>, and <span class="math inline">\(g(·,·)\)</span> is the scoring function that represents the neural networks used. We want <span class="math inline">\(g(s,d)\)</span> to be larger than <span class="math inline">\(g(s^w,d)\)</span> by a margin of 1, for any other word <span class="math inline">\(w\)</span> in the vocabulary, which corresponds to the training objective of minimizing the ranking loss for each <span class="math inline">\((s,d)\)</span> found in the corpus: <span class="math display">\[
C_{s,d}=\sum_{w\in V} max(0, 1-g(s,d)+g(s^w,d))
\]</span></p>
<h3 id="neural-network-architecture">Neural Network Architecture</h3>
<p>An overview of the neural language model. The model makes use of both local and global context to compute a score that should be large for the actual next word (bank in the example), compare to the score for other words. When word meaning is still ambiguous given local context, information in global context can help disambiguation.</p>
<figure>
<img src="/images/27.png">
</figure>
<p>The score of local context uses the local word sequence <span class="math inline">\(s\)</span>. We first represent the word sequence <span class="math inline">\(s\)</span> as an ordered list of vectors <span class="math inline">\(x=(x_1,x_2,...,x_m)\)</span> where <span class="math inline">\(x_i\)</span> is the embedding of word <span class="math inline">\(i\)</span> in the sequence, which is a column in the embedding matrix <span class="math inline">\(L\in \Bbb R^{n\times |V|}\)</span> where <span class="math inline">\(|V|\)</span> denotes the size of the vocabulary. <strong>The columns of this embedding matrix <span class="math inline">\(L\)</span> are the word vectors and will be learned and updated during training.</strong></p>
<p>To compute the score of local context, <span class="math inline">\(socre_l\)</span>, use a neural network with one hidden layer: <span class="math display">\[
a_1=f(W_1[x_1;x_2;...;x_m]+b_1)
\]</span> <span class="math display">\[
score_l=W_2a_1+b_2
\]</span> where <span class="math inline">\([x_1;x_2;...;x_m]\)</span> is the concatenation of the <span class="math inline">\(m\)</span> word embeddings representing sequence <span class="math inline">\(s\)</span>, <span class="math inline">\(f\)</span> is an element-wise activation function such as <span class="math inline">\(tanh\)</span>, <span class="math inline">\(a_1\in \Bbb R^{h\times 1}\)</span> is the activation of the hidden layer with <span class="math inline">\(h\)</span> hidden nodes, <span class="math inline">\(W_1\in \Bbb R^{h\times (mn)}\)</span> and <span class="math inline">\(W_2\in\Bbb R^{1\times h}\)</span> are respectively the first and second layer weights of the neural network, and <span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span> are the biases of each layer.</p>
<p>For the score of the global context, we represent the document also as an ordered list of word embedding, <span class="math inline">\(d=(d_1,d_2,...,d_k)\)</span>. First compute the weighted average of all word vectors in the document: <span class="math display">\[
c=\frac{\sum_{i=1}^k w(t_i)d_i}{\sum_{i=1}^k w(t_i)}
\]</span> where <span class="math inline">\(w(·)\)</span> can be any weighting function that captures the importance of word <span class="math inline">\(t_i\)</span> in the document. We use idf-weighting as the weighting function. Use a two-layer neural network to compute the global context score, <span class="math inline">\(score_g\)</span>, similar to the above: <span class="math display">\[
a_1^g=f(W_1^g[c;x_m]+b_1^g)
\]</span> <span class="math display">\[
score_g=W_2^ga_1^g+b_2^g
\]</span> where <span class="math inline">\([c;x_m]\)</span> is the concatenation of the weighted average document vector and the vector of the last word in <span class="math inline">\(s\)</span>.</p>
<p><strong>Note that instead of using the document where the sequence occurs, we can also specify a fixed <span class="math inline">\(k&gt;m\)</span> that captures larger context.</strong></p>
<p>The final score is the sum of the two scores: <span class="math display">\[
score = score_l+score_g
\]</span> The local score preserves word order and syntactic information, while the global score uses a weighted average which is similar to bag-of-words features, capturing more of the semantics and topics of the document.</p>
<h3 id="learning">Learning</h3>
<p>Word embeddings move to good positions in the vector space faster when using <code>mini-batch L-BFGS</code> (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt examples per batch for training, compared to stochastic gradient descent.</p>
<h2 id="multi-prototype-neural-language-model">Multi-Prototype Neural Language Model</h2>
<p>In order to learn multiple prototypes, we first gather the fixed-sized context windows of all occurrences of a word (we use 5 words before and after the word occurrence). Each context is represented by a weighted average of the context words’ vectors, where again, we use <code>idf-weighting</code> as the weighting function. Then use <code>spherical k-means</code> to cluster these context representations. Finally, each word occurrence in the corpus is re-labeled to its associated cluster and is used to train the word representation for that cluster.</p>
<p>Similarity between a pair of words <span class="math inline">\((w, w&#39;)\)</span> using the multi-prototype approach can be computed with of without context, as defined by Reisinger and Mooney (2010b): <span class="math display">\[
AvgSimC(w,w&#39;)={1 \over K^2}\sum_{i=1}^k \sum_{j=1}^k p(c,w,i)p(c&#39;,w&#39;,j)d(\mu_i(w), \mu_j(w&#39;))
\]</span> where <span class="math inline">\(p(c,w,i)\)</span> is the likelihood that word <span class="math inline">\(w\)</span> is in its cluser <span class="math inline">\(i\)</span> given context <span class="math inline">\(c\)</span>, <span class="math inline">\(\mu_i(w)\)</span> is the vector representing the <span class="math inline">\(i\)</span>-th cluster centroid of <span class="math inline">\(w\)</span>, and <span class="math inline">\(d(v,v&#39;)\)</span> is a function computing similarity between two vectors, which can be any of the distance functions presented by Curran(2004). The similarity measure can be computed in absence of context by assuming uniform <span class="math inline">\(p(c,w,i)\)</span> over <span class="math inline">\(i\)</span>.</p>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/07/14/Improving Word Representations via Global Context and Multiple Word Prototypes/" data-title="Improving Word Representations via Global Context and Multiple Word Prototypes" data-url="http://www.sgq.mobi/2016/07/14/Improving Word Representations via Global Context and Multiple Word Prototypes/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>