<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Improving Word Representations via Global Context and Multiple Word Prototypes | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Improving Word Representations via Global Context and Multiple Word Prototypes</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/deep-learning/">deep learning</a>/<a class="post-tag-link" href="/tags/nlp/">nlp</a>/<a class="post-tag-link" href="/tags/paper/">paper</a></div></div></div><article><div class="container post"><h2 id="Global-Context-Aware-Neural-Language-Model"><a href="#Global-Context-Aware-Neural-Language-Model" class="headerlink" title="Global Context-Aware Neural Language Model"></a>Global Context-Aware Neural Language Model</h2><h3 id="Training-Objective"><a href="#Training-Objective" class="headerlink" title="Training Objective"></a>Training Objective</h3><p>Given a word sequence $s$ and document $d$ in which the sequence occurs, our goal is to discriminate the correct last word in $s$ from other random words. We compute scores $g(s,d)$ and $g(s^w,d)$ where $s^w$ is $s$ with the last word replaced by word $w$, and $g(·,·)$ is the scoring function that represents the neural networks used. We want $g(s,d)$ to be larger than $g(s^w,d)$ by a margin of 1, for any other word $w$ in the vocabulary, which corresponds to the training objective of minimizing the  ranking loss for each $(s,d)$ found in the corpus:<br>$$<br>C<em>{s,d}=\sum</em>{w\in V} max(0, 1-g(s,d)+g(s^w,d))<br>$$</p>
<h3 id="Neural-Network-Architecture"><a href="#Neural-Network-Architecture" class="headerlink" title="Neural Network Architecture"></a>Neural Network Architecture</h3><p>An overview of the neural language model. The model makes use of both local and global context to compute a score that should be large for the actual next word (bank in the example), compare to the score for other words. When word meaning is still ambiguous given local context, information in global context can help disambiguation.</p>
<p><img src="/images/27.png" alt=""></p>
<p>The score of local context uses the local word sequence $s$. We first represent the word sequence $s$ as an ordered list of vectors $x=(x_1,x_2,…,x_m)$ where $x_i$ is the embedding of word $i$ in the sequence, which is a column in the embedding matrix $L\in \Bbb R^{n\times |V|}$ where $|V|$ denotes the size of the vocabulary. <strong>The columns of this embedding matrix $L$ are the word vectors and will be learned and updated during training.</strong></p>
<p>To compute the score of local context, $socre_l$, use a neural network with one hidden layer:<br>$$<br>a_1=f(W_1[x_1;x_2;…;x_m]+b_1)<br>$$<br>$$<br>score_l=W_2a_1+b_2<br>$$<br>where $[x_1;x_2;…;x_m]$ is the concatenation of the $m$ word embeddings representing sequence $s$, $f$ is an element-wise activation function such as $tanh$, $a_1\in \Bbb R^{h\times 1}$ is the activation of the hidden layer with $h$ hidden nodes, $W_1\in \Bbb R^{h\times (mn)}$ and $W_2\in\Bbb R^{1\times h}$ are respectively the first and second layer weights of the neural network, and $b_1$, $b_2$ are the biases of each layer.</p>
<p>For the score of the global context, we represent the document also as an ordered list of word embedding, $d=(d_1,d_2,…,d<em>k)$. First compute the weighted average of all word vectors in the document:<br>$$<br>c=\frac{\sum</em>{i=1}^k w(t_i)d<em>i}{\sum</em>{i=1}^k w(t_i)}<br>$$<br>where $w(·)$ can be any weighting function that captures the importance of word $t_i$ in the document. We use idf-weighting as the weighting function. Use a two-layer neural network to compute the global context score, $score_g$, similar to the above:<br>$$<br>a_1^g=f(W_1^g[c;x_m]+b_1^g)<br>$$<br>$$<br>score_g=W_2^ga_1^g+b_2^g<br>$$<br>where $[c;x_m]$ is the concatenation of the weighted average document vector and the vector of the last word in $s$.</p>
<p><strong>Note that instead of using the document where the sequence occurs, we can also specify a fixed $k&gt;m$ that captures larger context.</strong></p>
<p>The final score is the sum of the two scores:<br>$$<br>score = score_l+score_g<br>$$<br>The local score preserves word order and syntactic information, while the global score uses a weighted average which is similar to bag-of-words features, capturing more of the semantics and topics of the document.</p>
<h3 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h3><p>Word embeddings move to good positions in the vector space faster when using <code>mini-batch L-BFGS</code> (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt examples per batch for training, compared to stochastic gradient descent.</p>
<h2 id="Multi-Prototype-Neural-Language-Model"><a href="#Multi-Prototype-Neural-Language-Model" class="headerlink" title="Multi-Prototype Neural Language Model"></a>Multi-Prototype Neural Language Model</h2><p>In order to learn multiple prototypes, we first gather the fixed-sized context windows of all occurrences of a word (we use 5 words before and after the word occurrence). Each context is represented by a weighted average of the context words’ vectors, where again, we use <code>idf-weighting</code> as the weighting function. Then use <code>spherical k-means</code> to cluster these context representations. Finally, each word occurrence in the corpus is re-labeled to its associated cluster and is used to train the word representation for that cluster.</p>
<p>Similarity between a pair of words $(w, w’)$ using the multi-prototype approach can be computed with of without context, as defined by Reisinger and Mooney (2010b):<br>$$<br>AvgSimC(w,w’)={1 \over K^2}\sum<em>{i=1}^k \sum</em>{j=1}^k p(c,w,i)p(c’,w’,j)d(\mu_i(w), \mu_j(w’))<br>$$<br>where $p(c,w,i)$ is the likelihood that word $w$ is in its cluser $i$ given context $c$, $\mu_i(w)$ is the vector representing the $i$-th cluster centroid of $w$, and $d(v,v’)$ is a function computing similarity between two vectors, which can be any of the distance functions presented by Curran(2004). The similarity measure can be computed in absence of context by assuming uniform $p(c,w,i)$ over $i$.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>