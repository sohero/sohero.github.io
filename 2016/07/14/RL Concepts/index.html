<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>RL Concepts | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>RL Concepts</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/foundation/">foundation</a>/<a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><p><strong>backup</strong>: We <em>back up</em> the value of the state after each greedy move to the state before the move.</p>
<p><strong>experience</strong>: sample sequences of <em>states</em>, <em>actions</em>, and <em>rewards</em> from actual or simulated interaction with an environment.</p>
<p><strong>nonstationary</strong>: The return after taking an action in one state depends on the actions taken in later states in the same episode. Because all action selections are undergoing learning, the problem becomes nonstationary from the point view of the earlier state.</p>
<p><strong>prediction problem</strong>: The computation of <span class="math inline">\(v_\pi\)</span> and <span class="math inline">\(q_\pi\)</span> for a fixed solution policy <span class="math inline">\(\pi\)</span>.</p>
<p><strong>bootstrapping</strong>: All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea <em>bootstrapping</em>. <code>Updating an estimate from an estimate, a guess from a guess.</code></p>
<p><strong>exploring starts</strong>: For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes <em>start in a state-action pair</em>, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of <em>exploring starts</em>.</p>
<p><strong><span class="math inline">\(\varepsilon\)</span>-greedy</strong>: most of the time they choose an action that maximal estimated action value, but with probability <span class="math inline">\(\varepsilon\)</span> they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, <span class="math inline">\({\epsilon \over |\mathcal A(\mathcal s)|}\)</span>, and the remaining bulk of the probability, <span class="math inline">\(1-\varepsilon +{\epsilon \over |\mathcal A(\mathcal s)|}\)</span>, is given to the greedy action. The <span class="math inline">\(\varepsilon\)</span>-greedy policies are examples of <span class="math inline">\(\varepsilon\)</span>-soft policies, defined as policies for which <span class="math inline">\(\pi(a|s)\ge {\epsilon \over |\mathcal A(\mathcal s)|}\)</span> for all states and actions, for some <span class="math inline">\(\varepsilon &gt;0\)</span>. Among <span class="math inline">\(\varepsilon\)</span>-soft policies, $-greedy policies are in some sense those that are closest to greedy.</p>
<p><strong>target policy</strong>: The policy being learned about is called the <em>target policy</em>.</p>
<p><strong>behavior policy</strong>: The policy used to generate behavior is called the <em>behavior policy</em>.</p>
<p><strong>off-policy learning</strong>: Off-policy learning is learning about the value of <code>target policy</code> other than the <code>behavior policy</code> being used to generate the trajectory.</p>
<p><strong>on-policy learning</strong>: on-policy learning is when the two policies are the same.</p>
<blockquote>
<p>On-policy methods typically <code>perform better</code> than off-policy methods, but <code>find poorer policies</code>.</p>
</blockquote>
<p><strong>importance sampling</strong>: a general technique for estimating expected values under one distribution given samples from another.</p>
<p><strong>Score Function</strong>: The score function <span class="math inline">\(u(\theta)\)</span> is the partial derivative of the log-likelihood function <span class="math inline">\(F(\theta)=\text{ln}L(\theta)\)</span>, where <span class="math inline">\(L(\theta)\)</span> is the standard likelihood function. <span class="math display">\[
u(\theta)=\frac{\partial}{\partial\theta}F(\theta)
\]</span> Using formulatino of <span class="math inline">\(u\)</span>, one can easily compute various statistical measurements associated with <span class="math inline">\(u\)</span>. For example, the mean <span class="math inline">\(E(u(\theta))\)</span> can be shown to equal zero while the variance is precisely the Fisher information matrix.<a href="http://mathworld.wolfram.com/ScoreFunction.html" target="_blank" rel="external">[Link]</a></p>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/07/14/RL Concepts/" data-title="RL Concepts" data-url="http://www.sgq.mobi/2016/07/14/RL Concepts/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a></div><div class="footer">Â© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>