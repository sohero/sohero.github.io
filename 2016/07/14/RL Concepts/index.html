<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>RL Concepts | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>RL Concepts</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/foundation/">foundation</a>/<a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><p><strong>backup</strong>: We <em>back up</em> the value of the state after each greedy move to the state before the move.</p>
<p><strong>experience</strong>: sample sequences of <em>states</em>, <em>actions</em>, and <em>rewards</em> from actual or simulated interaction with an environment.</p>
<p><strong>nonstationary</strong>: The return after taking an action in one state depends on the actions taken in later states in the same episode. Because all action selections are undergoing learning, the problem becomes nonstationary from the point view of the earlier state.</p>
<p><strong>prediction problem</strong>: The computation of $v<em>\pi$ and $q</em>\pi$ for a fixed solution policy $\pi$.</p>
<p><strong>bootstrapping</strong>: All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea <em>bootstrapping</em>. <code>Updating an estimate from an estimate, a guess from a guess.</code></p>
<p><strong>exploring starts</strong>: For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes <em>start in a state-action pair</em>, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of <em>exploring starts</em>.</p>
<p><strong>$\varepsilon$-greedy</strong>: most of the time they choose an action that maximal estimated action value, but with probability $\varepsilon$ they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, ${\epsilon \over |\mathcal A(\mathcal s)|}$, and the remaining bulk of the probability, $1-\varepsilon +{\epsilon \over |\mathcal A(\mathcal s)|}$, is given to the greedy action. The $\varepsilon$-greedy policies are examples of $\varepsilon$-soft policies, defined as policies for which $\pi(a|s)\ge {\epsilon \over |\mathcal A(\mathcal s)|}$ for all states and actions, for some $\varepsilon &gt;0$. Among $\varepsilon$-soft policies, \varepsilon$-greedy policies are in some sense those that are closest to greedy.</p>
<p><strong>target policy</strong>: The policy being learned about is called the <em>target policy</em>.</p>
<p><strong>behavior policy</strong>: The policy used to generate behavior is called the <em>behavior policy</em>.</p>
<p><strong>off-policy learning</strong>: Off-policy learning is learning about the value of <code>target policy</code> other than the <code>behavior policy</code> being used to generate the trajectory.</p>
<p><strong>on-policy learning</strong>: on-policy learning is when the two policies are the same.</p>
<blockquote>
<p>On-policy methods typically <code>perform better</code> than off-policy methods, but <code>find poorer policies</code>.</p>
</blockquote>
<p><strong>importance sampling</strong>: a general technique for estimating expected values under one distribution given samples from another.</p>
<p><strong>Score Function</strong>: The score function $u(\theta)$ is the partial derivative of the log-likelihood function $F(\theta)=\text{ln}L(\theta)$, where $L(\theta)$ is the standard likelihood function.<br>$$<br>u(\theta)=\frac{\partial}{\partial\theta}F(\theta)<br>$$<br>Using formulatino of $u$, one can easily compute various statistical measurements associated with $u$. For example, the mean $E(u(\theta))$ can be shown to equal zero while the variance is precisely the Fisher information matrix.<a href="http://mathworld.wolfram.com/ScoreFunction.html" target="_blank" rel="external">[Link]</a></p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">Â© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>