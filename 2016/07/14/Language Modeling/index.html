<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Language Modeling | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Language Modeling</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/nlp/">nlp</a></div></div></div><article><div class="container post"><ul>
<li>Goal: compute the probability of a sentence or a sequence of words:<br>$$<br>P(W)=P(w_1,w_2,w_3,…,w_n)<br>$$</li>
<li>Related task: probability of an upcoming word:<br>$$<br>p(w_5|w_1,w_2,w_3,w_4)<br>$$</li>
<li>A model that computes either of these $P(W)$ or $P(w_n|w_1,w<em>2,…,w</em>{n-1})$ is called a <code>language model</code>.</li>
</ul>
<h2 id="The-Chain-Rule-of-Probability"><a href="#The-Chain-Rule-of-Probability" class="headerlink" title="The Chain Rule of Probability"></a>The Chain Rule of Probability</h2><p>$$<br>P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)<br>$$<br>The chain rule in general:<br>$$<br>P(x_1,x_2,x_3,…,x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)…P(x_n|x<em>1,…,x</em>{n-1})<br>$$</p>
<p>i.e.</p>
<p>$$<br>P(w_1w_2···w_n)=\prod_i P(w_i|w_1w<em>2···w</em>{i-1})<br>$$</p>
<h2 id="Markov-Assumption"><a href="#Markov-Assumption" class="headerlink" title="Markov Assumption"></a>Markov Assumption</h2><p>$$<br>P(w_1w_2···w_n)\approx \prod_i P(w<em>i|w</em>{i-k}···w_{i-1})<br>$$</p>
<ul>
<li>Unigram model<br>$$<br>P(w_1w_2···w_n)\approx \prod_i P(w_i)<br>$$</li>
<li>Bigram model<br>$$<br>P(w_1w_2···w_n)\approx \prod_i P(w<em>i|w</em>{i-1})<br>$$</li>
<li>N-gram model</li>
</ul>
<h2 id="Estimating-bigram-probabilities"><a href="#Estimating-bigram-probabilities" class="headerlink" title="Estimating bigram probabilities"></a>Estimating bigram probabilities</h2><p>The Maximum Likelihood Estimate<br>$$<br>P(w<em>i|w</em>{i-1})=\frac{count(w_{i-1},w<em>i)}{count(w</em>{i-1})}<br>$$</p>
<h3 id="Practical-Issues"><a href="#Practical-Issues" class="headerlink" title="Practical Issues"></a>Practical Issues</h3><p>do everything in log space</p>
<ul>
<li>Avoid underflow</li>
<li>also adding is faster than multiplying<br>$$<br>p_1\times p_2\times p_3\times p_4=logp_1+logp_2+logp_3+logp_4<br>$$</li>
</ul>
<h2 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h2><p>The best language model is the one that best predicts an unseen test set.<br>Perplexity is the probability of the test set, normalized by the number of words:<br>$$<br>PP(W)=P(w_1w_2…w_N)^{-{1 \over N}}<br>$$</p>
<p><strong>Chain rule:</strong><br>$$<br>PP(W)=\sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i|w<em>1…w</em>{i-1})}}<br>$$</p>
<p><strong>For bigrams:</strong><br>$$<br>PP(W)=\sqrt[N]{\prod_{i=1}^N \frac{1}{P(w<em>i|w</em>{i-1})}}<br>$$</p>
<p>Minimizing perplexity is the same as maximizing probability.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>