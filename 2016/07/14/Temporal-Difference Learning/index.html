<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Temporal-Difference Learning | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Temporal-Difference Learning</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><p>TD-Learning is a combination of Monte Carlo and Dynamic Programming ideas. Like Monte Carlo, TD works based on samples and doesn’t require a model of the environment. Like Dynamic Programming, TD uses bootstrapping to make updates.</p>
<p>Whether MC or TD is better depends on the problem and there are no theoretical results that prove a clear winner.</p>
<ul>
<li>TD methods learn directly from episodes of experience</li>
<li>TD is <em>model-free</em>: no knowledge of MDP transitions / rewards</li>
<li>TD learns from <em>incomplete</em> episodes, by <em>bootstrapping</em></li>
<li>TD updates a guess towards a guess</li>
</ul>
<p>##MC and TD<br><strong>Goal</strong>: learn $v_\pi$ online from experience under policy $\pi$.</p>
<p>###Incremental every-visit Monte-Carlo<br>Update value $V(S_t)$ toward <em>actual</em> return $G_t$:<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))<br>$$</p>
<p>###Simplest temporal-difference learning algorithm: TD(0)<br>Update value $V(S<em>t)$ toward <em>estimated</em> return $R</em>{t+1}+\gamma V(S_{t+1})$:<br>$$<br>V(S_t) \leftarrow V(S<em>t) + \alpha\left(R</em>{t+1}+\gamma V(S_{t+1})-V(S<em>t)\right)<br>$$<br>$R</em>{t+1}+\gamma V(S_{t+1})$ is called the <em>TD</em> target.</p>
<p>$\delta<em>t=R</em>{t+1}+\gamma V(S_{t+1})-V(S_t)$ is called the <em>TD</em> error.</p>
<p>##Bias/Variance Trade-Off</p>
<ul>
<li>Return $G<em>t=R</em>{t+1}+\gamma R_{t+2}+…+\gamma^{T-1}R<em>T$ is <em>unbiased</em> estimate of $v</em>\pi(S_t)$</li>
<li>True TD target $R<em>{t+1}+\gamma v</em>\pi(S<em>{t+1})$ is <em>unbiased</em> estimate of $v</em>\pi(S_t)$</li>
<li>TD target $R<em>{t+1}+\gamma V(S</em>{t+1})$ is <em>biased</em> estimate of $v_\pi(S_t)$</li>
<li>TD target is much lower variance than the return:<ul>
<li>Return depends on <em>many</em> random actions, transitions, rewards</li>
<li>TD target depends on <em>one</em> random action, transition, reward</li>
</ul>
</li>
</ul>
<p>##Advantages and Disadvantages of MC vs. TD</p>
<ul>
<li><p>MC has high variance, zero bias</p>
<ul>
<li>Good convergence properties</li>
<li>(even with function approximation)</li>
<li>Not very sensitive to initial value</li>
<li>Very simple to understand and use</li>
</ul>
</li>
<li><p>TD has low variance, some bias</p>
<ul>
<li>Usually more efficient than MC</li>
<li>TD(0) converges to $v_\pi(s)$</li>
<li>(but not always with function approximation)</li>
<li>More sensitive to initial value</li>
</ul>
</li>
</ul>
<p>##Monte-Carlo Backup</p>
<p> <img src="/images/42.png" alt=""></p>
<p>##Temporal-Difference Backup</p>
<p><img src="/images/43.png" alt=""></p>
<p>##Dynamic Programming Backup</p>
<p><img src="/images/44.png" alt=""></p>
<p>##Bootstrapping and Sampling<br><strong>Bootstrapping</strong>: update involves an estimate</p>
<ul>
<li>MC does not bootstrap</li>
<li>DP bootstraps</li>
<li>TD bootstraps</li>
</ul>
<p><strong>Sampling</strong>: update samples an expectation</p>
<ul>
<li>MC samples</li>
<li>DP does not sample</li>
<li>TD samples</li>
</ul>
<p>##Unified View of Reinforcement Learning</p>
<p><img src="/images/45.png" alt=""></p>
<p>##TD($\lambda$)</p>
<p>###$n$-step return<br>Define the $n$-step return:<br>$$<br>G<em>t^{(n)}=R</em>{t+1}+\gamma R<em>{t+2}+…+\gamma^{n-1}R</em>{t+n}+\gamma^nV(S_{t+n})<br>$$<br>$n$-step temporal-difference learning<br>$$<br>V(S_t) \leftarrow V(S_t)+\alpha\left(G_t^{(n)}-V(S_t)\right)<br>$$</p>
<p>###averaging $n$-step returns<br>e.g. average the 2-step and 4-step returns<br>$$<br>{1 \over 2}G^{(2)} + {1 \over 2}G^{(4)}<br>$$</p>
<p>###$\lambda$-return<br>The $\lambda$-return $G_t^{\lambda}$ combines all $n$-step returns $G_t^{(n)}$. Using weight $(1-\lambda)\lambda^{n-1}$:<br>$$<br>G<em>t^\lambda=(1-\lambda)\sum</em>{n=1}^\infty \lambda^{n-1}G_t^{(n)}<br>$$<br>Forward-view TD($\lambda$)<br>$$<br>V(S_t) \leftarrow V(S_t) + \alpha\left(G_t^\lambda-V(S_t)\right)<br>$$</p>
<p>###Eligibility Traces</p>
<p><img src="/images/46.png" alt=""></p>
<p>Credit assignment problem: did bell or light cause shock?</p>
<p><strong>Frequency heuristic</strong>: assign credit to most frequent states.</p>
<p><strong>Recency heuristic</strong>: assign credit to most recent states.</p>
<p><strong>Elegibility traces</strong> combine both heuristics:<br>$$<br>\begin{align}<br>E_0(s) &amp;= 0 \<br>E<em>t(s) &amp;= \gamma\lambda E</em>{t-1}(s) + 1(S_t=s)<br>\end{align}<br>$$</p>
<p><img src="/images/47.png" alt=""></p>
<p>###Backward view TD($\lambda$)<br>Keep an eligibility trace for every state $s$. Update value $V(s)$ for every state $s$. In proportion to TD-error $\delta_t$ and eligibility trace $E_t(s)$:<br>$$<br>\begin{align}<br>\delta<em>t &amp;= R</em>{t+1}+\gamma V(S_{t+1})-V(S_t) \<br>V(s) &amp;\leftarrow V(s)+\alpha\delta_t E_t(s)<br>\end{align}<br>$$</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>