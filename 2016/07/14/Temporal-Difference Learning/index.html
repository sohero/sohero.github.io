<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Temporal-Difference Learning | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Temporal-Difference Learning</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><p>TD-Learning is a combination of Monte Carlo and Dynamic Programming ideas. Like Monte Carlo, TD works based on samples and doesn’t require a model of the environment. Like Dynamic Programming, TD uses bootstrapping to make updates.</p>
<p>Whether MC or TD is better depends on the problem and there are no theoretical results that prove a clear winner.</p>
<ul>
<li>TD methods learn directly from episodes of experience</li>
<li>TD is <em>model-free</em>: no knowledge of MDP transitions / rewards</li>
<li>TD learns from <em>incomplete</em> episodes, by <em>bootstrapping</em></li>
<li>TD updates a guess towards a guess</li>
</ul>
<h2 id="mc-and-td">MC and TD</h2>
<p><strong>Goal</strong>: learn <span class="math inline">\(v_\pi\)</span> online from experience under policy <span class="math inline">\(\pi\)</span>.</p>
<h3 id="incremental-every-visit-monte-carlo">Incremental every-visit Monte-Carlo</h3>
<p>Update value <span class="math inline">\(V(S_t)\)</span> toward <em>actual</em> return <span class="math inline">\(G_t\)</span>: <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]</span></p>
<h3 id="simplest-temporal-difference-learning-algorithm-td0">Simplest temporal-difference learning algorithm: TD(0)</h3>
<p>Update value <span class="math inline">\(V(S_t)\)</span> toward <em>estimated</em> return <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span>: <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha\left(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right)
\]</span> <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> is called the <em>TD</em> target.</p>
<p><span class="math inline">\(\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\)</span> is called the <em>TD</em> error.</p>
<h2 id="biasvariance-trade-off">Bias/Variance Trade-Off</h2>
<ul>
<li>Return <span class="math inline">\(G_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1}R_T\)</span> is <em>unbiased</em> estimate of <span class="math inline">\(v_\pi(S_t)\)</span></li>
<li>True TD target <span class="math inline">\(R_{t+1}+\gamma v_\pi(S_{t+1})\)</span> is <em>unbiased</em> estimate of <span class="math inline">\(v_\pi(S_t)\)</span></li>
<li>TD target <span class="math inline">\(R_{t+1}+\gamma V(S_{t+1})\)</span> is <em>biased</em> estimate of <span class="math inline">\(v_\pi(S_t)\)</span></li>
<li>TD target is much lower variance than the return:
<ul>
<li>Return depends on <em>many</em> random actions, transitions, rewards</li>
<li>TD target depends on <em>one</em> random action, transition, reward</li>
</ul></li>
</ul>
<h2 id="advantages-and-disadvantages-of-mc-vs.td">Advantages and Disadvantages of MC vs. TD</h2>
<ul>
<li>MC has high variance, zero bias
<ul>
<li>Good convergence properties</li>
<li>(even with function approximation)</li>
<li>Not very sensitive to initial value</li>
<li>Very simple to understand and use</li>
</ul></li>
<li>TD has low variance, some bias
<ul>
<li>Usually more efficient than MC</li>
<li>TD(0) converges to <span class="math inline">\(v_\pi(s)\)</span></li>
<li>(but not always with function approximation)</li>
<li>More sensitive to initial value</li>
</ul></li>
</ul>
<h2 id="monte-carlo-backup">Monte-Carlo Backup</h2>
<figure>
<img src="/images/42.png">
</figure>
<h2 id="temporal-difference-backup">Temporal-Difference Backup</h2>
<figure>
<img src="/images/43.png">
</figure>
<h2 id="dynamic-programming-backup">Dynamic Programming Backup</h2>
<figure>
<img src="/images/44.png">
</figure>
<h2 id="bootstrapping-and-sampling">Bootstrapping and Sampling</h2>
<p><strong>Bootstrapping</strong>: update involves an estimate</p>
<ul>
<li>MC does not bootstrap</li>
<li>DP bootstraps</li>
<li>TD bootstraps</li>
</ul>
<p><strong>Sampling</strong>: update samples an expectation</p>
<ul>
<li>MC samples</li>
<li>DP does not sample</li>
<li>TD samples</li>
</ul>
<h2 id="unified-view-of-reinforcement-learning">Unified View of Reinforcement Learning</h2>
<figure>
<img src="/images/45.png">
</figure>
<h2 id="tdlambda">TD(<span class="math inline">\(\lambda\)</span>)</h2>
<h3 id="n-step-return"><span class="math inline">\(n\)</span>-step return</h3>
<p>Define the <span class="math inline">\(n\)</span>-step return: <span class="math display">\[
G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})
\]</span> <span class="math inline">\(n\)</span>-step temporal-difference learning <span class="math display">\[
V(S_t) \leftarrow V(S_t)+\alpha\left(G_t^{(n)}-V(S_t)\right)
\]</span></p>
<h3 id="averaging-n-step-returns">averaging <span class="math inline">\(n\)</span>-step returns</h3>
<p>e.g. average the 2-step and 4-step returns <span class="math display">\[
{1 \over 2}G^{(2)} + {1 \over 2}G^{(4)}
\]</span></p>
<h3 id="lambda-return"><span class="math inline">\(\lambda\)</span>-return</h3>
<p>The <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_t^{\lambda}\)</span> combines all <span class="math inline">\(n\)</span>-step returns <span class="math inline">\(G_t^{(n)}\)</span>. Using weight <span class="math inline">\((1-\lambda)\lambda^{n-1}\)</span>: <span class="math display">\[
G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)}
\]</span> Forward-view TD(<span class="math inline">\(\lambda\)</span>) <span class="math display">\[
V(S_t) \leftarrow V(S_t) + \alpha\left(G_t^\lambda-V(S_t)\right)
\]</span></p>
<h3 id="eligibility-traces">Eligibility Traces</h3>
<figure>
<img src="/images/46.png">
</figure>
<p>Credit assignment problem: did bell or light cause shock?</p>
<p><strong>Frequency heuristic</strong>: assign credit to most frequent states.</p>
<p><strong>Recency heuristic</strong>: assign credit to most recent states.</p>
<p><strong>Elegibility traces</strong> combine both heuristics: <span class="math display">\[
\begin{align}
E_0(s) &amp;= 0 \\
E_t(s) &amp;= \gamma\lambda E_{t-1}(s) + 1(S_t=s)
\end{align}
\]</span></p>
<figure>
<img src="/images/47.png">
</figure>
<h3 id="backward-view-tdlambda">Backward view TD(<span class="math inline">\(\lambda\)</span>)</h3>
<p>Keep an eligibility trace for every state <span class="math inline">\(s\)</span>. Update value <span class="math inline">\(V(s)\)</span> for every state <span class="math inline">\(s\)</span>. In proportion to TD-error <span class="math inline">\(\delta_t\)</span> and eligibility trace <span class="math inline">\(E_t(s)\)</span>: <span class="math display">\[
\begin{align}
\delta_t &amp;= R_{t+1}+\gamma V(S_{t+1})-V(S_t) \\
V(s) &amp;\leftarrow V(s)+\alpha\delta_t E_t(s)
\end{align}
\]</span></p>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/07/14/Temporal-Difference Learning/" data-title="Temporal-Difference Learning" data-url="http://www.sgq.mobi/2016/07/14/Temporal-Difference Learning/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>