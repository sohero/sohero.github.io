<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>linear regression example | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>linear regression example</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-07-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/lua/">lua</a>/<a class="post-tag-link" href="/tags/torch/">torch</a></div></div></div><article><div class="container post"><figure class="highlight lua"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div></pre></td><td class="code"><pre><div class="line"><span class="comment">----------------------------------------------------------------------</span></div><div class="line"><span class="comment">-- example-linear-regression.lua</span></div><div class="line"><span class="comment">--</span></div><div class="line"><span class="comment">-- This script provides a very simple step-by-step example of</span></div><div class="line"><span class="comment">-- linear regression, using Torch7's neural network (nn) package,</span></div><div class="line"><span class="comment">-- and the optimization package (optim).</span></div><div class="line"><span class="comment">--</span></div><div class="line"> </div><div class="line"><span class="comment">-- note: to run this script, simply do:</span></div><div class="line"><span class="comment">-- torch script.lua</span></div><div class="line"> </div><div class="line"><span class="comment">-- to run the script, and get an interactive shell once it terminates:</span></div><div class="line"><span class="comment">-- torch -i script.lua</span></div><div class="line"> </div><div class="line"><span class="comment">-- we first require the necessary packages.</span></div><div class="line"><span class="comment">-- note: optim is a 3rd-party package, and needs to be installed</span></div><div class="line"><span class="comment">-- separately. This can be easily done using Torch7's package manager:</span></div><div class="line"><span class="comment">-- torch-pkg install optim</span></div><div class="line"> </div><div class="line"><span class="built_in">require</span> <span class="string">'torch'</span></div><div class="line"><span class="built_in">require</span> <span class="string">'optim'</span></div><div class="line"><span class="built_in">require</span> <span class="string">'nn'</span></div><div class="line"> </div><div class="line"> </div><div class="line"><span class="comment">----------------------------------------------------------------------</span></div><div class="line"><span class="comment">-- 1. Create the training data</span></div><div class="line"> </div><div class="line"><span class="comment">-- In all regression problems, some training data needs to be</span></div><div class="line"><span class="comment">-- provided. In a realistic scenarios, data comes from some database</span></div><div class="line"><span class="comment">-- or file system, and needs to be loaded from disk. In that</span></div><div class="line"><span class="comment">-- tutorial, we create the data source as a Lua table.</span></div><div class="line"> </div><div class="line"><span class="comment">-- In general, the data can be stored in arbitrary forms, and using</span></div><div class="line"><span class="comment">-- Lua's flexible table data structure is usually a good idea.</span></div><div class="line"><span class="comment">-- Here we store the data as a Torch Tensor (2D Array), where each</span></div><div class="line"><span class="comment">-- row represents a training sample, and each column a variable. The</span></div><div class="line"><span class="comment">-- first column is the target variable, and the others are the</span></div><div class="line"><span class="comment">-- input variables.</span></div><div class="line"> </div><div class="line"><span class="comment">-- The data are from an example in Schaum's Outline:</span></div><div class="line"><span class="comment">-- Dominick Salvator and Derrick Reagle</span></div><div class="line"><span class="comment">-- Shaum's Outline of Theory and Problems of Statistics and Economics</span></div><div class="line"><span class="comment">-- 2nd edition</span></div><div class="line"><span class="comment">-- McGraw-Hill</span></div><div class="line"><span class="comment">-- 2002</span></div><div class="line"> </div><div class="line"><span class="comment">-- The data relate the amount of corn produced, given certain amounts</span></div><div class="line"><span class="comment">-- of fertilizer and insecticide. See p 157 of the text.</span></div><div class="line"> </div><div class="line"><span class="comment">-- In this example, we want to be able to predict the amount of</span></div><div class="line"><span class="comment">-- corn produced, given the amount of fertilizer and intesticide used.</span></div><div class="line"><span class="comment">-- In other words: fertilizer &amp; insecticide are our two input variables,</span></div><div class="line"><span class="comment">-- and corn is our target value.</span></div><div class="line"> </div><div class="line"><span class="comment">--  &#123;corn, fertilizer, insecticide&#125;</span></div><div class="line">data = torch.Tensor&#123;</div><div class="line">   &#123;<span class="number">40</span>,  <span class="number">6</span>,  <span class="number">4</span>&#125;,</div><div class="line">   &#123;<span class="number">44</span>, <span class="number">10</span>,  <span class="number">4</span>&#125;,</div><div class="line">   &#123;<span class="number">46</span>, <span class="number">12</span>,  <span class="number">5</span>&#125;,</div><div class="line">   &#123;<span class="number">48</span>, <span class="number">14</span>,  <span class="number">7</span>&#125;,</div><div class="line">   &#123;<span class="number">52</span>, <span class="number">16</span>,  <span class="number">9</span>&#125;,</div><div class="line">   &#123;<span class="number">58</span>, <span class="number">18</span>, <span class="number">12</span>&#125;,</div><div class="line">   &#123;<span class="number">60</span>, <span class="number">22</span>, <span class="number">14</span>&#125;,</div><div class="line">   &#123;<span class="number">68</span>, <span class="number">24</span>, <span class="number">20</span>&#125;,</div><div class="line">   &#123;<span class="number">74</span>, <span class="number">26</span>, <span class="number">21</span>&#125;,</div><div class="line">   &#123;<span class="number">80</span>, <span class="number">32</span>, <span class="number">24</span>&#125;</div><div class="line">&#125;</div><div class="line"> </div><div class="line"> </div><div class="line"><span class="comment">----------------------------------------------------------------------</span></div><div class="line"><span class="comment">-- 2. Define the model (predictor)</span></div><div class="line"> </div><div class="line"><span class="comment">-- The model will have one layer (called a module), which takes the</span></div><div class="line"><span class="comment">-- 2 inputs (fertilizer and insecticide) and produces the 1 output</span></div><div class="line"><span class="comment">-- (corn).</span></div><div class="line"> </div><div class="line"><span class="comment">-- Note that the Linear model specified below has 3 parameters:</span></div><div class="line"><span class="comment">--   1 for the weight assigned to fertilizer</span></div><div class="line"><span class="comment">--   1 for the weight assigned to insecticide</span></div><div class="line"><span class="comment">--   1 for the weight assigned to the bias term</span></div><div class="line"> </div><div class="line"><span class="comment">-- In some other model specification schemes, one needs to augment the</span></div><div class="line"><span class="comment">-- training data to include a constant value of 1, but this isn't done</span></div><div class="line"><span class="comment">-- with the linear model.</span></div><div class="line"> </div><div class="line"><span class="comment">-- The linear model must be held in a container. A sequential container</span></div><div class="line"><span class="comment">-- is appropriate since the outputs of each module become the inputs of</span></div><div class="line"><span class="comment">-- the subsequent module in the model. In this case, there is only one</span></div><div class="line"><span class="comment">-- module. In more complex cases, multiple modules can be stacked using</span></div><div class="line"><span class="comment">-- the sequential container.</span></div><div class="line"> </div><div class="line"><span class="comment">-- The modules are all defined in the neural network package, which is</span></div><div class="line"><span class="comment">-- named 'nn'.</span></div><div class="line"> </div><div class="line">model = nn.Sequential()                 <span class="comment">-- define the container</span></div><div class="line">ninputs = <span class="number">2</span>; noutputs = <span class="number">1</span></div><div class="line">model:add(nn.Linear(ninputs, noutputs)) <span class="comment">-- define the only module</span></div><div class="line"> </div><div class="line"> </div><div class="line"><span class="comment">----------------------------------------------------------------------</span></div><div class="line"><span class="comment">-- 3. Define a loss function, to be minimized.</span></div><div class="line"> </div><div class="line"><span class="comment">-- In that example, we minimize the Mean Square Error (MSE) between</span></div><div class="line"><span class="comment">-- the predictions of our linear model and the groundtruth available</span></div><div class="line"><span class="comment">-- in the dataset.</span></div><div class="line"> </div><div class="line"><span class="comment">-- Torch provides many common criterions to train neural networks.</span></div><div class="line"> </div><div class="line">criterion = nn.MSECriterion()</div><div class="line"> </div><div class="line"> </div><div class="line"><span class="comment">----------------------------------------------------------------------</span></div><div class="line"><span class="comment">-- 4. Train the model</span></div><div class="line"> </div><div class="line"><span class="comment">-- To minimize the loss defined above, using the linear model defined</span></div><div class="line"><span class="comment">-- in 'model', we follow a stochastic gradient descent procedure (SGD).</span></div><div class="line"> </div><div class="line"><span class="comment">-- SGD is a good optimization algorithm when the amount of training data</span></div><div class="line"><span class="comment">-- is large, and estimating the gradient of the loss function over the</span></div><div class="line"><span class="comment">-- entire training set is too costly.</span></div><div class="line"> </div><div class="line"><span class="comment">-- Given an arbitrarily complex model, we can retrieve its trainable</span></div><div class="line"><span class="comment">-- parameters, and the gradients of our loss function wrt these</span></div><div class="line"><span class="comment">-- parameters by doing so:</span></div><div class="line"> </div><div class="line">x, dl_dx = model:getParameters()</div><div class="line"> </div><div class="line"><span class="comment">-- In the following code, we define a closure, feval, which computes</span></div><div class="line"><span class="comment">-- the value of the loss function at a given point x, and the gradient of</span></div><div class="line"><span class="comment">-- that function with respect to x. x is the vector of trainable weights,</span></div><div class="line"><span class="comment">-- which, in this example, are all the weights of the linear matrix of</span></div><div class="line"><span class="comment">-- our model, plus one bias.</span></div><div class="line"> </div><div class="line">feval = <span class="function"><span class="keyword">function</span><span class="params">(x_new)</span></span></div><div class="line">   <span class="comment">-- set x to x_new, if differnt</span></div><div class="line">   <span class="comment">-- (in this simple example, x_new will typically always point to x,</span></div><div class="line">   <span class="comment">-- so the copy is really useless)</span></div><div class="line">   <span class="keyword">if</span> x ~= x_new <span class="keyword">then</span></div><div class="line">      x:copy(x_new)</div><div class="line">   <span class="keyword">end</span></div><div class="line"> </div><div class="line">   <span class="comment">-- select a new training sample</span></div><div class="line">   _nidx_ = (_nidx_ <span class="keyword">or</span> <span class="number">0</span>) + <span class="number">1</span></div><div class="line">   <span class="keyword">if</span> _nidx_ &gt; (#data)[<span class="number">1</span>] <span class="keyword">then</span> _nidx_ = <span class="number">1</span> <span class="keyword">end</span></div><div class="line"> </div><div class="line">   <span class="keyword">local</span> sample = data[_nidx_]</div><div class="line">   <span class="keyword">local</span> target = sample[&#123; &#123;<span class="number">1</span>&#125; &#125;]      <span class="comment">-- this funny looking syntax allows</span></div><div class="line">   <span class="keyword">local</span> inputs = sample[&#123; &#123;<span class="number">2</span>,<span class="number">3</span>&#125; &#125;]    <span class="comment">-- slicing of arrays.</span></div><div class="line"> </div><div class="line">   <span class="comment">-- reset gradients (gradients are always accumulated, to accomodate</span></div><div class="line">   <span class="comment">-- batch methods)</span></div><div class="line">   dl_dx:zero()</div><div class="line"> </div><div class="line">   <span class="comment">-- evaluate the loss function and its derivative wrt x, for that sample</span></div><div class="line">   <span class="keyword">local</span> loss_x = criterion:forward(model:forward(inputs), target)</div><div class="line">   model:backward(inputs, criterion:backward(model.output, target))</div><div class="line"> </div><div class="line">   <span class="comment">-- return loss(x) and dloss/dx</span></div><div class="line">   <span class="keyword">return</span> loss_x, dl_dx</div><div class="line"><span class="keyword">end</span></div><div class="line"> </div><div class="line"><span class="comment">-- Given the function above, we can now easily train the model using SGD.</span></div><div class="line"><span class="comment">-- For that, we need to define four key parameters:</span></div><div class="line"><span class="comment">--   + a learning rate: the size of the step taken at each stochastic</span></div><div class="line"><span class="comment">--     estimate of the gradient</span></div><div class="line"><span class="comment">--   + a weight decay, to regularize the solution (L2 regularization)</span></div><div class="line"><span class="comment">--   + a momentum term, to average steps over time</span></div><div class="line"><span class="comment">--   + a learning rate decay, to let the algorithm converge more precisely</span></div><div class="line"> </div><div class="line">sgd_params = &#123;</div><div class="line">   learningRate = <span class="number">1e-3</span>,</div><div class="line">   learningRateDecay = <span class="number">1e-4</span>,</div><div class="line">   weightDecay = <span class="number">0</span>,</div><div class="line">   momentum = <span class="number">0</span></div><div class="line">&#125;</div><div class="line"> </div><div class="line"><span class="comment">-- We're now good to go... all we have left to do is run over the dataset</span></div><div class="line"><span class="comment">-- for a certain number of iterations, and perform a stochastic update</span></div><div class="line"><span class="comment">-- at each iteration. The number of iterations is found empirically here,</span></div><div class="line"><span class="comment">-- but should typically be determinined using cross-validation.</span></div><div class="line"> </div><div class="line"><span class="comment">-- we cycle 1e4 times over our training data</span></div><div class="line"><span class="keyword">for</span> i = <span class="number">1</span>,<span class="number">1e4</span> <span class="keyword">do</span></div><div class="line"> </div><div class="line">   <span class="comment">-- this variable is used to estimate the average loss</span></div><div class="line">   current_loss = <span class="number">0</span></div><div class="line"> </div><div class="line">   <span class="comment">-- an epoch is a full loop over our training data</span></div><div class="line">   <span class="keyword">for</span> i = <span class="number">1</span>,(#data)[<span class="number">1</span>] <span class="keyword">do</span></div><div class="line"> </div><div class="line">      <span class="comment">-- optim contains several optimization algorithms.</span></div><div class="line">      <span class="comment">-- All of these algorithms assume the same parameters:</span></div><div class="line">      <span class="comment">--   + a closure that computes the loss, and its gradient wrt to x,</span></div><div class="line">      <span class="comment">--     given a point x</span></div><div class="line">      <span class="comment">--   + a point x</span></div><div class="line">      <span class="comment">--   + some parameters, which are algorithm-specific</span></div><div class="line"> </div><div class="line">      _,fs = optim.sgd(feval,x,sgd_params)</div><div class="line"> </div><div class="line">      <span class="comment">-- Functions in optim all return two things:</span></div><div class="line">      <span class="comment">--   + the new x, found by the optimization method (here SGD)</span></div><div class="line">      <span class="comment">--   + the value of the loss functions at all points that were used by</span></div><div class="line">      <span class="comment">--     the algorithm. SGD only estimates the function once, so</span></div><div class="line">      <span class="comment">--     that list just contains one value.</span></div><div class="line"> </div><div class="line">      current_loss = current_loss + fs[<span class="number">1</span>]</div><div class="line">   <span class="keyword">end</span></div><div class="line"> </div><div class="line">   <span class="comment">-- report average error on epoch</span></div><div class="line">   current_loss = current_loss / (#data)[<span class="number">1</span>]</div><div class="line">   <span class="built_in">print</span>(<span class="string">'current loss = '</span> .. current_loss)</div><div class="line"> </div><div class="line"><span class="keyword">end</span></div><div class="line"> </div><div class="line"> </div><div class="line"><span class="comment">----------------------------------------------------------------------</span></div><div class="line"><span class="comment">-- 5. Test the trained model.</span></div><div class="line"> </div><div class="line"><span class="comment">-- Now that the model is trained, one can test it by evaluating it</span></div><div class="line"><span class="comment">-- on new samples.</span></div><div class="line"> </div><div class="line"><span class="comment">-- The text solves the model exactly using matrix techniques and determines</span></div><div class="line"><span class="comment">-- that</span></div><div class="line"><span class="comment">--   corn = 31.98 + 0.65 * fertilizer + 1.11 * insecticides</span></div><div class="line"> </div><div class="line"><span class="comment">-- We compare our approximate results with the text's results.</span></div><div class="line"> </div><div class="line">text = &#123;<span class="number">40.32</span>, <span class="number">42.92</span>, <span class="number">45.33</span>, <span class="number">48.85</span>, <span class="number">52.37</span>, <span class="number">57</span>, <span class="number">61.82</span>, <span class="number">69.78</span>, <span class="number">72.19</span>, <span class="number">79.42</span>&#125;</div><div class="line"> </div><div class="line"><span class="built_in">print</span>(<span class="string">'id  approx   text'</span>)</div><div class="line"><span class="keyword">for</span> i = <span class="number">1</span>,(#data)[<span class="number">1</span>] <span class="keyword">do</span></div><div class="line">   <span class="keyword">local</span> myPrediction = model:forward(data[i][&#123;&#123;<span class="number">2</span>,<span class="number">3</span>&#125;&#125;])</div><div class="line">   <span class="built_in">print</span>(<span class="built_in">string</span>.format(<span class="string">"%2d  %6.2f %6.2f"</span>, i, myPrediction[<span class="number">1</span>], text[i]))</div><div class="line"><span class="keyword">end</span></div></pre></td></tr></table></figure>
</div><!-- comment system--><div class="container"><hr><div data-thread-key="2016/07/14/linear regression example/" data-title="linear regression example" data-url="http://www.sgq.mobi/2016/07/14/linear regression example/" class="ds-thread"></div><script>var duoshuoQuery = {short_name:'sgqmobi'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">Â© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>