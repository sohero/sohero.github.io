<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Machine Learning Theory | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Machine Learning Theory</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-10-09</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/foundation/">foundation</a></div></div></div><article><div class="container post"><blockquote>
<p><strong>Core</strong>: 说明了the learning problem is solvable. 结论在最后一部分加粗字体。Part 1给出了learning problem基于统计的推导。</p>
</blockquote>
<h1 id="part-1"><a href="https://mostafa-samir.github.io/ml-theory-pt1/" title="Part 1" target="_blank" rel="external">Part 1</a></h1>
<h2 id="the-target-function">The Target Function</h2>
<p>There are two fundamental statistics we can use to decompose a random variable: these are the <a href="https://en.wikipedia.org/wiki/Expected_value" target="_blank" rel="external">mean (or the expected value)</a> and the <a href="https://en.wikipedia.org/wiki/variance" target="_blank" rel="external">variance</a>. The <strong>mean</strong> is the value around which the random variable is centered, and the <strong>variance</strong> is the measure of how the random variable is distributed around the mean. Given two random variables <span class="math inline">\(V\)</span> and <span class="math inline">\(W\)</span>, we can say that: <span class="math display">\[
V = \mathbb{E}[V|W] + (V - \mathbb{E}[V|W])
\]</span> where <span class="math inline">\(\mathbb{E}[V|W]\)</span> is the conditional mean of the random variable <span class="math inline">\(V\)</span> given <span class="math inline">\(W\)</span>. This essentially means that we can decompose the value of <span class="math inline">\(V\)</span> into two parts: the first can be explained in terms of the other variable <span class="math inline">\(W\)</span>, and another <em>noisy</em> part that cannot be explained by <span class="math inline">\(W\)</span>.</p>
<p>We can denote the unexplained part as an indepentdent random variable <span class="math inline">\(Z = V - \mathbb{E}[V|W]\)</span>. It easy to see (using the <a href="https://en.wikipedia.org/wiki/Law_of_total_expectation" target="_blank" rel="external">law of total expectation</a>) that the mean of the <span class="math inline">\(Z\)</span> is zero. Hence <span class="math inline">\(Z\)</span> is a source of pure variance, that is the variance in <span class="math inline">\(V\)</span> that cannot be explained by <span class="math inline">\(W\)</span>.</p>
<blockquote>
<p><strong>Law of Total Expectation</strong>: the expected value of the conditional expected value of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> is the same as the expected value of <span class="math inline">\(X\)</span>, <span class="math inline">\(E(X)=E(E(X|Y))\)</span>.</p>
</blockquote>
<p>Now we can write the relationship between any two associated realizations <span class="math inline">\((w_i, v_i)\)</span> of <span class="math inline">\(W\)</span> and <span class="math inline">\(V\)</span> as: <span class="math display">\[
v_i = \mathbb{E}[V|W=w_i] + \zeta
\]</span> where <span class="math inline">\(\zeta\)</span> is a realization of the noise variable <span class="math inline">\(Z\)</span>, we call that the <strong>noise term</strong>. We can apply the same reasoning on the statiscal model to get the following for any realization <span class="math inline">\((x_i, y_i)\)</span>. <span class="math display">\[
y_i = \mathbb{E}[Y|X=x_i] + \zeta
\]</span> Now It’s easy to notice that there is some function <span class="math inline">\(f:\mathcal{X} \rightarrow \mathcal{Y}\)</span> such that: <span class="math display">\[
\mathbb{E}[Y|X=x] = f(x)
\]</span> That is, the conditional expectation is a function of the realizations <span class="math inline">\(x\)</span> that maps the input space <span class="math inline">\(\mathcal X\)</span> to the output space <span class="math inline">\(\mathcal Y\)</span>. Now we can describe the relation between the features and the labels using the formula: <span class="math display">\[
y = f(x) + \zeta
\]</span> Thus abstracting any mention of the conditional distribution <span class="math inline">\(P(Y|X)\)</span>. Now we can use the function <span class="math inline">\(f=f(x)\)</span>, which we call the <strong>target function</strong>, as the proxy for the conditional distribution. The statistical model now simplifies to the following.</p>
<h2 id="the-loss-function">The Loss Function</h2>
<p>Using the loss function, we can calculate the performance of a hypothesis function <span class="math inline">\(h\)</span> on the entire dataset by taking the mean of the losses on each sample. We call this quantity the <strong>in-sample error</strong>, or as we’ll call it from now on: <strong>the empirical risk</strong>: <span class="math display">\[
R_{\text{emp}}(h) = \frac{1}{m}\sum_{i=1}^{m}L(y_i, h(x_i))
\]</span></p>
<h2 id="the-generalization-error">The Generalization Error</h2>
<p>Remember that the goal is to learn the probability distribution underlying the dataset, not just do well on the dataset samples. That means that the hypothesis should also have low errors on new unseen data samples from the distribution.</p>
<p>For the hypothesis to perform well on unseen new data is for the hypothesis to <strong>generalize</strong> over the underlying probability distribution. We formally capture that by defining the <strong>generalization error</strong> (also referred to as the <strong>risk</strong>), it simply the expected value of the loss over the whole joint distribution <span class="math inline">\(P(X,Y)\)</span>: <span class="math display">\[
R(h) = \mathbb{E}_{(x,y) \sim P(X,Y)}[L(y, h(x))]
\]</span> Now we can say that the solution to the learning problem is the hypothesis with the least generalization error <span class="math inline">\(R\)</span>.</p>
<p>This question boils down to calculating the following probability: <span class="math display">\[
\mathbb{P}[\sup_{h \in \mathcal{H}}|R(h) - R_{\text{emp}}(h)| &gt; \epsilon]
\]</span> That is the probability that the least upper bound (that is the <a href="https://en.wikipedia.org/wiki/Infimum_and_supremum" target="_blank" rel="external">supremum</a> <span class="math inline">\(\sup_{h \in \mathcal{H}}\)</span>) of the absolute difference between <span class="math inline">\(R\)</span> and <span class="math inline">\(R_{emp}\)</span> is larger than a very small value <span class="math inline">\(\epsilon\)</span>. If this probability is sufficiently small, that is there is a very little change that <span class="math inline">\(R_{emp}\)</span> differs much than <span class="math inline">\(R\)</span>, then the learning problem is solvable.</p>
<h1 id="part-2"><a href="https://mostafa-samir.github.io/ml-theory-pt2/" title="Part 2" target="_blank" rel="external">Part 2</a></h1>
<h2 id="independently-and-identically-distributed">Independently, and Identically Distributed</h2>
<p>A reasonable assumption we can make about the problem we have at hand is that our training dataset samples are <strong>independently, and identically distributed</strong>(or <em>i.i.d.</em> for short), that means that all the samples are drawn from the same probability distribution and that each sample is independent from the others.</p>
<p><strong>The law of large numbers</strong>: If <span class="math inline">\(x_1,x_2,...,x_m\)</span> are <span class="math inline">\(m\)</span> i.i.d. samples of a random variable <span class="math inline">\(X\)</span> distributed by <span class="math inline">\(P\)</span>. then for a small positive non-zero value <span class="math inline">\(\epsilon\)</span>: <span class="math display">\[
\lim_{m \rightarrow \infty} \mathbb{P}\left[\left|\mathop{\mathbb{E}}_{X \sim P}[X] - \frac{1}{m}\sum_{i=1}^{m}x_i \right| &gt; \epsilon\right] = 0
\]</span> This version of law if called the <strong>weak law of large nubmers</strong>. It’s weak because it guarantees that as the sample size goes larger, the sample and true means will likely be very close to each other by a non-zero distance no greater than epsilon. On the other hand, the strong version says that with very large sample size, the sample mean is almost surely equal to the true mean.</p>
<h2 id="hoeffdings-inequality">Hoeffding’s inequality</h2>
<p>To our destination of ensuring that the training and generalization errors do not differ much, we need to know more info about the how the road down the law of large numbers look like. These info are provided by what we call the <a href="https://en.wikipedia.org/wiki/Concentration_inequality" target="_blank" rel="external"><strong>concentration inequalities</strong></a>. This is a set of inequalities that quantifies how much random variables (or function of them) deviate from their expected values (or, also, functions of them). One inequality of those is <strong>Heoffding’s inequality</strong>: If <span class="math inline">\(x_1,x_2,...,x_m\)</span> are <span class="math inline">\(m\)</span> i.i.d. samples of a random variable <span class="math inline">\(X\)</span> distributed by <span class="math inline">\(P\)</span>, and <span class="math inline">\(a\leq x_i \leq b\)</span> for every <span class="math inline">\(i\)</span>, then for a small positive non-zero value <span class="math inline">\(\epsilon\)</span>: <span class="math display">\[
\mathbb{P}\left[\left|\mathop{\mathbb{E}}_{X \sim P}[X] - \frac{1}{m}\sum_{i=0}^{m}x_i\right| &gt; \epsilon\right] \leq 2\exp\left(\frac{-2m\epsilon^2}{(b -a)^2}\right)
\]</span></p>
<p>You probably see why we specifically chose Heoffding’s inequality from among the others. We can naturally apply this inequality to our generalization probability, assuming that our errors are bounded between 0 and 1 (which is a reasonable assumption, as we can get that using a 0/1 loss function or by squashing any other loss between 0 and 1) and get for a <strong>single</strong> hypothesis <span class="math inline">\(h\)</span>: <span class="math display">\[
\mathbb{P}[|R(h) - R_{\text{emp}}(h)| &gt; \epsilon] \leq 2\exp(-2m\epsilon^2)
\]</span></p>
<p>This means that the probability of the difference between the training and the generalization errors exceeding <span class="math inline">\(\epsilon\)</span> exponentially decays as the dataset size goes larger. This should align well with our practical experience that the bigger the dataset gets, the better the results becone.</p>
<p>If you noticed, all our analysis up till now was focusing on a <strong>single</strong> hypothesis <span class="math inline">\(h\)</span>. But the learning problem doesn’t know that single hypothesis beforehand, it needs to pick one out of an entire hypothesis space <span class="math inline">\(\mathcal H\)</span>, so we need a generalization bound that reflects the chanllenge of choosing the right hypothesis.</p>
<h2 id="generalization-bound-1st-attempt">Generalization Bound: 1st Attempt</h2>
<p>In order for the entire hypothesis space to have a generalization gap bigger than <span class="math inline">\(\epsilon\)</span>, at least one of its hypothesis: <span class="math inline">\(h_1\)</span> or <span class="math inline">\(h_2\)</span> or <span class="math inline">\(h_3\)</span> or … etc should have. This can be expressed formally by staing that: <span class="math display">\[
\mathbb{P}\left[\sup_{h \in \mathcal{H}}|R(h) - R_\text{emp}(h)| &gt; \epsilon\right] = \mathbb{P}\left[\bigcup_{h \in \mathcal{H}} |R(h) - R_\text{emp}(h)| &gt; \epsilon\right]
\]</span></p>
<p>Where <span class="math inline">\(\bigcup\)</span> denotes the union of the events, which <a href="https://en.wikipedia.org/wiki/Logical_disjunction#Union" target="_blank" rel="external">also corresponds</a> to the logical<strong>OR</strong> operator. Using the <a href="https://en.wikipedia.org/wiki/Boole%27s_inequality" target="_blank" rel="external">union bound inequality</a>, we get: <span class="math display">\[
\mathbb{P}\left[\sup_{h \in \mathcal{H}}|R(h) - R_\text{emp}(h)| &gt; \epsilon\right] \leq \sum_{h \in \mathcal{H}} \mathbb{P}[|R(h) - R_\text{emp}(h)| &gt; \epsilon]
\]</span></p>
<p>We exactly know the bound on the probability under the summation from our analysis using the Heoffding’s inequality, so we end up with: <span class="math display">\[
\mathbb{P}\left[\sup_{h \in \mathcal{H}}|R(h) - R_\text{emp}(h)| &gt; \epsilon\right] \leq 2|\mathcal{H}|\exp(-2m\epsilon^2)
\]</span></p>
<p>Where <span class="math inline">\(\mathcal H\)</span> is the size of the hypothesis space. By denoting the right hand side of the above inequality by <span class="math inline">\(\delta\)</span>, we can say that with a confidence <span class="math inline">\(1-\delta\)</span>: <span class="math display">\[
|R(h) - R_\text{emp}| \leq \epsilon \Rightarrow R(h) \leq R_\text{emp}(h) + \epsilon
\]</span></p>
<p>And with some basic algebra, we can express <span class="math inline">\(\epsilon\)</span> in terms of <span class="math inline">\(\delta\)</span> and get: <span class="math display">\[
R(h) \leq R_\text{emp}(h) + \sqrt{\frac{\ln|\mathcal{H}| + \ln{\frac{2}{\delta}}}{2m}}
\]</span></p>
<p>This is our first generalization bound, it states that the generalization error is bounded by the training error plus a function of the hypothesis space size and the dataset size. We can also see that the bigger the hypothesis space gets, the bigger the generalization error becomes. This explains why the memorization hypothesis form last time, which theoretically has <span class="math inline">\(|\mathcal H|=\infty\)</span>, fails miserably as a solution to the learning problem depite having <span class="math inline">\(R_{emp}=0\)</span>; because for the memorization hypothesis <span class="math inline">\(h_{mem}\)</span>: <span class="math display">\[
R(h_\text{mem}) \leq 0 + \infty \leq \infty
\]</span></p>
<p>But wait a second! For a linear hypothesis of the form <span class="math inline">\(h(x)=wx+b\)</span>, we also have <span class="math inline">\(|\mathcal H|=\infty\)</span> as there is infinitely many lines that can be draw. So the generalization error of the linear hypothesis space should be unbounded just as the memorization hypothesis! If that’s true, why does perceptrons, logistic regression, support vector machines and essentially any ML model that uses a linear hypothesis work?</p>
<p>Our theoretical result was able to account for some phenomena (the memorization hypothesis, and any finite hypothesis space) but not for others (the linear hypothesis, or other infinite hypothesis spaces that empirically work). This means that there’s still something missing from our theoretical model, and it’s time for us to revise our steps. A good starting point is from the source of the problem itself, which is the infinity in <span class="math inline">\(|\mathcal H|\)</span>.</p>
<p>Notice that the term <span class="math inline">\(|\mathcal H|\)</span> resulted from our use of the union bound. The basic idea of the union bound is that it bounds the probability by the worst case possible, which is when all the events under union are independent. <strong>This bound gets more tight as the events under consideration get less dependent.</strong> In our case, for the bound to be tight and reasonable, we need the following to be true:</p>
<blockquote>
<p>For every two hypothesis <span class="math inline">\(h_1,h_2 \in \mathcal H\)</span>, the two events <span class="math inline">\(|R(h_1) - R_\text{emp}(h_1)| &gt; \epsilon\)</span> and <span class="math inline">\(|R(h_2) - R_\text{emp}(h_2)| &gt; \epsilon\)</span> are likely to be independent. This means that the event that <span class="math inline">\(h_1\)</span> has a generalization gap bigger than <span class="math inline">\(\epsilon\)</span> should be independent of the event that also <span class="math inline">\(h_2\)</span> has a generalization gap bigger than <span class="math inline">\(\epsilon\)</span>, no matter how much <span class="math inline">\(h_1\)</span> and <span class="math inline">\(h_2\)</span> are close or related; the events should be coincidental.</p>
</blockquote>
<p>But is that true?</p>
<h2 id="examining-the-independence-assumption">Examining the Independence Assumption</h2>
<p>The formulation of the generalization inequality reveals a main reason why we need to consider all the hypothesis in <span class="math inline">\(\mathcal H\)</span>. It has to do with the existence of <span class="math inline">\(\sup_{h \in \mathcal{H}}\)</span>. The supremum in the inequality guarantees that there’s a very little chance that the biggest generalization gap possible is greater than <span class="math inline">\(\epsilon\)</span>; this is a strong claim and if we omit a single hypothesis out of <span class="math inline">\(\mathcal H\)</span>, we might miss that “biggest generalization gap possible” and lose that strength, and that’s something we cannot afford to lose. We need to be able to make that claim to ensure that the learning algorithm would never land on a hypothesis with a bigger generalization gap than <span class="math inline">\(\epsilon\)</span>.</p>
<figure>
<img src="/images/hyp_rainbow.png">
</figure>
<p>Looking at the above plot of binary classification problem, it’s clear that this rainbow of hypothesis produces the same classification on the data points, so all of them have the same empirical risk. So one might think, as they all have the same <span class="math inline">\(R_{emp}\)</span>, why not choose one and omit the others?!</p>
<p>This would be a very good solution if we’re only inerested in the empirical risk, but our inequality takes into its consideration the out-of-sample risk as well, which is expressed as : <span class="math display">\[
R(h) = \mathop{\mathbb{E}}_{(x,y) \sim P}[L(y, h(x))] = \int_{\mathcal{Y}}\int_{\mathcal{X}}L(y, h(x))P(x, y)\,\mathrm{d}x \,\mathrm{d}y
\]</span></p>
<p>This is an integration over every possible combination of the whole input and output spaces <span class="math inline">\(\mathcal{X,Y}\)</span>. So in order to ensure our supremum claim, we need the hypothesis to cover the whole of <span class="math inline">\(\mathcal{X \times Y}\)</span>, hence we need all the possible hypothesis in <span class="math inline">\(\mathcal H\)</span>.</p>
<p>Now that we’ve established that we do need to consider every single hypothesis in <span class="math inline">\(\mathcal H\)</span>, we can ask ourselves: <strong>are the events of each hypothesis having a big generaliation gap are likely to be independent?</strong></p>
<p>Well, Not even close! Take for example the rainbow of hypotheses in the above plot, it’s very clear that if the red hypothesis has a generalization gap greater than <span class="math inline">\(\epsilon\)</span>, then, with 100% certainty, every hypothesis with the same slope in the region above it will also have that. The same argument can be made for many different regions in the <span class="math inline">\(\mathcal{X \times Y}\)</span> space with different degrees of certainty as in the following figure.</p>
<figure>
<img src="/images/regions.png">
</figure>
<p>But this is not helpful for our mathematical analysis, as the regions seems to be dependent on the distribution of the sample points and there is no way we can precisely capture these dependencies mathematically, and we cannot make assumptions about them without risking to compromise the supremum claim.</p>
<p>So the union bound and the independence assumption seem like the best approximation we can make,but it highly overestimates the probability and makes the bound very loose, and very pessimistic!</p>
<p>However, what if somehow we can get a very good estimate of the risk <span class="math inline">\(R(h)\)</span> without needing to go over the whole of the <span class="math inline">\(\mathcal{X \times Y}\)</span> space, would there be any hope to get a better bound?</p>
<h2 id="the-symmetrization-lemma">The Symmetrization Lemma</h2>
<p>Let’s think for a moment about something we do usually in machine learning practice. In order to measure the accuracy of our model, we hold out a part of the training set to evaluate the model on after training, and we consider the model’s accuracy on this left out portion as an estimate for the generalization error. This works because we assume that this <strong>test set is drawn i.i.d. from the same distribution of the training set</strong> (this is why we usually shuffle the whole dataset beforehand to break any correlation between the samples).</p>
<p>It turns out that we can do a similar thing mathematically, but instead of taking out a portion of our dataset <span class="math inline">\(S\)</span>, we imagine that we have another dataset <span class="math inline">\(S′\)</span> with also size <span class="math inline">\(m\)</span>, we call this the <strong>ghost dataset</strong>. Note that this has no practical implications, we don’t need to have another dataset at training, it’s just a mathematical trick we’re gonna use to git rid of the restrictions of <span class="math inline">\(R(h)\)</span> in the inequality.</p>
<p>We’re not gonna go over the proof here, but using that ghost dataset one can actually prove that: <span class="math display">\[
\mathbb{P}\left[\sup_{h \in \mathcal{H}}\left|R(h) - R_\text{emp}(h)\right| &gt; \epsilon\right] \leq 2\mathbb{P}\left[\sup_{h \in \mathcal{H}}\left|R_\text{emp}(h) - R_\text{emp}&#39;(h)\right| &gt; \frac{\epsilon}{2}\right] \hspace{2em} (1)
\]</span></p>
<p>where <span class="math inline">\(R′_{emp}(h)\)</span> is the empirical risk of hypothesis <span class="math inline">\(h\)</span> on the ghost dataset. This means that the probability of the largest generalization gap being bigger than <span class="math inline">\(\epsilon\)</span> is at most twice the probability that the empirical risk difference between <span class="math inline">\(S,S′\)</span> is larger than <span class="math inline">\(\frac{\epsilon}{2}\)</span>. Now that the right hand side in expressed only in terms of empirical risks, we can bound it without needing to consider the the whole of <span class="math inline">\(\mathcal{X \times Y}\)</span>, and hence we can bound the term with the risk <span class="math inline">\(R(h)\)</span> without considering the whole of input and output spaces!</p>
<p>This, which is called the <strong>symmetrization lemma</strong>, was one of the two key parts in the work of Vapnik-Chervonenkis (1971).</p>
<h2 id="the-growth-function">The Growth Function</h2>
<p>Now that we are bounding only the empirical risk, if we have many hypotheses that have the same empirical risk (a.k.a. producing the same labels/values on the data points), we can safely choose one of them as a representative of the whole group, we’ll call that an <strong>effective</strong> hypothesis, and discard all the others.</p>
<p>By only choosing the distinct effective hypotheses on the dataset <span class="math inline">\(S\)</span>, we restrict the hypothesis space <span class="math inline">\(H\)</span> to a smaller subspace that depends on the dataset <span class="math inline">\(\mathcal{H}_{|S}\)</span>.</p>
<p>We can assume the independence of the hypotheses in <span class="math inline">\(\mathcal{H}_{|S}\)</span> like we did before with <span class="math inline">\(H\)</span> (but it’s more plausible now), and use the union bound to get that: <span class="math display">\[
\mathbb{P}\left[\sup_{h \in \mathcal{H}_{|S\cup S&#39;}}\left|R_\text{emp}(h) - R_\text{emp}&#39;(h)\right|&gt;\frac{\epsilon}{2}\right] \leq \left|\mathcal{H}_{|S\cup S&#39;}\right| \mathbb{P}\left[\left|R_\text{emp}(h) - R_\text{emp}&#39;(h)\right|&gt;\frac{\epsilon}{2}\right]
\]</span> Notice that the hypothesis space is restricted by <span class="math inline">\(S \cup S&#39;\)</span> because we using the empirical risk on both the original dataset <span class="math inline">\(S\)</span> and the ghost <span class="math inline">\(S&#39;\)</span>. The question now is what is the maximum size of a restricted hypothesis space? The answer is very simple; we consider a hypothesis to be a new effective one if it produces new labels/values on the dataset samples, then the maximum number of distinct hypothesis (a.k.a the maximum number of the restricted space) is the maximum number of distinct labels/values the dataset points can take. A cool feature about that maximum size is that its a combinatorial measure, so we don’t need to worry about how the samples are distributed!</p>
<p>For simplicity, we’ll focus now on the case of binary classification, in which <span class="math inline">\(\mathcal Y=\\{−1,+1\\}\)</span>. Later we’ll show that the same concepts can be extended to both multiclass classification and regression. In that case, for a dataset with <span class="math inline">\(m\)</span> samples, each of which can take one of two labels: either -1 or +1, the maximum number of distinct labellings is <span class="math inline">\(2^m\)</span>.</p>
<p>We’ll define the maximum number of distinct labellings/values on a dataset <span class="math inline">\(S\)</span> of size <span class="math inline">\(m\)</span> by a hypothesis space <span class="math inline">\(\mathcal H\)</span> as the <strong>growth function</strong> of <span class="math inline">\(H\)</span> given <span class="math inline">\(m\)</span>, and we’ll denote that by <span class="math inline">\(\Delta_\mathcal{H}(m)\)</span>. It’s called the growth function because it’s value for a single hypothesis space <span class="math inline">\(H\)</span> (aka the size of the restricted subspace <span class="math inline">\(\mathcal{H_{|S}}\)</span>) grows as the size of the dataset grows. Now we can say that: <span class="math display">\[
\mathbb{P}\left[\sup_{h \in \mathcal{H}_{|S\cup S&#39;}}\left|R_\text{emp}(h) - R_\text{emp}&#39;(h)\right|&gt;\frac{\epsilon}{2}\right] \leq \Delta_\mathcal{H}(2m) \mathbb{P}\left[\left|R_\text{emp}(h) - R_\text{emp}&#39;(h)\right|&gt;\frac{\epsilon}{2}\right] \hspace{2em} (2)
\]</span> Notice that we used <span class="math inline">\(2m\)</span> because we have two datasets <span class="math inline">\(S,S&#39;\)</span> each with size <span class="math inline">\(m\)</span>.</p>
<p>For the binary classification case, we can say that: <span class="math display">\[
\Delta_\mathcal{H}(m) \leq 2^m
\]</span> But <span class="math inline">\(2^m\)</span> is exponential in <span class="math inline">\(m\)</span> and would grow too fast for large datasets, which makes the odds in our inequality go too bad too fast! Is that the best bound we can get on that growth function?</p>
<h2 id="the-vc-dimension">The VC-Dimension</h2>
<p>The <span class="math inline">\(2^m\)</span> bound is based on the fact that the hypothesis space <span class="math inline">\(\mathcal H\)</span> can produce all the possible labellings on the <span class="math inline">\(m\)</span> data points. If a hypothesis space can indeed produce all the possible labels on a set of data points, we say that the hypothesis space <strong>shatters</strong> that set.</p>
<p>But can any hypothesis space shatter any dataset of any size? Let’s investigate that with the binary classification case and the <span class="math inline">\(\mathcal H\)</span> of linear classifiers <span class="math inline">\(\mathrm{sign}(wx + b)\)</span>. The following animation shows how many ways a linear classifier in 2D can label 3 points (on the left) and 4 points (on the right).</p>
<figure>
<img src="/images/shatter.gif">
</figure>
<p>In the animation, the whole space of possible effective hypotheses is swept. For the the three points, the hypothesis shattered the set of points and produced all the possible <span class="math inline">\(2^3=8\)</span> labellings. However for the four points,the hypothesis couldn’t get more than 14 and never reached <span class="math inline">\(2^4=16\)</span>, so it failed to shatter this set of points. Actually, no linear classifier in 2D can shatter any set of 4 points, not just that set; because there will always be two labellings that cannot be produced by a linear classifier which is depicted in the following figure.</p>
<figure>
<img src="/images/impossible-dichotomy.png">
</figure>
<p>From the decision boundary plot (on the right), it’s clear why no linear classifier can produce such labellings; as no linear classifier can divide the space in this way. So it’s possible for a hypothesis space <span class="math inline">\(\mathcal H\)</span> to be unable to shatter all sizes. This fact can be used to get a better bound on the growth function, and this is done using <strong>Sauer’s lemma</strong>:</p>
<blockquote>
<p>If a hypothesis space <span class="math inline">\(\mathcal H\)</span> cannot shatter any dataset with size more than <span class="math inline">\(k\)</span>, then: <span class="math display">\[
\Delta_{\mathcal{H}}(m) \leq \sum_{i=0}^{k}\binom{m}{i}
\]</span></p>
</blockquote>
<p>This was the other key part of Vapnik-Chervonenkis work (1971), but it’s named after another mathematician, Norbert Sauer; because it was independently proved by him around the same time (1972). However, Vapnik and Chervonenkis weren’t completely left out from this contribution; as that <span class="math inline">\(k\)</span>, which is the maximum number of points that can be shattered by <span class="math inline">\(\mathcal H\)</span>, is now called the <em>Vapnik-Chervonenkis-dimension</em> or the <strong>VC-dimension</strong> <span class="math inline">\(d_{vc}\)</span> of <span class="math inline">\(\mathcal H\)</span>.</p>
<p>For the case of the linear classifier in 2D, <span class="math inline">\(d_{vc}=3\)</span>. In general, it can be proved that hyperplane classifiers (the higher-dimensional generalization of line classifiers) in <span class="math inline">\(\mathbb{R}^n\)</span> space has <span class="math inline">\(d_{vc}=n+1\)</span>.</p>
<p>The bound on the growth function provided by sauer’s lemma is indeed much better than the exponential one we already have, it’s actually polynomial! Using algebraic manipulation, we can prove that: <span class="math display">\[
\Delta_\mathcal{H}(m) \leq \sum_{i=0}^{k}\binom{m}{i} \leq \left(\frac{me}{d_\mathrm{vc}}\right)^{d_\mathrm{vc}}\leq O(m^{d_\mathrm{vc}})
\]</span> Where <span class="math inline">\(O\)</span> refers to the <a href="https://en.wikipedia.org/wiki/Big_O_notation" target="_blank" rel="external">Big-O notation</a> for functions asymptotic (near the limits) behavior, and <span class="math inline">\(e\)</span> is the mathematical constant.</p>
<p>Thus we can use the VC-dimension as a proxy for growth function and, hence, for the size of the restricted space <span class="math inline">\(\mathcal{H_{|S}}\)</span>. In that case, <span class="math inline">\(d_{vc}\)</span> would be a measure of the complexity or richness of the hypothesis space.</p>
<h2 id="the-vc-generalization-bound">The VC Generalization Bound</h2>
<p>With a little change in the constants, it can be shown that Heoffding’s inequality is applicable on the probability <span class="math inline">\(\mathbb{P}\left[|R_\mathrm{emp}(h) - R_\mathrm{emp}’(h)| &gt; \frac{\epsilon}{2}\right]\)</span>. With that, With that, and by combining inequalities (1) and (2), the <strong>Vapnik-Chervonenkis theory</strong> follows: <span class="math display">\[
\mathbb{P}\left[\sup_{h \in \mathcal{H}}|R(h) - R_\mathrm{emp}(h)| &gt; \epsilon\right] \leq 4\Delta_\mathcal{H}(2m)\exp\left(-\frac{m\epsilon^2}{8}\right)
\]</span> This can be re-expressed as a bound on the generalization error, just as we did earlier with the previous bound, to get the <strong>VC generalization bound</strong>: <span class="math display">\[
R(h) \leq R_\mathrm{emp}(h) + \sqrt{\frac{8\ln\Delta_\mathcal{H}(2m) + 8\ln\frac{4}{\delta}}{m}}
\]</span> or, by using the bound on growth function in terms of <span class="math inline">\(d_{vc}\)</span> as: <span class="math display">\[
R(h) \leq R_\mathrm{emp}(h) + \sqrt{\frac{8d_\mathrm{vc}(\ln\frac{2m}{d_\mathrm{vc}} + 1) + 8\ln\frac{4}{\delta}}{m}}
\]</span> This is a significant result! It’s a clear and concise mathematical statement that the learning problem is solvable, and that for infinite hypotheses spaces there is a finite bound on the their generalization error! Furthermore, this bound can be described in term of a quantity (<span class="math inline">\(d_{vc}\)</span>), that solely depends on the hypothesis space and not on the distribution of the data points!</p>
<p>Now, in light of these results, is there’s any hope for the memorization hypothesis?</p>
<p>It turns out that there’s still no hope! The memorization hypothesis can shatter any dataset no matter how big it is, that means that its <span class="math inline">\(d_{vc}\)</span> is infinite, yielding an infinite bound on <span class="math inline">\(R(h_{mem})\)</span> as before. However, the success of linear hypothesis can now be explained by the fact that they have a finite <span class="math inline">\(d_{vc}=n+1\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span>. The theory is now consistent with the empirical observations.</p>
<h2 id="distribution-based-bounds">Distribution-Based Bounds</h2>
<p>The fact that <span class="math inline">\(d_{vc}\)</span> is distribution-free comes with a price: by not exploiting the structure and the distribution of the data samples, the bound tends to get loose. Consider for example the case of linear binary classifiers in a very higher n-dimensional feature space, using the distribution-free <span class="math inline">\(d_{vc}=n+1\)</span> means that the bound on the generalization error would be poor unless the size of the dataset <span class="math inline">\(N\)</span> is also very large to balance the effect of the large <span class="math inline">\(d_{vc}\)</span>. This is the good old <em>curse of dimensionality</em> we all know and endure.</p>
<p>However, a careful investigation into the distribution of the data samples can bring more hope to the situation. For example, For data points that are linearly separable, contained in a ball of radius <span class="math inline">\(R\)</span>, with a margin <span class="math inline">\(\rho\)</span> between the closest points in the two classes, one can prove that for a hyperplane classifier: <span class="math display">\[
d_\mathrm{vc} \leq \left\lceil \frac{R^2}{\rho^2} \right\rceil
\]</span> It follows that the larger the margin, the lower the <span class="math inline">\(d_{vc}\)</span> of the hypothesis. This is theoretical motivation behind <strong>Support Vector Machines (SVMs)</strong> which attempts to classify data using the maximum margin hyperplane. This was also proved by Vapnik and Chervonenkis.</p>
<h2 id="one-inequality-to-rule-them-all">One Inequality to Rule Them All</h2>
<p>Up until this point, all our analysis was for the case of binary classification. And it’s indeed true that the form of the vc bound we arrived at here only works for the binary classification case. However, the conceptual framework of VC (that is: shattering, growth function and dimension) generalizes very well to both multi-class classification and regression.</p>
<p>Due to the work of Natarajan (1989), the <strong>Natarajan dimension</strong> is defined as a generalization of the VC-dimension for multiple classes classification, and a bound similar to the VC-Bound is derived in terms of it. Also, through the work of Pollard (1984), the <strong>pseudo-dimension</strong> generalizes the VC-dimension for the regression case with a bound on the generalization error also similar to VC’s.</p>
<p>There is also <em>Rademacher’s complexity</em>, which is a relatively new tool (devised in the 2000s) that measures the richness of a hypothesis space by measuring how well it can fit to random noise. The cool thing about Rademacher’s complexity is that it’s flexible enough to be adapted to any learning problem, and it yields very similar generalization bounds to the other methods mentioned.</p>
<p>However, no matter what the exact form of the bound produced by any of these methods is, it always takes the form: <span class="math display">\[
R(h) \leq R_\mathrm{emp}(h) + C(|\mathcal{H}|, N, \delta)
\]</span> where <span class="math inline">\(C\)</span> is a function of the hypothesis space complexity (or size, or richness), the size of the dataset, and the confidence <span class="math inline">\(1-\delta\)</span> about the bound. <strong>This inequality basically says the generalization error can be decomposed into two parts: the empirical training error, and the complexity of the learning model.</strong></p>
<p>This form of the inequality holds to any learning problem no matter the exact form of the bound, and this is the one we’re gonna use throughout the rest of the series to guide us through the process of machine learning.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:herosgq@gmail.com" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>