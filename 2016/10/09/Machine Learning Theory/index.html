<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Machine Learning Theory | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Machine Learning Theory</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-10-09</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/foundation/">foundation</a></div></div></div><article><div class="container post"><blockquote>
<p><strong>Core</strong>: 说明了the learning problem is solvable. 结论在最后一部分加粗字体。Part 1给出了learning problem基于统计的推导。</p>
</blockquote>
<h1 id="Part-1"><a href="#Part-1" class="headerlink" title="Part 1"></a><a href="https://mostafa-samir.github.io/ml-theory-pt1/" title="Part 1" target="_blank" rel="external">Part 1</a></h1><h2 id="The-Target-Function"><a href="#The-Target-Function" class="headerlink" title="The Target Function"></a>The Target Function</h2><p>There are two fundamental statistics we can use to decompose a random variable: these are the <a href="https://en.wikipedia.org/wiki/Expected_value" target="_blank" rel="external">mean (or the expected value)</a> and the <a href="https://en.wikipedia.org/wiki/variance" target="_blank" rel="external">variance</a>. The <strong>mean</strong> is the value around which the random variable is centered, and the <strong>variance</strong> is the measure of how the random variable is distributed around the mean. Given two random variables $V$ and $W$, we can say that:<br>$$<br>V = \mathbb{E}[V|W] + (V - \mathbb{E}[V|W])<br>$$<br>where $\mathbb{E}[V|W]$ is the conditional mean of the random variable $V$ given $W$. This essentially means that we can decompose the value of $V$ into two parts: the first can be explained in terms of the other variable $W$, and another <em>noisy</em> part that cannot be explained by $W$.</p>
<p>We can denote the unexplained part as an indepentdent random variable $Z = V - \mathbb{E}[V|W]$. It easy to see (using the <a href="https://en.wikipedia.org/wiki/Law_of_total_expectation" target="_blank" rel="external">law of total expectation</a>) that the mean of the $Z$ is zero. Hence $Z$ is a source of pure variance, that is the variance in $V$ that cannot be explained by $W$.</p>
<blockquote>
<p><strong>Law of Total Expectation</strong>: the expected value of the conditional expected value of $X$ given $Y$ is the same as the expected value of $X$, $E(X)=E(E(X|Y))$.</p>
</blockquote>
<p>Now we can write the relationship between any two associated realizations $(w_i, v_i)$ of $W$ and $V$ as:<br>$$<br>v_i = \mathbb{E}[V|W=w_i] + \zeta<br>$$<br>where $\zeta$ is a realization of the noise variable $Z$, we call that the <strong>noise term</strong>. We can apply the same reasoning on the statiscal model to get the following for any realization $(x_i, y_i)$.<br>$$<br>y_i = \mathbb{E}[Y|X=x_i] + \zeta<br>$$<br>Now It’s easy to notice that there is some function $f:\mathcal{X} \rightarrow \mathcal{Y}$ such that:<br>$$<br>\mathbb{E}[Y|X=x] = f(x)<br>$$<br>That is, the conditional expectation is a function of the realizations $x$ that maps the input space $\mathcal X$ to the output space $\mathcal Y$. Now we can describe the relation between the features and the labels using the formula:<br>$$<br>y = f(x) + \zeta<br>$$<br>Thus abstracting any mention of the conditional distribution $P(Y|X)$. Now we can use the function $f=f(x)$, which we call the <strong>target function</strong>, as the proxy for the conditional distribution. The statistical model now simplifies to the following.</p>
<h2 id="The-Loss-Function"><a href="#The-Loss-Function" class="headerlink" title="The Loss Function"></a>The Loss Function</h2><p>Using the loss function, we can calculate the performance of a hypothesis function $h$ on the entire dataset by taking the mean of the losses on each sample. We call this quantity the <strong>in-sample error</strong>, or as we’ll call it from now on: <strong>the empirical risk</strong>:<br>$$<br>R<em>{\text{emp}}(h) = \frac{1}{m}\sum</em>{i=1}^{m}L(y_i, h(x_i))<br>$$</p>
<h2 id="The-Generalization-Error"><a href="#The-Generalization-Error" class="headerlink" title="The Generalization Error"></a>The Generalization Error</h2><p>Remember that the goal is to learn the probability distribution underlying the dataset, not just do well on the dataset samples. That means that the hypothesis should also have low errors on new unseen data samples from the distribution.</p>
<p>For the hypothesis to perform well on unseen new data is for the hypothesis to <strong>generalize</strong> over the underlying probability distribution. We formally capture that by defining the <strong>generalization error</strong> (also referred to as the <strong>risk</strong>), it simply the expected value of the loss over the whole joint distribution $P(X,Y)$:<br>$$<br>R(h) = \mathbb{E}_{(x,y) \sim P(X,Y)}[L(y, h(x))]<br>$$<br>Now we can say that the solution to the learning problem is the hypothesis with the least generalization error $R$.</p>
<p>This question boils down to calculating the following probability:<br>$$<br>\mathbb{P}[\sup<em>{h \in \mathcal{H}}|R(h) - R</em>{\text{emp}}(h)| &gt; \epsilon]<br>$$<br>That is the probability that the least upper bound (that is the <a href="https://en.wikipedia.org/wiki/Infimum_and_supremum" target="_blank" rel="external">supremum</a> $\sup<em>{h \in \mathcal{H}}$) of the absolute difference between $R$ and $R</em>{emp}$ is larger than a very small value $\epsilon$. If this probability is sufficiently small, that is there is a very little change that $R_{emp}$ differs much than $R$, then the learning problem is solvable.</p>
<h1 id="Part-2"><a href="#Part-2" class="headerlink" title="Part 2"></a><a href="https://mostafa-samir.github.io/ml-theory-pt2/" title="Part 2" target="_blank" rel="external">Part 2</a></h1><h2 id="Independently-and-Identically-Distributed"><a href="#Independently-and-Identically-Distributed" class="headerlink" title="Independently, and Identically Distributed"></a>Independently, and Identically Distributed</h2><p>A reasonable assumption we can make about the problem we have at hand is that our training dataset samples are <strong>independently, and identically distributed</strong>(or <em>i.i.d.</em> for short), that means that all the samples are drawn from the same probability distribution and that each sample is independent from the others.</p>
<p><strong>The law of large numbers</strong>: If $x_1,x_2,…,x<em>m$ are $m$ i.i.d. samples of a random variable $X$ distributed by $P$. then for a small positive non-zero value $\epsilon$:<br>$$<br>\lim</em>{m \rightarrow \infty} \mathbb{P}\left[\left|\mathop{\mathbb{E}}<em>{X \sim P}[X] - \frac{1}{m}\sum</em>{i=1}^{m}x_i \right| &gt; \epsilon\right] = 0<br>$$<br>This version of law if called the <strong>weak law of large nubmers</strong>. It’s weak because it guarantees that as the sample size goes larger, the sample and true means will likely be very close to each other by a non-zero distance no greater than epsilon. On the other hand, the strong version says that with very large sample size, the sample mean is almost surely equal to the true mean.</p>
<h2 id="Hoeffding’s-inequality"><a href="#Hoeffding’s-inequality" class="headerlink" title="Hoeffding’s inequality"></a>Hoeffding’s inequality</h2><p>To our destination of ensuring that the training and generalization errors do not differ much, we need to know more info about the how the road down the law of large numbers look like. These info are provided by what we call the <a href="https://en.wikipedia.org/wiki/Concentration_inequality" target="_blank" rel="external"><strong>concentration inequalities</strong></a>. This is a set of inequalities that quantifies how much random variables (or function of them) deviate from their expected values (or, also, functions of them). One inequality of those is <strong>Heoffding’s inequality</strong>:<br>If $x_1,x_2,…,x_m$ are $m$ i.i.d. samples of a random variable $X$ distributed by $P$, and $a\leq x<em>i \leq b$ for every $i$, then for a small positive non-zero value $\epsilon$:<br>$$<br>\mathbb{P}\left[\left|\mathop{\mathbb{E}}</em>{X \sim P}[X] - \frac{1}{m}\sum_{i=0}^{m}x_i\right| &gt; \epsilon\right] \leq 2\exp\left(\frac{-2m\epsilon^2}{(b -a)^2}\right)<br>$$</p>
<p>You probably see why we specifically chose Heoffding’s inequality from among the others. We can naturally apply this inequality to our generalization probability, assuming that our errors are bounded between 0 and 1 (which is a reasonable assumption, as we can get that using a 0/1 loss function or by squashing any other loss between 0 and 1) and get for a <strong>single</strong> hypothesis $h$:<br>$$<br>\mathbb{P}[|R(h) - R_{\text{emp}}(h)| &gt; \epsilon] \leq 2\exp(-2m\epsilon^2)<br>$$</p>
<p>This means that the probability of the difference between the training and the generalization errors exceeding $\epsilon$ exponentially decays as the dataset size goes larger. This should align well with our practical experience that the bigger the dataset gets, the better the results becone.</p>
<p>If you noticed, all our analysis up till now was focusing on a <strong>single</strong> hypothesis $h$. But the learning problem doesn’t know that single hypothesis beforehand, it needs to pick one out of an entire hypothesis space $\mathcal H$, so we need a generalization bound that reflects the chanllenge of choosing the right hypothesis.</p>
<h2 id="Generalization-Bound-1st-Attempt"><a href="#Generalization-Bound-1st-Attempt" class="headerlink" title="Generalization Bound: 1st Attempt"></a>Generalization Bound: 1st Attempt</h2><p>In order for the entire hypothesis space to have a generalization gap bigger than $\epsilon$, at least one of its hypothesis: $h_1$ or $h_2$ or $h<em>3$ or … etc should have.  This can be expressed formally by staing that:<br>$$<br>\mathbb{P}\left[\sup</em>{h \in \mathcal{H}}|R(h) - R<em>\text{emp}(h)| &gt; \epsilon\right] = \mathbb{P}\left[\bigcup</em>{h \in \mathcal{H}} |R(h) - R_\text{emp}(h)| &gt; \epsilon\right]<br>$$</p>
<p>Where $\bigcup$ denotes the union of the events, which  <a href="https://en.wikipedia.org/wiki/Logical_disjunction#Union" target="_blank" rel="external">also corresponds</a> to the logical<strong>OR</strong> operator. Using the <a href="https://en.wikipedia.org/wiki/Boole%27s_inequality" target="_blank" rel="external">union bound inequality</a>, we get:<br>$$<br>\mathbb{P}\left[\sup<em>{h \in \mathcal{H}}|R(h) - R</em>\text{emp}(h)| &gt; \epsilon\right] \leq \sum<em>{h \in \mathcal{H}} \mathbb{P}[|R(h) - R</em>\text{emp}(h)| &gt; \epsilon]<br>$$</p>
<p>We exactly know the bound on the probability under the summation from our analysis using the Heoffding’s inequality, so we end up with:<br>$$<br>\mathbb{P}\left[\sup<em>{h \in \mathcal{H}}|R(h) - R</em>\text{emp}(h)| &gt; \epsilon\right] \leq 2|\mathcal{H}|\exp(-2m\epsilon^2)<br>$$</p>
<p>Where $\mathcal H$ is the size of the hypothesis space. By denoting the right hand side of the above inequality by $\delta$, we can say that with a confidence $1-\delta$:<br>$$<br>|R(h) - R<em>\text{emp}| \leq \epsilon \Rightarrow R(h) \leq R</em>\text{emp}(h) + \epsilon<br>$$</p>
<p>And with some basic algebra, we can express $\epsilon$ in terms of $\delta$ and get:<br>$$<br>R(h) \leq R_\text{emp}(h) + \sqrt{\frac{\ln|\mathcal{H}| + \ln{\frac{2}{\delta}}}{2m}}<br>$$</p>
<p>This is our first generalization bound, it states that the generalization error is bounded by the training error plus a function of the hypothesis space size and the dataset size. We can also see that the bigger the hypothesis space gets, the bigger the generalization error becomes. This explains why the memorization hypothesis form last time, which theoretically has $|\mathcal H|=\infty$, fails miserably as a solution to the learning problem depite having $R<em>{emp}=0$; because for the memorization hypothesis $h</em>{mem}$:<br>$$<br>R(h_\text{mem}) \leq 0 + \infty \leq \infty<br>$$</p>
<p>But wait a second! For a linear hypothesis of the form $h(x)=wx+b$, we also have $|\mathcal H|=\infty$ as there is infinitely many lines that can be draw. So the generalization error of the linear hypothesis space should be unbounded just as the memorization hypothesis! If that’s true, why does perceptrons, logistic regression, support vector machines and essentially any ML model that uses a linear hypothesis work?</p>
<p>Our theoretical result was able to account for some phenomena (the memorization hypothesis, and any finite hypothesis space) but not for others (the linear hypothesis, or other infinite hypothesis spaces that empirically work). This means that there’s still something missing from our theoretical model, and it’s time for us to revise our steps. A good starting point is from the source of the problem itself, which is the infinity in $|\mathcal H|$.</p>
<p>Notice that the term $|\mathcal H|$ resulted from our use of the union bound. The basic idea of the union bound is that it bounds the probability by the worst case possible, which is when all the events under union are independent. <strong>This bound gets more tight as the events under consideration get less dependent.</strong> In our case, for the bound to be tight and reasonable, we need the following to be true:</p>
<blockquote>
<p>For every two hypothesis $h_1,h_2 \in \mathcal H$, the two events $|R(h<em>1) - R</em>\text{emp}(h_1)| &gt; \epsilon$ and $|R(h<em>2) - R</em>\text{emp}(h_2)| &gt; \epsilon$ are likely to be independent. This means that the event that $h_1$ has a generalization gap bigger than $\epsilon$ should be independent of the event that also $h_2$ has a generalization gap bigger than $\epsilon$, no matter how much $h_1$ and $h_2$ are close or related; the events should be coincidental.</p>
</blockquote>
<p>But is that true?</p>
<h2 id="Examining-the-Independence-Assumption"><a href="#Examining-the-Independence-Assumption" class="headerlink" title="Examining the Independence Assumption"></a>Examining the Independence Assumption</h2><p>The formulation of the generalization inequality reveals a main reason why we need to consider all the hypothesis in $\mathcal H$. It has to do with the existence of $\sup_{h \in \mathcal{H}}$. The supremum in the inequality guarantees that there’s a very little chance that the biggest generalization gap possible is greater than $\epsilon$; this is a strong claim and if we omit a single hypothesis out of $\mathcal H$, we might miss that “biggest generalization gap possible” and lose that strength, and that’s something we cannot afford to lose. We need to be able to make that claim to ensure that the learning algorithm would never land on a hypothesis with a bigger generalization gap than $\epsilon$.</p>
<p><img src="/images/hyp_rainbow.png" alt=""></p>
<p>Looking at the above plot of binary classification problem, it’s clear that this rainbow of hypothesis produces the same classification on the data points, so all of them have the same empirical risk. So one might think, as they all have the same $R_{emp}$, why not choose one and omit the others?!</p>
<p>This would be a very good solution if we’re only inerested in the empirical risk, but our inequality takes into its consideration the out-of-sample risk as well, which is expressed as :<br>$$<br>R(h) = \mathop{\mathbb{E}}<em>{(x,y) \sim P}[L(y, h(x))] = \int</em>{\mathcal{Y}}\int_{\mathcal{X}}L(y, h(x))P(x, y)\,\mathrm{d}x \,\mathrm{d}y<br>$$</p>
<p>This is an integration over every possible combination of the whole input and output spaces $\mathcal{X,Y}$. So in order to ensure our supremum claim, we need the hypothesis to cover the whole of $\mathcal{X \times Y}$, hence we need all the possible hypothesis in $\mathcal H$.</p>
<p>Now that we’ve established that we do need to consider every single hypothesis in $\mathcal H$, we can ask ourselves: <strong>are the events of each hypothesis having a big generaliation gap are likely to be independent?</strong></p>
<p>Well, Not even close! Take for example the rainbow of hypotheses in the above plot, it’s very clear that if the red hypothesis has a generalization gap greater than $\epsilon$, then, with 100% certainty, every hypothesis with the same slope in the region above it will also have that. The same argument can be made for many different regions in the $\mathcal{X \times Y}$ space with different degrees of certainty as in the following figure.</p>
<p><img src="/images/regions.png" alt=""></p>
<p>But this is not helpful for our mathematical analysis, as the regions seems to be dependent on the distribution of the sample points and there is no way we can precisely capture these dependencies mathematically, and we cannot make assumptions about them without risking to compromise the supremum claim.</p>
<p>So the union bound and the independence assumption seem like the best approximation we can make,but it highly overestimates the probability and makes the bound very loose, and very pessimistic!</p>
<p>However, what if somehow we can get a very good estimate of the risk $R(h)$ without needing to go over the whole of the $\mathcal{X \times Y}$ space, would there be any hope to get a better bound?</p>
<h2 id="The-Symmetrization-Lemma"><a href="#The-Symmetrization-Lemma" class="headerlink" title="The Symmetrization Lemma"></a>The Symmetrization Lemma</h2><p>Let’s think for a moment about something we do usually in machine learning practice. In order to measure the accuracy of our model, we hold out a part of the training set to evaluate the model on after training, and we consider the model’s accuracy on this left out portion as an estimate for the generalization error. This works because we assume that this <strong>test set is drawn i.i.d. from the same distribution of the training set</strong> (this is why we usually shuffle the whole dataset beforehand to break any correlation between the samples).</p>
<p>It turns out that we can do a similar thing mathematically, but instead of taking out a portion of our dataset $S$, we imagine that we have another dataset $S′$ with also size $m$, we call this the <strong>ghost dataset</strong>. Note that this has no practical implications, we don’t need to have another dataset at training, it’s just a mathematical trick we’re gonna use to git rid of the restrictions of $R(h)$ in the inequality.</p>
<p>We’re not gonna go over the proof here, but using that ghost dataset one can actually prove that:<br>$$<br>\mathbb{P}\left[\sup<em>{h \in \mathcal{H}}\left|R(h) - R</em>\text{emp}(h)\right| &gt; \epsilon\right] \leq 2\mathbb{P}\left[\sup<em>{h \in \mathcal{H}}\left|R</em>\text{emp}(h) - R_\text{emp}’(h)\right| &gt; \frac{\epsilon}{2}\right] \hspace{2em} (1)<br>$$</p>
<p>where $R′_{emp}(h)$ is the empirical risk of hypothesis $h$ on the ghost dataset. This means that the probability of the largest generalization gap being bigger than $\epsilon$ is at most twice the probability that the empirical risk difference between $S,S′$ is larger than $\frac{\epsilon}{2}$. Now that the right hand side in expressed only in terms of empirical risks, we can bound it without needing to consider the the whole of $\mathcal{X \times Y}$, and hence we can bound the term with the risk $R(h)$ without considering the whole of input and output spaces!</p>
<p>This, which is called the <strong>symmetrization lemma</strong>, was one of the two key parts in the work of Vapnik-Chervonenkis (1971).</p>
<h2 id="The-Growth-Function"><a href="#The-Growth-Function" class="headerlink" title="The Growth Function"></a>The Growth Function</h2><p>Now that we are bounding only the empirical risk, if we have many hypotheses that have the same empirical risk (a.k.a. producing the same labels/values on the data points), we can safely choose one of them as a representative of the whole group, we’ll call that an <strong>effective</strong> hypothesis, and discard all the others.</p>
<p>By only choosing the distinct effective hypotheses on the dataset $S$, we restrict the hypothesis space $H$ to a smaller subspace that depends on the dataset $\mathcal{H}_{|S}$.</p>
<p>We can assume the independence of the hypotheses in $\mathcal{H}<em>{|S}$ like we did before with $H$ (but it’s more plausible now), and use the union bound to get that:<br>$$<br>\mathbb{P}\left[\sup</em>{h \in \mathcal{H}<em>{|S\cup S’}}\left|R</em>\text{emp}(h) - R<em>\text{emp}’(h)\right|&gt;\frac{\epsilon}{2}\right] \leq \left|\mathcal{H}</em>{|S\cup S’}\right| \mathbb{P}\left[\left|R<em>\text{emp}(h) - R</em>\text{emp}’(h)\right|&gt;\frac{\epsilon}{2}\right]<br>$$<br>Notice that the hypothesis space is restricted by $S \cup S’$ because we using the empirical risk on both the original dataset $S$ and the ghost $S’$. The question now is what is the maximum size of a restricted hypothesis space? The answer is very simple; we consider a hypothesis to be a new effective one if it produces new labels/values on the dataset samples, then the maximum number of distinct hypothesis (a.k.a the maximum number of the restricted space) is the maximum number of distinct labels/values the dataset points can take. A cool feature about that maximum size is that its a combinatorial measure, so we don’t need to worry about how the samples are distributed!</p>
<p>For simplicity, we’ll focus now on the case of binary classification, in which $\mathcal Y=\{−1,+1\}$. Later we’ll show that the same concepts can be extended to both multiclass classification and regression. In that case, for a dataset with $m$ samples, each of which can take one of two labels: either -1 or +1, the maximum number of distinct labellings is $2^m$.</p>
<p>We’ll define the maximum number of distinct labellings/values on a dataset $S$ of size $m$ by a hypothesis space $\mathcal H$ as the <strong>growth function</strong> of $H$ given $m$, and we’ll denote that by $\Delta<em>\mathcal{H}(m)$. It’s called the growth function because it’s value for a single hypothesis space $H$ (aka the size of the restricted subspace $\mathcal{H</em>{|S}}$) grows as the size of the dataset grows. Now we can say that:<br>$$<br>\mathbb{P}\left[\sup<em>{h \in \mathcal{H}</em>{|S\cup S’}}\left|R<em>\text{emp}(h) - R</em>\text{emp}’(h)\right|&gt;\frac{\epsilon}{2}\right] \leq \Delta<em>\mathcal{H}(2m) \mathbb{P}\left[\left|R</em>\text{emp}(h) - R_\text{emp}’(h)\right|&gt;\frac{\epsilon}{2}\right] \hspace{2em} (2)<br>$$<br>Notice that we used $2m$ because we have two datasets $S,S’$ each with size $m$.</p>
<p>For the binary classification case, we can say that:<br>$$<br>\Delta_\mathcal{H}(m) \leq 2^m<br>$$<br>But $2^m$ is exponential in $m$ and would grow too fast for large datasets, which makes the odds in our inequality go too bad too fast! Is that the best bound we can get on that growth function?</p>
<h2 id="The-VC-Dimension"><a href="#The-VC-Dimension" class="headerlink" title="The VC-Dimension"></a>The VC-Dimension</h2><p>The $2^m$ bound is based on the fact that the hypothesis space $\mathcal H$ can produce all the possible labellings on the $m$ data points. If a hypothesis space can indeed produce all the possible labels on a set of data points, we say that the hypothesis space <strong>shatters</strong> that set.</p>
<p>But can any hypothesis space shatter any dataset of any size? Let’s investigate that with the binary classification case and the $\mathcal H$ of linear classifiers $\mathrm{sign}(wx + b)$. The following animation shows how many ways a linear classifier in 2D can label 3 points (on the left) and 4 points (on the right).</p>
<p><img src="/images/shatter.gif" alt=""></p>
<p>In the animation, the whole space of possible effective hypotheses is swept. For the the three points, the hypothesis shattered the set of points and produced all the possible $2^3=8$ labellings. However for the four points,the hypothesis couldn’t get more than 14 and never reached $2^4=16$, so it failed to shatter this set of points. Actually, no linear classifier in 2D can shatter any set of 4 points, not just that set; because there will always be two labellings that cannot be produced by a linear classifier which is depicted in the following figure.</p>
<p><img src="/images/impossible-dichotomy.png" alt=""></p>
<p>From the decision boundary plot (on the right), it’s clear why no linear classifier can produce such labellings; as no linear classifier can divide the space in this way. So it’s possible for a hypothesis space $\mathcal H$ to be unable to shatter all sizes. This fact can be used to get a better bound on the growth function, and this is done using <strong>Sauer’s lemma</strong>:</p>
<blockquote>
<p>If a hypothesis space $\mathcal H$ cannot shatter any dataset with size more than $k$, then:<br>$$<br>\Delta<em>{\mathcal{H}}(m) \leq \sum</em>{i=0}^{k}\binom{m}{i}<br>$$</p>
</blockquote>
<p>This was the other key part of Vapnik-Chervonenkis work (1971), but it’s named after another mathematician, Norbert Sauer; because it was independently proved by him around the same time (1972). However, Vapnik and Chervonenkis weren’t completely left out from this contribution; as that $k$, which is the maximum number of points that can be shattered by $\mathcal H$, is now called the <em>Vapnik-Chervonenkis-dimension</em> or the <strong>VC-dimension</strong> $d_{vc}$ of $\mathcal H$.</p>
<p>For the case of the linear classifier in 2D, $d<em>{vc}=3$. In general, it can be proved that hyperplane classifiers (the higher-dimensional generalization of line classifiers) in $\mathbb{R}^n$ space has $d</em>{vc}=n+1$.</p>
<p>The bound on the growth function provided by sauer’s lemma is indeed much better than the exponential one we already have, it’s actually polynomial! Using algebraic manipulation, we can prove that:<br>$$<br>\Delta<em>\mathcal{H}(m) \leq \sum</em>{i=0}^{k}\binom{m}{i} \leq \left(\frac{me}{d<em>\mathrm{vc}}\right)^{d</em>\mathrm{vc}}\leq O(m^{d_\mathrm{vc}})<br>$$<br>Where $O$ refers to the <a href="https://en.wikipedia.org/wiki/Big_O_notation" target="_blank" rel="external">Big-O notation</a> for functions asymptotic (near the limits) behavior, and $e$ is the mathematical constant.</p>
<p>Thus we can use the VC-dimension as a proxy for growth function and, hence, for the size of the restricted space $\mathcal{H<em>{|S}}$. In that case, $d</em>{vc}$ would be a measure of the complexity or richness of the hypothesis space.</p>
<h2 id="The-VC-Generalization-Bound"><a href="#The-VC-Generalization-Bound" class="headerlink" title="The VC Generalization Bound"></a>The VC Generalization Bound</h2><p>With a little change in the constants, it can be shown that Heoffding’s inequality is applicable on the probability $\mathbb{P}\left[|R<em>\mathrm{emp}(h) - R</em>\mathrm{emp}’(h)| &gt; \frac{\epsilon}{2}\right]$. With that,  With that, and by combining inequalities (1) and (2), the <strong>Vapnik-Chervonenkis theory</strong> follows:<br>$$<br>\mathbb{P}\left[\sup<em>{h \in \mathcal{H}}|R(h) - R</em>\mathrm{emp}(h)| &gt; \epsilon\right] \leq 4\Delta<em>\mathcal{H}(2m)\exp\left(-\frac{m\epsilon^2}{8}\right)<br>$$<br>This can be re-expressed as a bound on the generalization error, just as we did earlier with the previous bound, to get the <strong>VC generalization bound</strong>:<br>$$<br>R(h) \leq R</em>\mathrm{emp}(h) + \sqrt{\frac{8\ln\Delta<em>\mathcal{H}(2m) + 8\ln\frac{4}{\delta}}{m}}<br>$$<br>or, by using the bound on growth function in terms of $d</em>{vc}$ as:<br>$$<br>R(h) \leq R<em>\mathrm{emp}(h) + \sqrt{\frac{8d</em>\mathrm{vc}(\ln\frac{2m}{d<em>\mathrm{vc}} + 1) + 8\ln\frac{4}{\delta}}{m}}<br>$$<br>This is a significant result! It’s a clear and concise mathematical statement that the learning problem is solvable, and that for infinite hypotheses spaces there is a finite bound on the their generalization error! Furthermore, this bound can be described in term of a quantity ($d</em>{vc}$), that solely depends on the hypothesis space and not on the distribution of the data points!</p>
<p>Now, in light of these results, is there’s any hope for the memorization hypothesis?</p>
<p>It turns out that there’s still no hope! The memorization hypothesis can shatter any dataset no matter how big it is, that means that its $d<em>{vc}$ is infinite, yielding an infinite bound on $R(h</em>{mem})$ as before. However, the success of linear hypothesis can now be explained by the fact that they have a finite $d_{vc}=n+1$ in $\mathbb{R}^n$. The theory is now consistent with the empirical observations.</p>
<h2 id="Distribution-Based-Bounds"><a href="#Distribution-Based-Bounds" class="headerlink" title="Distribution-Based Bounds"></a>Distribution-Based Bounds</h2><p>The fact that $d<em>{vc}$ is distribution-free comes with a price: by not exploiting the structure and the distribution of the data samples, the bound tends to get loose. Consider for example the case of linear binary classifiers in a very higher n-dimensional feature space, using the distribution-free $d</em>{vc}=n+1$ means that the bound on the generalization error would be poor unless the size of the dataset $N$ is also very large to balance the effect of the large $d_{vc}$. This is the good old <em>curse of dimensionality</em> we all know and endure.</p>
<p>However, a careful investigation into the distribution of the data samples can bring more hope to the situation. For example, For data points that are linearly separable, contained in a ball of radius $R$, with a margin $\rho$ between the closest points in the two classes, one can prove that for a hyperplane classifier:<br>$$<br>d<em>\mathrm{vc} \leq \left\lceil \frac{R^2}{\rho^2} \right\rceil<br>$$<br>It follows that the larger the margin, the lower the $d</em>{vc}$ of the hypothesis. This is theoretical motivation behind <strong>Support Vector Machines (SVMs)</strong> which attempts to classify data using the maximum margin hyperplane. This was also proved by Vapnik and Chervonenkis.</p>
<h2 id="One-Inequality-to-Rule-Them-All"><a href="#One-Inequality-to-Rule-Them-All" class="headerlink" title="One Inequality to Rule Them All"></a>One Inequality to Rule Them All</h2><p>Up until this point, all our analysis was for the case of binary classification. And it’s indeed true that the form of the vc bound we arrived at here only works for the binary classification case. However, the conceptual framework of VC (that is: shattering, growth function and dimension) generalizes very well to both multi-class classification and regression.</p>
<p>Due to the work of Natarajan (1989), the <strong>Natarajan dimension</strong> is defined as a generalization of the VC-dimension for multiple classes classification, and a bound similar to the VC-Bound is derived in terms of it. Also, through the work of Pollard (1984), the <strong>pseudo-dimension</strong> generalizes the VC-dimension for the regression case with a bound on the generalization error also similar to VC’s.</p>
<p>There is also <em>Rademacher’s complexity</em>, which is a relatively new tool (devised in the 2000s) that measures the richness of a hypothesis space by measuring how well it can fit to random noise. The cool thing about Rademacher’s complexity is that it’s flexible enough to be adapted to any learning problem, and it yields very similar generalization bounds to the other methods mentioned.</p>
<p>However, no matter what the exact form of the bound produced by any of these methods is, it always takes the form:<br>$$<br>R(h) \leq R_\mathrm{emp}(h) + C(|\mathcal{H}|, N, \delta)<br>$$<br>where $C$ is a function of the hypothesis space complexity (or size, or richness), the size of the dataset, and the confidence $1-\delta$ about the bound. <strong>This inequality basically says the generalization error can be decomposed into two parts: the empirical training error, and the complexity of the learning model.</strong></p>
<p>This form of the inequality holds to any learning problem no matter the exact form of the bound, and this is the one we’re gonna use throughout the rest of the series to guide us through the process of machine learning.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>