<!DOCTYPE html><html><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Off-policy Prediction via Importance Sampling | An old brother's memo.</title><link rel="stylesheet" type="text/css" href="/css/normalize.css"><link rel="stylesheet" type="text/css" href="/css/highlight.css"><link rel="stylesheet" type="text/css" href="/css/very-simple.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><script src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script></head><body><!-- include the sidebar--><!-- include ./includes/sidebar.jade--><!-- Blog title and subtitle--><header><div class="container header"><a id="logo" href="/." class="title">An old brother's memo.</a><span class="subtitle"></span><label id="toggle-menu" for="menu" onclick><i class="fa fa-bars"></i></label></div></header><!-- use checkbox hack for toggle nav-bar on small screens--><input id="menu" type="checkbox"><!-- Navigation Links--><nav id="nav"><div class="container"><a href="/" class="sidebar-nav-item active">Home</a><a href="/archives" class="sidebar-nav-item">Archives</a></div></nav><div id="header-margin-bar"></div><!-- gallery that comes before the header--><div class="wrapper"><div class="container post-header"><h1>Off-policy Prediction via Importance Sampling</h1></div></div><div class="wrapper"><div class="container meta"><div class="post-time">2016-11-14</div><div class="post-categories"><a class="post-category-link" href="/categories/machine-learning/">machine learning</a></div><div class="post-tags"><a class="post-tag-link" href="/tags/reinforcement-learning/">reinforcement learning</a></div></div></div><article><div class="container post"><blockquote>
<p>p111 (129)</p>
</blockquote>
<p>All learning control methods face a dilemma: They seek to learn action values conditional on subsequent <em>optimal</em> behavior, but they need to behave non-optimally in order to explore all actions (to <em>find</em> the optimal actions). How can they learn about the optimal policy while behaving according to an exploratory policy? The on-policy approach is actually a compromise - it learns action values not for the optimal policy, but for a near-optimal policy that still explores. A more straightforward approach is to use two policies, one that is learned about and that becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy being learned about is called the <em>target policy</em>, and the policy used to generate behavior is called the <em>behavior policy</em>. In this case we say that learning is from data “off” the target policy, and the overall process is termed <em>off-policy learning</em>.</p>
<p>On-policy methods are generally simpler. Off-policy methods require additional concepts and notation, and because the data is due to a different policy, off-policy methods are often of <strong>greater variance and are slower to converge</strong>. On the other hand, off-policy methods are more powerful and general. They include on-policy methods as the special case in which the target and behavior policies are the same. Off-policy methods also have a variety of additional uses in applications. For example, they can often be applied to learn from data generated by a conventional non-learning controller, or from a human expert. Off-policy learning is also seen by some as key to learning multi-step predictive models of the world’s dynamics (Sutton, 2009, Sutton et al., 2011).</p>
<p>In order to use episodes from $\mu$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $\mu$. That is, we require that $\pi(a|s)\gt 0$ implies $\mu(a|s)\gt 0$. This is called the assumption of <em>coverage</em>. It follows from coverage that $\mu$ must be stochastic in states where it is not identical to $\pi$. The target policy $\pi$, on the other hand, may be deterministic, and, in fact, this is a case of particular interest in control problems. In control, the target policy is typically the deterministic greedy policy with respect to the current action-value function estimate. This policy becomes a deterministic optimal policy while the behavior policy remains stochastic and more exploratory, for example, an $\epsilon$-greedy policy.</p>
<p>Almost all off-policy methods utilize <em>importance sampling</em>, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the <em>importance-sampling ratio</em>. Given a starting state $S_t$, the probability of the subsequent state-action trajectory, $A<em>t,S</em>{t+1},A_{t+1},…,S<em>T$, occurring under any policy $\pi$ is<br>$$<br>\prod^{T-1}</em>{k=t} \pi(A_k|S<em>k)p(S</em>{k+1}|S_k,A_k)<br>$$<br>where $p$ here is the state-transition probability function. Thus, the relative probability of the trajectory under the target and behavior policies (the <strong>importance-sampling ratio</strong>) is<br>$$<br>\rho<em>t^T \doteq \frac{\prod</em>{k=t}^{T-1}\pi(A_k|S<em>k)p(S</em>{k+1}|S_k,A<em>k)}{\prod</em>{k=t}^{T-1}\mu(A_k|S<em>k)p(S</em>{k+1}|S_k,A<em>k)}<br>=\prod</em>{k=t}^{T-1}\frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}<br>$$<br>Note that although the trajectory probabilities depend on the MDP’s transition probabilities, which are generally unknown, all the transition probabilities cancel. The importance sampling ratio ends up depending only on the two policies and not at all on the MDP.</p>
<p><strong>ordinary importance sampling</strong>: Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy $\mu$ to estimate $v_\pi(\mathcal{s})$. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time $t=101$. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state $\mathcal{s}$ is visited, denoted $\mathfrak{T}(\mathcal{s})$. This is for an every-visit method; for a first-visit method, $\mathfrak{T}(\mathcal{s})$ would only include time steps that were first visits to $\mathcal{s}$ within their episodes. Also, let $T(t)$ denote the first time of termination following time $t$, and $G_t$ denote the return after $t$ up through $T(t)$. Then ${G<em>t}</em>{t\in{\mathcal{T}(\mathcal{s})}}$ are the returns that pertain to state $\mathcal{s}$, and ${\rho<em>t^{T(t)}}</em>{t\in{\mathcal{T}(\mathcal{s})}}$ are the corresponding importance-sampling ratios. To estimate $v<em>\pi(\mathcal{s})$, we simply scale the returns by the ratios and average the results:<br>$$<br>V(\mathcal{s}) \doteq \frac{\sum</em>{t\in{\mathfrak{T}(\mathcal{s})}}\rho_t^{T(t)}G_t}{|\mathfrak{T}(\mathcal{s})|}<br>$$<br>When importance sampling is done as a simple average in this way it is called <em>ordinary importance sampling</em>.</p>
<p><strong>weighted importance sampling</strong>:<br>$$<br>V(\mathcal{s}) \doteq \frac{\sum_{t\in\mathfrak{T}(\mathcal{s})} \rho_t^{T(t)}G<em>t}{\sum</em>{t\in\mathfrak{T}(\mathcal{s})}\rho_t^{T(t)}}<br>$$<br>or zero if the denominator is zero. </p>
<p>To understand these two varieties of importance sampling, consider their estimates after observing a single return. In the <strong>weighted average estimate</strong>, <strong>the ratio $\rho_t^{T(t)}$ for the single return cancels in the numerator and denominator, so that the estimate is equal to the observed return independent of the ratio (assuming the ratio is nonzero).</strong> Given that this return was the only one observed, this is a reasonable estimate, but of course its expectation is $v<em>\mu(s)$ rather than $v</em>\pi(s)$, and in this statistical sense it is biased. In contrast, the <strong>ordinary importance sampling</strong> is always $v_\pi(s)$ is expectation (it is unbiased), but it can be extreme. Suppose the ratio were ten, indicating that the trajectory observed is ten times as likely under the target policy as under the behavior policy. In this case the ordinary importance sampling estimate would be <em>ten times</em> the observed return. That is, it would  be quite far from the observed return even though the episode’s trajectory is considered very representative of the target policy.</p>
<p>Formally, the difference between the two kinds of importance sampling is expressed in their <strong>biases and variances</strong>. The ordinary importance-sampling estimator is unbiased whereas the weighted importance-sampling estimator is biased (the bias converges asymptotically to zero). On the other hand, the variance of the ordinary importance-sampling estimator is in general unbounded because the variance of the ratios can be unbounded, whereas in the weighted estimator the largest weight on any single return is one. In fact, assuming bounded returns, the variance of the weighted importance-sampling estimator converges to zero even if the variance of the ratios themselves is infinite (Precup, Sutton, and Dasgupta 2001). In practice, the weighted estimator usually has dramatically lower variance and is strongly preferred. Nevertheless, we will not totally abandon ordinary importance sampling as it is easier to extend to the approximate methods using function approximation that we explore in the second part of this book.</p>
</div><!-- comment system--><div class="container"><hr></div></article><footer id="footer"><div class="container"><div class="bar"><div class="social"><a href="mailto:dad@sdz.red" target="_blank"><i class="fa fa-envelope-o"></i></a><a href="https://github.com/sohero" target="_blank"><i class="fa fa-github"></i></a><script>var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cspan id='cnzz_stat_icon_1261298720'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s95.cnzz.com/z_stat.php%3Fid%3D1261298720%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
</script></div><div class="footer">© 2017 <a href="/" rel="nofollow">Hero Memo</a>. Powered by <a rel="nofollow" target="_blank" href="https://hexo.io">Hexo</a>. Theme <a target="_blank" href="https://github.com/lotabout/very-simple">very-simple</a>.</div></div></div></footer><script>MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.css"><script src="//cdn.bootcss.com/jquery/2.0.3/jquery.min.js"></script><script src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js"></script><script>$(document).ready(function() {
    $(".fancybox").fancybox();
});
</script></body></html>