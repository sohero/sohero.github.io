<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Improving Word Representations via Global Context and Multiple Word Prototypes</title>
	
	<meta name="author" content="sohero">

	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

	<!-- Le styles -->
    <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css">
	<link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/css/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">

	<!-- Le fav and touch icons -->
	<link rel="shortcut icon" href="favicon.ico">
    <!-- Update these with your own images
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
	-->

	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <script type="text/javascript" src="http://static.blog.csdn.net/public/res/bower-libs/MathJax/MathJax.js?config=TeX-AMS_HTML"></script>
    
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/sohero">
				<i class="fa fa-github"></i>
			</a>
			
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
            <input type="text" class="st-default-search-input navbar-toggle" style="width:100px;">
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-3 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/assets/images/bl.png" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/"></a>
    </h3>
</header>


<div id="bio" class="text-center">
	An elder's memo.
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/sohero">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
        <li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
</div>
<div style="width:171px;margin-left:auto;margin-right:auto;">
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','ajvL7F6K122b67uo9oa2','2.0.0');
</script>
<input type="text" class="st-default-search-input" style="width:130px;">
</div>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-9 col-sm-offset-3">
		<div class="page-header">
  <h1>Improving Word Representations via Global Context and Multiple Word Prototypes </h1>
</div>
	
<article>

	<div class="col-sm-10">
	 <span class="post-date">
	 	2015-12-23 
	 </span>
	  <div class="article_body">
	  <h2 id="global-context-aware-neural-language-model">Global Context-Aware Neural Language Model</h2>
<h3 id="training-objective">Training Objective</h3>
<p>Given a word sequence <span class="math inline">\(s\)</span> and document <span class="math inline">\(d\)</span> in which the sequence occurs, our goal is to discriminate the correct last word in <span class="math inline">\(s\)</span> from other random words. We compute scores <span class="math inline">\(g(s,d)\)</span> and <span class="math inline">\(g(s^w,d)\)</span> where <span class="math inline">\(s^w\)</span> is <span class="math inline">\(s\)</span> with the last word replaced by word <span class="math inline">\(w\)</span>, and <span class="math inline">\(g(·,·)\)</span> is the scoring function that represents the neural networks used. We want <span class="math inline">\(g(s,d)\)</span> to be larger than <span class="math inline">\(g(s^w,d)\)</span> by a margin of 1, for any other word <span class="math inline">\(w\)</span> in the vocabulary, which corresponds to the training objective of minimizing the ranking loss for each <span class="math inline">\((s,d)\)</span> found in the corpus: <span class="math display">\[
C_{s,d}=\sum_{w\in V} max(0, 1-g(s,d)+g(s^w,d))
\]</span></p>
<h3 id="neural-network-architecture">Neural Network Architecture</h3>
<figure>
<img src="/assets/images/27.png" alt="An overview of the neural language model. The model makes use of both local and global context to compute a score that should be large for the actual next word (bank in the example), compare to the score for other words. When word meaning is still ambiguous given local context, information in global context can help disambiguation." /><figcaption>An overview of the neural language model. The model makes use of both local and global context to compute a score that should be large for the actual next word (bank in the example), compare to the score for other words. When word meaning is still ambiguous given local context, information in global context can help disambiguation.</figcaption>
</figure>
<p>The score of local context uses the local word sequence <span class="math inline">\(s\)</span>. We first represent the word sequence <span class="math inline">\(s\)</span> as an ordered list of vectors <span class="math inline">\(x=(x_1,x_2,...,x_m)\)</span> where <span class="math inline">\(x_i\)</span> is the embedding of word <span class="math inline">\(i\)</span> in the sequence, which is a column in the embedding matrix <span class="math inline">\(L\in \Bbb R^{n\times |V|}\)</span> where <span class="math inline">\(|V|\)</span> denotes the size of the vocabulary. <strong>The columns of this embedding matrix <span class="math inline">\(L\)</span> are the word vectors and will be learned and updated during training.</strong></p>
<p>To compute the score of local context, <span class="math inline">\(socre_l\)</span>, use a neural network with one hidden layer: <span class="math display">\[
a_1=f(W_1[x_1;x_2;...;x_m]+b_1)
\]</span> <span class="math display">\[
score_l=W_2a_1+b_2
\]</span> where <span class="math inline">\([x_1;x_2;...;x_m]\)</span> is the concatenation of the <span class="math inline">\(m\)</span> word embeddings representing sequence <span class="math inline">\(s\)</span>, <span class="math inline">\(f\)</span> is an element-wise activation function such as <span class="math inline">\(tanh\)</span>, <span class="math inline">\(a_1\in \Bbb R^{h\times 1}\)</span> is the activation of the hidden layer with <span class="math inline">\(h\)</span> hidden nodes, <span class="math inline">\(W_1\in \Bbb R^{h\times \\(mn\\)}\)</span> and <span class="math inline">\(W_2\in\Bbb R^{1\times h}\)</span> are respectively the first and second layer weights of the neural network, and <span class="math inline">\(b_1\)</span>, <span class="math inline">\(b_2\)</span> are the biases of each layer.</p>
<p>For the score of the global context, we represent the document also as an ordered list of word embedding, <span class="math inline">\(d=(d_1,d_2,...,d_k)\)</span>. First compute the weighted average of all word vectors in the document: <span class="math display">\[
c=\frac{\sum_{i=1}^k w(t_i)d_i}{\sum_{i=1}^k w(t_i)}
\]</span> where <span class="math inline">\(w(·)\)</span> can be any weighting function that captures the importance of word <span class="math inline">\(t_i\)</span> in the document. We use idf-weighting as the weighting function. Use a two-layer neural network to compute the global context score, <span class="math inline">\(score_g\)</span>, similar to the above: <span class="math display">\[
a_1^g=f(W_1^g[c;x_m]+b_1^g)
\]</span> <span class="math display">\[
score_g=W_2^ga_1^g+b_2^g
\]</span> where <span class="math inline">\([c;x_m]\)</span> is the concatenation of the weighted average document vector and the vector of the last word in <span class="math inline">\(s\)</span>.</p>
<p><strong>Note that instead of using the document where the sequence occurs, we can also specify a fixed <span class="math inline">\(k&gt;m\)</span> that captures larger context.</strong></p>
<p>The final score is the sum of the two scores: <span class="math display">\[
score = score_l+score_g
\]</span> The local score preserves word order and syntactic information, while the global score uses a weighted average which is similar to bag-of-words features, capturing more of the semantics and topics of the document.</p>
<h3 id="learning">Learning</h3>
<p>Word embeddings move to good positions in the vector space faster when using <code>mini-batch L-BFGS</code> (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt examples per batch for training, compared to stochastic gradient descent.</p>
<h2 id="multi-prototype-neural-language-model">Multi-Prototype Neural Language Model</h2>
<p>In order to learn multiple prototypes, we first gather the fixed-sized context windows of all occurrences of a word (we use 5 words before and after the word occurrence). Each context is represented by a weighted average of the context words’ vectors, where again, we use <code>idf-weighting</code> as the weighting function. Then use <code>spherical k-means</code> to cluster these context representations. Finally, each word occurrence in the corpus is re-labeled to its associated cluster and is used to train the word representation for that cluster.</p>
<p>Similarity between a pair of words <span class="math inline">\((w, w&#39;)\)</span> using the multi-prototype approach can be computed with of without context, as defined by Reisinger and Mooney (2010b): <span class="math display">\[
AvgSimC(w,w&#39;)={1 \over K^2}\sum_{i=1}^k \sum_{j=1}^k p(c,w,i)p(c&#39;,w&#39;,j)d(\mu_i(w), \mu_j(w&#39;))
\]</span> where <span class="math inline">\(p(c,w,i)\)</span> is the likelihood that word <span class="math inline">\(w\)</span> is in its cluser <span class="math inline">\(i\)</span> given context <span class="math inline">\(c\)</span>, <span class="math inline">\(\mu_i(w)\)</span> is the vector representing the <span class="math inline">\(i\)</span>-th cluster centroid of <span class="math inline">\(w\)</span>, and <span class="math inline">\(d(v,v&#39;)\)</span> is a function computing similarity between two vectors, which can be any of the distance functions presented by Curran(2004). The similarity measure can be computed in absence of context by assuming uniform <span class="math inline">\(p(c,w,i)\)</span> over <span class="math inline">\(i\)</span>.</p>

	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="/categories.html#foundation-ref">
					foundation <span>(28)</span>
					,
				</a></li>
			 
				<li><a href="/categories.html#paper-ref">
					paper <span>(6)</span>
					
				</a></li>
			
		  
		</ul>
		  

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#foundation-ref">
					foundation <span>(28)</span>
					,
					</a>
				</li>
			 
				<li>
					<a href="/tags.html#paper-ref">
					paper <span>(6)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-12">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=Improving Word Representations via Global Context and Multiple Word Prototypes"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/foundation/paper/2015/12/21/neural-reasoner.html" title="Neural Reasoner">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="/foundation/nlp/2015/12/23/sentiment-analysis.html" title="Sentiment analysis">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>
<script>
(function(){
    var bp = document.createElement('script');
    bp.src = '//push.zhanzhang.baidu.com/push.js';
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>




		<footer>
			<hr/>
			<p>
				&copy; 2016 <a href="http://guoqiang.gq">sohero</a>.
			</p>
		</footer>
	</div>

	<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
	<script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



