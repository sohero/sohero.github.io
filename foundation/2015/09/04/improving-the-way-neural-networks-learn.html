<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<title>Improving the way neural networks learn</title>
	
	<meta name="author" content="sohero">

	<!-- Enable responsive viewport -->
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
	<!--[if lt IE 9]>
	<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

	<!-- Le styles -->
    <link rel="stylesheet" href="//cdn.bootcss.com/bootstrap/3.3.5/css/bootstrap.min.css">
	<link href="//cdn.bootcss.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet">
	<link href="/assets/css/syntax.css" rel="stylesheet">
	<link href="/assets/css/style.css" rel="stylesheet">

	<!-- Le fav and touch icons -->
	<link rel="shortcut icon" href="favicon.ico">
    <!-- Update these with your own images
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
	-->

	<link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
    <script type="text/javascript" src="http://static.blog.csdn.net/public/res/bower-libs/MathJax/MathJax.js?config=TeX-AMS_HTML"></script>
    
</head>

<body>
	<nav class="navbar navbar-default visible-xs" role="navigation">
		<!-- Brand and toggle get grouped for better mobile display -->
		<div class="navbar-header">
			<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
			<a type="button" class="navbar-toggle nav-link" href="http://github.com/sohero">
				<i class="fa fa-github"></i>
			</a>
			
			
			
			<a type="button" class="navbar-toggle nav-link" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope"></i>
			</a>
			
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				<li class="active"><a href="/">Home</a></li>
				<li><a href="/categories.html">Categories</a></li>
				<li><a href="/tags.html">Tags</a></li>
			</ul>
		</div><!-- /.navbar-collapse -->
	</nav>

	<!-- nav-menu-dropdown -->
	<div class="btn-group hidden-xs" id="nav-menu">
		<button type="button" class="btn btn-default dropdown-toggle" data-toggle="dropdown">
			<i class="fa fa-bars"></i>
		</button>
		<ul class="dropdown-menu" role="menu">
			<li><a href="/"><i class="fa fa-home"></i>Home</a></li>
			<li><a href="/categories.html"><i class="fa fa-folder"></i>Categories</a></li>
			<li><a href="/tags.html"><i class="fa fa-tags"></i>Tags</a></li>
			<li class="divider"></li>
			<li><a href="#"><i class="fa fa-arrow-up"></i>Top of Page</a></li>
		</ul>
	</div>

	<div class="col-sm-3 sidebar hidden-xs">
		<!-- sidebar.html -->
<header class="sidebar-header" role="banner">
	<a href="/">
		<img src="/assets/images/bl.png" class="img-circle" />
	</a>
	<h3 class="title">
        <a href="/"></a>
    </h3>
</header>


<div id="bio" class="text-center">
	An elder's memo.
</div>


<div id="contact-list" class="text-center">
	<ul class="list-unstyled list-inline">
		
		<li>
			<a class="btn btn-default btn-sm" href="https://github.com/sohero">
				<i class="fa fa-github-alt fa-lg"></i>
			</a>
		</li>
		
		
		<li>
			<a class="btn btn-default btn-sm" href="mailto:herosgq@gmail.com">
				<i class="fa fa-envelope fa-lg"></i>
			</a>
		</li>
		
        <li>
			<a class="btn btn-default btn-sm" href="/feed.xml">
				<i class="fa fa-rss fa-lg"></i>
			</a>
		</li>
	</ul>
	<ul id="contact-list-secondary" class="list-unstyled list-inline">
		
		
        
		
	</ul>
</div>
<script type="text/javascript">(function(){document.write(unescape('%3Cdiv id="bdcs"%3E%3C/div%3E'));var bdcs = document.createElement('script');bdcs.type = 'text/javascript';bdcs.async = true;bdcs.src = 'http://znsv.baidu.com/customer_search/api/js?sid=13267021605797832700' + '&plate_url=' + encodeURIComponent(window.location.href) + '&t=' + Math.ceil(new Date()/3600000);var s = document.getElementsByTagName('script')[0];s.parentNode.insertBefore(bdcs, s);})();</script>
<!-- sidebar.html end -->

	</div>

	<div class="col-sm-9 col-sm-offset-3">
		<div class="page-header">
  <h1>Improving the way neural networks learn </h1>
</div>
	
<article>

	<div class="col-sm-10">
	 <span class="post-date">
	 	2015-09-04 
	 </span>
	  <div class="article_body">
	  <p><a href="http://neuralnetworksanddeeplearning.com/chap3.html">LINK</a></p>
<h2 id="why-sigmoid-quadratic-cost-function-learning-slow">Why sigmoid + quadratic cost function learning slow?</h2>
<p>The quadratic cost function is given by <span class="math display">\[
C=\frac {(y-a)^2}{2} \tag 1
\]</span> where <span class="math inline">\(a\)</span> is the neuron’s output. <span class="math inline">\(a=\sigma (z)\)</span>, where <span class="math inline">\(z=wx+b\)</span>. Using the chain rule to differentiate with respect to the weight and bias we get <span class="math display">\[
  \frac{\partial C}{\partial w}  =  (a-y)\sigma&#39;(z) x = a \sigma&#39;(z) \tag{2}\]</span> <span class="math display">\[
  \frac{\partial C}{\partial b}  =  (a-y)\sigma&#39;(z) = a \sigma&#39;(z)
\tag{3}\]</span> where I have substituted <span class="math inline">\(x=1\)</span> and <span class="math inline">\(y=0\)</span>. Recall the shape of the <span class="math inline">\(\sigma\)</span> function: <img src="/assets/images/8.jpg" alt="sigmoid function" /></p>
<p>We can see from this graph that when the neuron’s output is close to 1, the curve gets very flat, and so <span class="math inline">\(\sigma &#39;(z)\)</span> gets very small. Equations (2) and (3) then tell us that <span class="math inline">\(\partial C/\partial w\)</span> and <span class="math inline">\(\partial C/\partial b\)</span> get very small.</p>
<blockquote>
<strong>Using the quadratic cost when we have linear neurons in the output layer</strong>. Suppose that we have a many-layer multi-neuron network. Suppose all the neurons in the final layer are linear neurons, meaning that the sigmoid activation function is not applied, and the outputs are simply <span class="math inline">\(a^L_j=z^L_j\)</span>. Show that if we use the quadratic cost function then the output error <span class="math inline">\(δ^L\)</span> for a single training example <span class="math inline">\(x\)</span> is given by <span class="math display">\[δ^L=a^L−y\]</span> Similarly to the previous problem, use this expression to show that the partial derivatives with respect to the weights and biases in the output layer are given by
\begin{eqnarray}
      \frac{\partial C}{\partial w^L_{jk}} &amp; = &amp; \frac{1}{n} \sum_x 
      a^{L-1}_k  (a^L_j-y_j) \\
      \frac{\partial C}{\partial b^L_{j}} &amp; = &amp; \frac{1}{n} \sum_x 
      (a^L_j-y_j).
  \end{eqnarray}
</blockquote>
<p>This shows that if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown. In this case the quadratic cost is, in fact, an appropriate cost function to use.</p>
<h2 id="sigmoid-cross-entropy-cost-function">sigmoid + cross-entropy cost function</h2>
<p>The cross-entropy cost function <span class="math display">\[
C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a) \right] \tag 4
\]</span> where <span class="math inline">\(n\)</span> is the total number of items of training data, the sum is over all training inputs, <span class="math inline">\(x\)</span>, and <span class="math inline">\(y\)</span> is the corresponding desired output.</p>
The partial derivative of the cross-entropy cost with respect to the weights. We substitute <span class="math inline">\(a=\partial (z)\)</span> into (4), and apply the chain rule twice, obtaining:
\begin{eqnarray}
  \frac{\partial C}{\partial w_j} &amp; = &amp; -\frac{1}{n} \sum_x \left(
    \frac{y }{\sigma(z)} -\frac{(1-y)}{1-\sigma(z)} \right)
  \frac{\partial \sigma}{\partial w_j} \tag{5}\\
 &amp; = &amp; -\frac{1}{n} \sum_x \left( 
    \frac{y}{\sigma(z)} 
    -\frac{(1-y)}{1-\sigma(z)} \right)\sigma&#39;(z) x_j.
\tag{6}\end{eqnarray}
Putting everything over a common denominator and simplifying this becomes:
\begin{eqnarray}
  \frac{\partial C}{\partial w_j} &amp; = &amp; \frac{1}{n}
  \sum_x \frac{\sigma&#39;(z) x_j}{\sigma(z) (1-\sigma(z))}
  (\sigma(z)-y).
\tag{7}\end{eqnarray}
Using the definition of the sigmoid function, <span class="math inline">\(\sigma(z) =1/(1+e^{-z})\)</span>, and a little algebra we can show that <span class="math inline">\(\sigma&#39;(z) =\sigma(z)(1-\sigma(z))\)</span>. We see that the <span class="math inline">\(\sigma &#39; (z)\)</span> and <span class="math inline">\(\sigma (z)(1-\sigma (z))\)</span> terms cancel in the equation just above, and it simplifies to become:
\begin{eqnarray} 
  \frac{\partial C}{\partial w_j} =  \frac{1}{n} \sum_x x_j(\sigma(z)-y).
\tag{8}\end{eqnarray}
<p>This is a beautiful expression. It tells us that the rate at which the weight learns is controlled by <span class="math inline">\(σ(z)−y\)</span>, i.e., by the error in the output. The larger the error, the faster the neuron will learn. In particular, it avoids the learning slowdown caused by the <span class="math inline">\(σ′(z)\)</span> term in the analogous equation for the quadratic cost, Equation (2).</p>
In a similar way, we can compute the partial derivative for the bias.
\begin{eqnarray} 
  \frac{\partial C}{\partial b} = \frac{1}{n} \sum_x (\sigma(z)-y).
\tag{9}\end{eqnarray}
It’s easy to generalize the cross-entropy to many-neuron multi-layer networks. In particular, suppose <span class="math inline">\(y=y_1,y_2,...\)</span> are the desired values at the output neurons, i.e., the neurons in the final layer, while <span class="math inline">\(a^L_1,a^L_2,...\)</span> are the actual output values. Then we define the cross-entropy by
\begin{eqnarray} C = -\frac{1}{n} \sum_x \sum_j \left[y_j \ln a^L_j + (1-y_j) \ln (1-a^L_j) \right]. \end{eqnarray}
<h2 id="softmax-log-likelihood-cost">Softmax + log-likelihood cost</h2>
In a softmax layer we apply the so-called <span class="math inline">\(softmax function\)</span> to the <span class="math inline">\(z^L_j\)</span>. According to this function, the activation <span class="math inline">\(a^L_j\)</span> of the <span class="math inline">\(j\)</span>th output neuron is
\begin{eqnarray} 
  a^L_j = \frac{e^{z^L_j}}{\sum_k e^{z^L_k}},
\tag{10}\end{eqnarray}
<p>where in the denominator we sum over all the output neurons.</p>
The log-likelihood cost:
\begin{eqnarray}
  C \equiv -\ln a^L_y.
\tag{11}\end{eqnarray}
The partial derivative:
\begin{eqnarray}
  \frac{\partial C}{\partial b^L_j} &amp; = &amp; a^L_j-y_j  \tag{12}\\
  \frac{\partial C}{\partial w^L_{jk}} &amp; = &amp; a^{L-1}_k (a^L_j-y_j) 
\tag{13}\end{eqnarray}
<p>These expressions ensure that we will not encounter a learning slowdown. In fact, it’s useful to think of a softmax output layer with log-likelihood cost as being quite similar to a sigmoid output layer with cross-entropy cost.</p>
<blockquote>
<p>Given this similarity, should you use a sigmoid output layer and cross-entropy, or a softmax output layer and log-likelihood? In fact, in many situations both approaches work well. As a more general point of principle, softmax plus log-likelihood is worth using whenever you want to interpret the output activations as probabilities. That’s not always a concern, but can be useful with classification problems (like MNIST) involving disjoint classes.</p>
</blockquote>
<h2 id="overfitting">overfitting</h2>
<p>In general, one of the best ways of reducing overfitting is to increase the size of the training data. With enough training data it is difficult for even a very large network to overfit.</p>

	  </div>

		
		<ul class="tag_box list-unstyled list-inline">
		  <li><i class="fa fa-folder-open"></i></li>
		  
		  
			 
				<li><a href="/categories.html#foundation-ref">
					foundation <span>(27)</span>
					
				</a></li>
			
		  
		</ul>
		  

		
		<ul class="list-inline">
		  <li><i class="fa fa-tags"></i></li>
		  
		  
			 
				<li>
					<a href="/tags.html#foundation-ref">
					foundation <span>(27)</span>
					
					</a>
				</li>
			
		  
		  
		</ul>
		  

		<hr>

		<div>
      <section class="share col-sm-12">
        <h4 class="section-title">Share Post</h4>
        <a class="btn btn-default btn-sm twitter" href="http://twitter.com/share?text=Improving the way neural networks learn"
           onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
          <i class="fa fa-twitter fa-lg"></i>
          Twitter
        </a>
        <a class="btn btn-default btn-sm facebook" href="https://www.facebook.com/sharer/sharer.php"
           onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
          <i class="fa fa-facebook fa-lg"></i>
          Facebook
        </a>
        <a class="btn btn-default btn-sm gplus"
           onclick="window.open('https://plus.google.com/share?url='+window.location.href, 'google-plus-share', 'width=490,height=530');return false;">
          <i class="fa fa-google-plus fa-lg"></i>
          Google+
        </a>
      </section>
    </div>

    <div class="clearfix"></div>

		<ul class="pager">
		  
		  <li class="previous"><a href="/lua/2015/09/03/torch7-tensor-slicing.html" title="Torch7 Tensor slicing">&larr; Previous</a></li>
		  
		  
		  <li class="next"><a href="/lua/2015/09/14/linear-regression-example.html" title="linear regression example">Next &rarr;</a></li>
		  
		</ul>

		<hr>
	</div>
	
	<div class="col-sm-2 sidebar-2">
	
	</div>
</article>
<div class="clearfix"></div>





		<footer>
			<hr/>
			<p>
				&copy; 2015 <a href="http://guoqiang.gq">sohero</a>.
			</p>
		</footer>
	</div>

	<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
	<script src="//cdn.bootcss.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
	<script type="text/javascript" src="/assets/js/app.js"></script>
</body>
</html>



