<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Qiang Brother Notes.</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Policy Gradient</title>
				<description>&lt;p&gt;Directly parametrise the &lt;strong&gt;policy&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\pi_\theta(s,a)=\Bbb P[a|s,\theta]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;advantages-of-policy-based-rl&quot;&gt;Advantages of Policy-Based RL&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Better convergence properties&lt;/li&gt;
&lt;li&gt;Effective in high-dimensional or continuous action spaces&lt;/li&gt;
&lt;li&gt;Can learn stochastic policies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Typically converge to a local rather than global optimum&lt;/li&gt;
&lt;li&gt;Evaluting a policy is typically inefficient and high variance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;policy-objective-functions&quot;&gt;Policy Objective Functions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: given policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_\theta(s,a)\)&lt;/span&gt; with parameters &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;, find best &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Policy based reinforcement learning is an &lt;code&gt;optimisation&lt;/code&gt; problem, find &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt; that &lt;strong&gt;maximises&lt;/strong&gt; &lt;span class=&quot;math inline&quot;&gt;\(J(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;policy-gradient&quot;&gt;Policy Gradient&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&quot;math inline&quot;&gt;\(J(\theta)\)&lt;/span&gt; be any policy objective function. Policy gradient algorithms search for a &lt;em&gt;local&lt;/em&gt; maximum in &lt;span class=&quot;math inline&quot;&gt;\(J(\theta)\)&lt;/span&gt; by ascending the gradient of the policy, w.r.t. parameters &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\theta=\alpha\nabla_\theta J(\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;score-function&quot;&gt;Score Function&lt;/h2&gt;
&lt;p&gt;Assume policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_\theta\)&lt;/span&gt; is differentiable whenever it is non-zero and we know the gradient &lt;span class=&quot;math inline&quot;&gt;\(\nabla_\theta \pi_\theta(s,a)\)&lt;/span&gt;. &lt;code&gt;Likelihood ratios&lt;/code&gt; exploit the following identity &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\nabla_\theta\pi_\theta(s,a) &amp;amp;= \pi_\theta(s,a) \frac{\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)} \\
&amp;amp;= \pi_\theta(s,a) \nabla_\theta log \pi_\theta(s,a)
\end{align}
\]&lt;/span&gt; The &lt;code&gt;score function&lt;/code&gt; is &lt;span class=&quot;math inline&quot;&gt;\(\nabla_\theta log \pi_\theta(s,a)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h2&gt;
&lt;h3 id=&quot;one-step-mdps&quot;&gt;One-Step MDPs&lt;/h3&gt;
&lt;p&gt;Consider a simple class of &lt;em&gt;one-step&lt;/em&gt; MDPs: Starting in state &lt;span class=&quot;math inline&quot;&gt;\(s\sim d(s)\)&lt;/span&gt;; Terminating after one time-step with reward &lt;span class=&quot;math inline&quot;&gt;\(r=\mathcal R_{s,a}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Use likelihood ratios to compute the policy gradient &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
J(\theta) &amp;amp;= \Bbb E_{\pi_\theta}[r] \\
&amp;amp;= \sum_{\mathcal{s\in S}} d(s) \sum_{a\in\mathcal A} \pi_\theta(s,a)\mathcal R_{s,a} \\
\nabla_\theta J(\theta) &amp;amp;= \sum_{s\in\mathcal S}d(s)\sum_{a\in\mathcal A}\pi_\theta(s,a)\nabla_\theta log \pi_\theta(s,a)\mathcal R_{s,a} \\
&amp;amp;=\Bbb E_{\pi_\theta}\left[\nabla_\theta log \pi_\theta(s,a)r\right]
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For any differentiable policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_\theta(s,a)\)&lt;/span&gt;, for any of the policy objective functions &lt;span class=&quot;math inline&quot;&gt;\(J\)&lt;/span&gt;, the policy gradient is &lt;span class=&quot;math display&quot;&gt;\[
\nabla_\theta J(\theta)=\Bbb E_{\pi_\theta}
\left[
\nabla_\theta log\pi_\theta(s,a)Q^{\pi_\theta}(s,a)
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;monte-carlo-policy-gradient&quot;&gt;Monte-Carlo Policy Gradient&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Update parameters by stochastic gradient ascent&lt;/li&gt;
&lt;li&gt;Using policy gradient theorem&lt;/li&gt;
&lt;li&gt;Using return &lt;span class=&quot;math inline&quot;&gt;\(v_t\)&lt;/span&gt; as an unbiased sample of &lt;span class=&quot;math inline&quot;&gt;\(Q^{\pi_\theta}(s_t,a_t)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\theta_t=\alpha\nabla_\theta log \pi_\theta(s_t,a_t)v_t
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;actor-critic-policy-gradient&quot;&gt;Actor-Critic Policy Gradient&lt;/h2&gt;
&lt;h3 id=&quot;reducing-variance-using-a-critic&quot;&gt;Reducing Variance Using a Critic&lt;/h3&gt;
&lt;p&gt;Monte-Carlo policy gradient still has high variance. We use a &lt;code&gt;critic&lt;/code&gt; to estimate the action-value function &lt;span class=&quot;math display&quot;&gt;\[
Q_w(s,a)\approx Q^{\pi_\theta}(s,a)
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 23 Jun 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/06/23/policy-gradient.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/06/23/policy-gradient.html</guid>
			</item>
		
			<item>
				<title>Value Function Approximation</title>
				<description>&lt;h2 id=&quot;incremental-methods&quot;&gt;Incremental Methods&lt;/h2&gt;
&lt;h3 id=&quot;types-of-value-function-approximation&quot;&gt;Types of Value Function Approximation&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/48.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;value-function-approx.-by-stochastic-gradient-descent&quot;&gt;Value Function Approx. By Stochastic Gradient Descent&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: find parameter vector &lt;span class=&quot;math inline&quot;&gt;\(\mathbf w\)&lt;/span&gt; minimising mean-squared error between approximate value fn &lt;span class=&quot;math inline&quot;&gt;\(\hat v(s,\mathbf w)\)&lt;/span&gt; and true value fn &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
J(\mathbf w)=\mathbb E_\pi\left[
\left(
v_\pi(S)-\hat v(S,\mathbf w)
\right)^2
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;feature-vectors&quot;&gt;Feature Vectors&lt;/h3&gt;
&lt;p&gt;Represent state by a &lt;em&gt;feature vector&lt;/em&gt; &lt;span class=&quot;math display&quot;&gt;\[
\mathbf x(S)=\begin{bmatrix}
x_1(S) \\
\vdots \\
x_n(S)
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;linear-value-function-approximation&quot;&gt;Linear Value Function Approximation&lt;/h3&gt;
&lt;p&gt;Represent value function by a linear combination of features &lt;span class=&quot;math display&quot;&gt;\[
\hat v(S,\mathbf w)=\mathbf x(S)^T \mathbf w
=\sum_{j=1}^n \mathbf x_j(S)\mathbf w_j
\]&lt;/span&gt; Objective function is quadratic in parameters &lt;span class=&quot;math inline&quot;&gt;\(\mathbf w\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
J(\mathbf w)=\mathbb E_\pi\left[
\left(
v_\pi(S)- \mathbf x(S)^T \mathbf w
\right)^2
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;incremental-prediction-algorithms&quot;&gt;Incremental Prediction Algorithms&lt;/h3&gt;
&lt;p&gt;In practice, we substitute a &lt;em&gt;target&lt;/em&gt; for &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt;. For MC, the target is the return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\mathbf w=\alpha(\color{red}{G_t}-\hat v(S_t,\mathbf w))
\nabla_{\mathbf w}\hat v(S_t,\mathbf w)
\]&lt;/span&gt; For TD(0) the target is the TD target &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma\hat v(S_{t+1},\mathbf w)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\mathbf w=\alpha
(\color{red}{R_{t+1}+\gamma\hat v(S_{t+1},\mathbf w)}-\hat v(S_t,\mathbf w))
\nabla_{\mathbf w}\hat v(S_t,\mathbf w)
\]&lt;/span&gt; For TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;), the target is the &lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;-return &lt;span class=&quot;math inline&quot;&gt;\(G_t^\lambda\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\mathbf w=\alpha(\color{red}{G_t^\lambda}-\hat v(S_t,\mathbf w))
\nabla_{\mathbf w}\hat v(S_t,\mathbf w)
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Monte-Carlo evaluation converges to a local optimum, even when using non-linear value function approximation. Linear TD(0) converges (close) to global optimum.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;action-value-function-approximation&quot;&gt;Action-Value Function Approximation&lt;/h3&gt;
&lt;p&gt;Approximate the action-value function &lt;span class=&quot;math display&quot;&gt;\[
\hat q(S,A,\mathbf w) \approx q_\pi(S,A)
\]&lt;/span&gt; Minimise mean-squared error between approximate action-value fn &lt;span class=&quot;math inline&quot;&gt;\(\hat q(S,A,\mathbf w)\)&lt;/span&gt; and true action-value fn &lt;span class=&quot;math inline&quot;&gt;\(q_\pi(S,A)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
J(\mathbf w)=\Bbb E_\pi\left[
\left(
q_\pi(S,A)-\hat q(S,A,\mathbf w)
\right)^2
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;convergence-of-prediction-algorithms&quot;&gt;Convergence of Prediction Algorithms&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/49.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;gradient-temporal-difference-learning&quot;&gt;Gradient Temporal-Difference Learning&lt;/h3&gt;
&lt;p&gt;TD does not follow the gradient of &lt;em&gt;any&lt;/em&gt; objective function, this is why TD can diverge when off-policy or using non-linear function approximation.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Gradient TD&lt;/code&gt; follows true gradient of projected Bellman error&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/50.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;convergence-of-control-algorithms&quot;&gt;Convergence of Control Algorithms&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/51.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;batch-reinforcement-learning&quot;&gt;Batch Reinforcement Learning&lt;/h2&gt;
&lt;h3 id=&quot;least-squares-prediction&quot;&gt;Least Squares Prediction&lt;/h3&gt;
&lt;p&gt;Give value function approximation &lt;span class=&quot;math inline&quot;&gt;\(\hat v(s,\mathbf w)\approx v_\pi(s)\)&lt;/span&gt;, and experience &lt;span class=&quot;math inline&quot;&gt;\(\mathcal D\)&lt;/span&gt; consisting of &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;state, value \right&amp;gt;\)&lt;/span&gt; pairs &lt;span class=&quot;math display&quot;&gt;\[
\mathcal D=\left\{
\left&amp;lt;s_1,v_1^\pi \right&amp;gt;,
\left&amp;lt;s_2,v_2^\pi \right&amp;gt;,
...,
\left&amp;lt;s_T,v_T^\pi \right&amp;gt;,
\right\}
\]&lt;/span&gt; &lt;strong&gt;Least squared&lt;/strong&gt; algorithms find parameter vector &lt;span class=&quot;math inline&quot;&gt;\(\mathbf w\)&lt;/span&gt; minimising sum-squared error between &lt;span class=&quot;math inline&quot;&gt;\(\hat v(s_t,\mathbf w)\)&lt;/span&gt; and target values &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
LS(\mathbf w) &amp;amp;= \sum_{t=1}^T (v_t^\pi-\hat v(s_t,\mathbf w))^2 \\
&amp;amp;= \Bbb E_{\mathcal D} \left[(v_t^\pi-\hat v(s_t,\mathbf w))^2\right]
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;stochastic-gradient-descent-with-experience-replay&quot;&gt;Stochastic Gradient Descent with Experience Replay&lt;/h3&gt;
&lt;p&gt;Given experience consisting of &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;state, value\right&amp;gt;\)&lt;/span&gt; pairs &lt;span class=&quot;math display&quot;&gt;\[
\mathcal D=\left\{
\left&amp;lt;s_1,v_1^\pi \right&amp;gt;,
\left&amp;lt;s_2,v_2^\pi \right&amp;gt;,
...,
\left&amp;lt;s_T,v_T^\pi \right&amp;gt;,
\right\}
\]&lt;/span&gt; Repeat:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample state, value from experience &lt;span class=&quot;math display&quot;&gt;\[
 \left&amp;lt;s, v^\pi\right&amp;gt;\sim \mathcal D
 \]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Apply stochastic gradient descent update &lt;span class=&quot;math display&quot;&gt;\[
 \Delta\mathbf w=\alpha(v^\pi-\hat v(s,\mathbf w)) \nabla_{\mathbf w}
 \hat v(s, \mathbf w)
 \]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Converges to least squares solution &lt;span class=&quot;math display&quot;&gt;\[
\mathbf w^\pi=\mathop{\text{argmin}}\limits_{\mathbf w} LS(\mathbf w)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;linear-least-squares-prediction&quot;&gt;Linear Least Squares Prediction&lt;/h3&gt;
&lt;p&gt;At minimum of &lt;span class=&quot;math inline&quot;&gt;\(LS(\mathbf w)\)&lt;/span&gt;, the expected update must be zero &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\Bbb E_{\mathcal D}[\Delta\mathbf w] &amp;amp;= 0 \\
\alpha\sum_{t=1}^T \mathbf x(s_t)(v_t^\pi-\mathbf x(s_t)^T\mathbf w) &amp;amp;= 0 \\
\sum_{t=1}^T\mathbf x(s_t)v_t^\pi &amp;amp;= \sum_{t=1}^T \mathbf x(s_t)\mathbf x(s_t)^T\mathbf w \\
\mathbf w &amp;amp;= \left(
\sum_{t=1}^T\mathbf x(s_t)\mathbf x(s_t)^T
\right)^{-1} \sum_{t=1}^T \mathbf x(s_t)v_t^\pi
\end{align}
\]&lt;/span&gt; For &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; features, direct solution time is &lt;span class=&quot;math inline&quot;&gt;\(O(N^3)\)&lt;/span&gt;. Incremental solution time is &lt;span class=&quot;math inline&quot;&gt;\(O(N^2)\)&lt;/span&gt; using Shermann-Morrison.&lt;/p&gt;
&lt;h3 id=&quot;linear-least-squares-prediction-algorithms&quot;&gt;Linear Least Squares Prediction Algorithms&lt;/h3&gt;
&lt;p&gt;In practice, we do not know true values &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\)&lt;/span&gt;, our “training data” muse use noisy or biased samples of &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LSMC (Least Squares Monte-Carlo) uses return &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\approx G_t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;LSTD (Least Squares Temporal-Difference) uses TD target &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\approx R_{t+1}+\gamma\hat v(S_{t+1},\mathbf w)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;LSTD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;) (Least Squares TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)) use &lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;-return &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\approx G_t^\lambda\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;convergence-of-linear-least-squares-prediction-algorithms&quot;&gt;Convergence of Linear Least Squares Prediction Algorithms&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/52.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
</description>
				<pubDate>Sun, 19 Jun 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/06/19/value-function-approximation.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/06/19/value-function-approximation.html</guid>
			</item>
		
			<item>
				<title>Model free Control</title>
				<description>&lt;p&gt;&lt;strong&gt;Model-free prediction&lt;/strong&gt;: &lt;em&gt;Estimate&lt;/em&gt; the value function of an &lt;em&gt;unknown&lt;/em&gt; MDP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model-free Control&lt;/strong&gt;: &lt;em&gt;Optimise&lt;/em&gt; the value function of an &lt;em&gt;unknown&lt;/em&gt; MDP.&lt;/p&gt;
&lt;h2 id=&quot;on-and-off-policy-learning&quot;&gt;On and Off-Policy Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;On-policy learning
&lt;ul&gt;
&lt;li&gt;“Learn on the job”&lt;/li&gt;
&lt;li&gt;Learn about policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; from experience sampled from &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Off-policy learning
&lt;ul&gt;
&lt;li&gt;“Look over someone’s shoulder”&lt;/li&gt;
&lt;li&gt;Learn about policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; from experience sampled from &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;glie&quot;&gt;GLIE&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Greedy in the Limit with Infinite Exploration&lt;/em&gt; (GLIE).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All state-action pairs are explored infinitely many times, &lt;span class=&quot;math inline&quot;&gt;\(\lim\limits_{k\to\infty}N_k(s,a)=\infty\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The policy converges on a greedy policy, &lt;span class=&quot;math display&quot;&gt;\[
\lim\limits_{k\to\infty}\pi_k(a|s)=1(a=\mathop{\text{argmax}}\limits_{a&amp;#39;\in\mathcal{A}}Q_k(s,a&amp;#39;))
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;-greedy is GLIE if &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt; reduces to zero at &lt;span class=&quot;math inline&quot;&gt;\(\epsilon_k={1\over k}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;glie-monte-carlo-control&quot;&gt;GLIE Monte-Carlo Control&lt;/h2&gt;
&lt;p&gt;Sample &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt;th episode using &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;:&lt;span class=&quot;math inline&quot;&gt;\(\{\mathcal{S_1,A_1,R_2,...,S_T}\}\sim\pi\)&lt;/span&gt;, for each state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal S_t\)&lt;/span&gt; and action &lt;span class=&quot;math inline&quot;&gt;\(\mathcal A_t\)&lt;/span&gt; in the episode &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
N(\mathcal{S_t,A_t}) &amp;amp;\leftarrow N(\mathcal{S_t,A_t})+1 \\
Q(\mathcal{S_t,A_t}) &amp;amp;\leftarrow Q(\mathcal{S_t,A_t}) + {1\over N(\mathcal{S_t,A_t})}\left(G_t-Q(\mathcal{S_t,A_t})\right)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Improve policy based on new action-value function &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\epsilon &amp;amp;\leftarrow {1\over k} \\
\pi &amp;amp;\leftarrow \epsilon\text{-greedy}(Q)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;saralambda&quot;&gt;Sara(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
Q(\mathcal{S,A})\leftarrow Q(\mathcal{S,A})+\alpha(R+\gamma Q(\mathcal{S&amp;#39;,A&amp;#39;})-Q(\mathcal{S,A}))
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;n-step-sarsa&quot;&gt;&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-Step Sarsa&lt;/h3&gt;
&lt;p&gt;Define the &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step &lt;span class=&quot;math inline&quot;&gt;\(Q\)&lt;/span&gt;-return &lt;span class=&quot;math display&quot;&gt;\[
q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n})
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step Sarsa updates &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt; towards the &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step &lt;span class=&quot;math inline&quot;&gt;\(Q\)&lt;/span&gt;-return &lt;span class=&quot;math display&quot;&gt;\[
Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha\left(q_t^{(n)}-Q(S_t,A_t)\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;forward-view-sarsalambda&quot;&gt;Forward View Sarsa(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;The &lt;span class=&quot;math inline&quot;&gt;\(q^\lambda\)&lt;/span&gt; return combines all &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step &lt;span class=&quot;math inline&quot;&gt;\(Q\)&lt;/span&gt;-returns &lt;span class=&quot;math inline&quot;&gt;\(q_t^{(n)}\)&lt;/span&gt;, using weight &lt;span class=&quot;math inline&quot;&gt;\((1-\lambda)\lambda^{n-1}\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
q_t^\lambda=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)}
\]&lt;/span&gt; Forward-view Sarsa(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;) &lt;span class=&quot;math display&quot;&gt;\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left(q_t^\lambda-Q(S_t,A_t)\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;backward-view-sarsalambda&quot;&gt;Backward View Sarsa(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;Just like TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;), we use &lt;code&gt;eligibility traces&lt;/code&gt; in an online algorithm. But Sarsa(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;) has one eligibility trace for each state-action pair &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
E_0(s,a) &amp;amp;= 0 \\
E_t(s,a) &amp;amp;= \gamma\lambda E_{t-1}(s,a)+1(S_t=s,A_t=a)
\end{align}
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt; is updated for every state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; and action &lt;span class=&quot;math inline&quot;&gt;\(a\)&lt;/span&gt;. Inproportion to TD-error &lt;span class=&quot;math inline&quot;&gt;\(\delta_t\)&lt;/span&gt; and eligibility trace &lt;span class=&quot;math inline&quot;&gt;\(E_t(s,a)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})-Q(S_t,A_t)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
Q(s,a)\leftarrow Q(s,a)+\alpha\delta_t E_t(s,a)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;off-policy-learning&quot;&gt;Off-Policy Learning&lt;/h2&gt;
&lt;p&gt;Evaluate target policy &lt;span class=&quot;math inline&quot;&gt;\(\pi(a|s)\)&lt;/span&gt; to compute &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(q_\pi(s,a)\)&lt;/span&gt;, while following behaviour policy &lt;span class=&quot;math inline&quot;&gt;\(\mu(a|s)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\{\mathcal{S_1,A_1,R_2,...,S_t}\}\sim \mu
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;importance-sampling-for-off-policy-monte-carlo&quot;&gt;Importance Sampling for Off-Policy Monte-Carlo&lt;/h3&gt;
&lt;p&gt;Use returns generated from &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; to evaluate &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;. Weight return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; according to similarity between policies. Multiply importance sampling corrections along whole episode &lt;span class=&quot;math display&quot;&gt;\[
G_t^{\pi / \mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}
\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}
...
\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}
G_t
\]&lt;/span&gt; Update value towards &lt;em&gt;corrected&lt;/em&gt; return &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t)+\alpha\left(
\color{red}{G_t^{\pi / \mu}}
-V(S_t)\right)
\]&lt;/span&gt; Cannot use if &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; is zero when &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; is non-zero. Importance sampling can dramatically increase variance.&lt;/p&gt;
&lt;h3 id=&quot;importance-sampling-for-off-policy-td&quot;&gt;Importance Sampling for Off-Policy TD&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
V(S_t)\leftarrow V(S_t)+\alpha\left(
\color{red}{\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}}
(R_{t+1}+\gamma V(S_{t+1}))-V(s_t)
\right)
\]&lt;/span&gt; Much lower variance than Monte-Carlo importance sampling. Policies only need to be similar over a single step.&lt;/p&gt;
&lt;h3 id=&quot;q-learning&quot;&gt;Q-Learning&lt;/h3&gt;
&lt;p&gt;Next action is chosen using behaviour policy &lt;span class=&quot;math inline&quot;&gt;\(A_{t+1}\sim\mu(\cdot |S_t)\)&lt;/span&gt;, but we consider alternative successor action &lt;span class=&quot;math inline&quot;&gt;\(A&amp;#39;\sim\pi(\cdot |S_t)\)&lt;/span&gt;, and update &lt;span class=&quot;math inline&quot;&gt;\(Q(S_t,A_t)\)&lt;/span&gt; towards value of alternative action &lt;span class=&quot;math display&quot;&gt;\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left(
\color{red}{R_{t+1}+\gamma Q(S_{t+1},A&amp;#39;)}
-Q(S_t,A_t)
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;off-policy-control-with-q-learning&quot;&gt;Off-Policy Control with Q-Learning&lt;/h3&gt;
&lt;p&gt;We now allow both behaviour and target policies to improve. The target policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; is greedy w.r.t. &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\pi(S_{t+1})=\mathop{\text{argmax}} \limits_{a&amp;#39;}Q(S_{t+1},a&amp;#39;)
\]&lt;/span&gt; The behaviour policy &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; is e.g. &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;-greedy w.r.t. &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt;. The Q-learning target then simplifies: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
R_{t+1} &amp;amp;+ \gamma Q(S_{t+1},A&amp;#39;) \\
=R_{t+1} &amp;amp;+ \gamma Q(S_{t+1}, \mathop{\text{argmax}} \limits_{a&amp;#39;} Q(S_{t+1},a&amp;#39;)) \\
=R_{t+1} &amp;amp;+ \max_{a&amp;#39;} \gamma Q(S_{t+1},a&amp;#39;)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;q-learning-control-algorithm&quot;&gt;Q-Learning Control Algorithm&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
Q(S,A)\leftarrow Q(S,A)+\alpha\left(R+\gamma\max_{a&amp;#39;}Q(S&amp;#39;,a&amp;#39;)-Q(S,A)\right)
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 11 Jun 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/06/11/model-free-control.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/06/11/model-free-control.html</guid>
			</item>
		
			<item>
				<title>RL Concepts</title>
				<description>&lt;p&gt;&lt;strong&gt;backup&lt;/strong&gt;: We &lt;em&gt;back up&lt;/em&gt; the value of the state after each greedy move to the state before the move.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;experience&lt;/strong&gt;: sample sequences of &lt;em&gt;states&lt;/em&gt;, &lt;em&gt;actions&lt;/em&gt;, and &lt;em&gt;rewards&lt;/em&gt; from actual or simulated interaction with an environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nonstationary&lt;/strong&gt;: The return after taking an action in one state depends on the actions taken in later states in the same episode. Because all action selections are undergoing learning, the problem becomes nonstationary from the point view of the earlier state.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;prediction problem&lt;/strong&gt;: The computation of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(q_\pi\)&lt;/span&gt; for a fixed solution policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bootstrapping&lt;/strong&gt;: All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea &lt;em&gt;bootstrapping&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;exploring starts&lt;/strong&gt;: For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes &lt;em&gt;start in a state-action pair&lt;/em&gt;, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of &lt;em&gt;exploring starts&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-greedy&lt;/strong&gt;: most of the time they choose an action that maximal estimated action value, but with probability &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt; they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, &lt;span class=&quot;math inline&quot;&gt;\({\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt;, and the remaining bulk of the probability, &lt;span class=&quot;math inline&quot;&gt;\(1-\varepsilon +{\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt;, is given to the greedy action. The &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-greedy policies are examples of &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-soft policies, defined as policies for which &lt;span class=&quot;math inline&quot;&gt;\(\pi(a|s)\ge {\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt; for all states and actions, for some &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon &amp;gt;0\)&lt;/span&gt;. Among &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-soft policies, $-greedy policies are in some sense those that are closest to greedy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;target policy&lt;/strong&gt;: The policy being learned about is called the &lt;em&gt;target policy&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;behavior policy&lt;/strong&gt;: The policy used to generate behavior is called the &lt;em&gt;behavior policy&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;off-policy learning&lt;/strong&gt;: Learning is from data “off” the target policy, and the overall process is termed &lt;em&gt;off-policy learning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;importance sampling&lt;/strong&gt;: a general technique for estimating expected values under one distribution given samples from another.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;importance-sampling ratio&lt;/strong&gt;: We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the &lt;em&gt;importance-sampling ratio&lt;/em&gt;. &lt;span class=&quot;math display&quot;&gt;\[
\rho_t^T=\frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}\mu(A_k|S_k)p(S_{k+1}|S_k,A_k)}
=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ordinary importance sampling&lt;/strong&gt;: Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; to estimate &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(\mathcal{s})\)&lt;/span&gt;. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time &lt;span class=&quot;math inline&quot;&gt;\(t=101\)&lt;/span&gt;. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{s}\)&lt;/span&gt; is visited, denoted &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{T}(\mathcal{s})\)&lt;/span&gt;. This is for an every-visit method; for a first-visit method, &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{T}(\mathcal{s})\)&lt;/span&gt; would only include time steps that were first visits to &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{s}\)&lt;/span&gt; within their episodes. Also, let &lt;span class=&quot;math inline&quot;&gt;\(T(t)\)&lt;/span&gt; denote the first time of termination following time &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; denote the return after &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt; up through &lt;span class=&quot;math inline&quot;&gt;\(T(t)\)&lt;/span&gt;. Then &lt;span class=&quot;math inline&quot;&gt;\(\{G_t\}_{t\in{\mathcal{T}(\mathcal{s})}}\)&lt;/span&gt; are the returns that pertain to state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{s}\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(\{\rho_t^{T(t)}\}_{t\in{\mathcal{T}(\mathcal{s})}}\)&lt;/span&gt; are the corresponding importance-sampling ratios. To estimate &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(\mathcal{s})\)&lt;/span&gt;, we simply scale the returns by the ratios and average the results: &lt;span class=&quot;math display&quot;&gt;\[
V(\mathcal{s})=\frac{\sum_{t\in{\mathcal{T}(\mathcal{s})}}\rho_t^{T(t)}G_t}{|\mathcal{T}(\mathcal{s})|}
\]&lt;/span&gt; When importance sampling is done as a simple average in this way it is called &lt;em&gt;ordinary importance sampling&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;weighted importance sampling&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
V(\mathcal{s})=\frac{\sum_{t\in\mathcal{T}(\mathcal{s})} \rho_t^{T(t)}G_t}{\sum_{t\in\mathcal{T}(\mathcal{s})}\rho_t^{T(t)}}
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 20 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/20/rl-concepts.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/20/rl-concepts.html</guid>
			</item>
		
			<item>
				<title>Temporal-Difference Learning</title>
				<description>&lt;ul&gt;
&lt;li&gt;TD methods learn directly from episodes of experience&lt;/li&gt;
&lt;li&gt;TD is &lt;em&gt;model-free&lt;/em&gt;: no knowledge of MDP transitions / rewards&lt;/li&gt;
&lt;li&gt;TD learns from &lt;em&gt;incomplete&lt;/em&gt; episodes, by &lt;em&gt;bootstrapping&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;TD updates a guess towards a guess&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;mc-and-td&quot;&gt;MC and TD&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: learn &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; online from experience under policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;incremental-every-visit-monte-carlo&quot;&gt;Incremental every-visit Monte-Carlo&lt;/h3&gt;
&lt;p&gt;Update value &lt;span class=&quot;math inline&quot;&gt;\(V(S_t)\)&lt;/span&gt; toward &lt;em&gt;actual&lt;/em&gt; return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;simplest-temporal-difference-learning-algorithm-td0&quot;&gt;Simplest temporal-difference learning algorithm: TD(0)&lt;/h3&gt;
&lt;p&gt;Update value &lt;span class=&quot;math inline&quot;&gt;\(V(S_t)\)&lt;/span&gt; toward &lt;em&gt;estimated&lt;/em&gt; return &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma V(S_{t+1})\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha\left(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right)
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma V(S_{t+1})\)&lt;/span&gt; is called the &lt;em&gt;TD&lt;/em&gt; target.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\)&lt;/span&gt; is called the &lt;em&gt;TD&lt;/em&gt; error.&lt;/p&gt;
&lt;h2 id=&quot;biasvariance-trade-off&quot;&gt;Bias/Variance Trade-Off&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Return &lt;span class=&quot;math inline&quot;&gt;\(G_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1}R_T\)&lt;/span&gt; is &lt;em&gt;unbiased&lt;/em&gt; estimate of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(S_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;True TD target &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma v_\pi(S_{t+1})\)&lt;/span&gt; is &lt;em&gt;unbiased&lt;/em&gt; estimate of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(S_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;TD target &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma V(S_{t+1})\)&lt;/span&gt; is &lt;em&gt;biased&lt;/em&gt; estimate of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(S_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;TD target is much lower variance than the return:
&lt;ul&gt;
&lt;li&gt;Return depends on &lt;em&gt;many&lt;/em&gt; random actions, transitions, rewards&lt;/li&gt;
&lt;li&gt;TD target depends on &lt;em&gt;one&lt;/em&gt; random action, transition, reward&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;advantages-and-disadvantages-of-mc-vs.td&quot;&gt;Advantages and Disadvantages of MC vs. TD&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MC has high variance, zero bias
&lt;ul&gt;
&lt;li&gt;Good convergence properties&lt;/li&gt;
&lt;li&gt;(even with function approximation)&lt;/li&gt;
&lt;li&gt;Not very sensitive to initial value&lt;/li&gt;
&lt;li&gt;Very simple to understand and use&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;TD has low variance, some bias
&lt;ul&gt;
&lt;li&gt;Usually more efficient than MC&lt;/li&gt;
&lt;li&gt;TD(0) converges to &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;(but not always with function approximation)&lt;/li&gt;
&lt;li&gt;More sensitive to initial value&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;monte-carlo-backup&quot;&gt;Monte-Carlo Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/42.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;temporal-difference-backup&quot;&gt;Temporal-Difference Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/43.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;dynamic-programming-backup&quot;&gt;Dynamic Programming Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/44.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;bootstrapping-and-sampling&quot;&gt;Bootstrapping and Sampling&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt;: update involves an estimate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC does not bootstrap&lt;/li&gt;
&lt;li&gt;DP bootstraps&lt;/li&gt;
&lt;li&gt;TD bootstraps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sampling&lt;/strong&gt;: update samples an expectation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC samples&lt;/li&gt;
&lt;li&gt;DP does not sample&lt;/li&gt;
&lt;li&gt;TD samples&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;unified-view-of-reinforcement-learning&quot;&gt;Unified View of Reinforcement Learning&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/45.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;tdlambda&quot;&gt;TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h2&gt;
&lt;h3 id=&quot;n-step-return&quot;&gt;&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step return&lt;/h3&gt;
&lt;p&gt;Define the &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step return: &lt;span class=&quot;math display&quot;&gt;\[
G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step temporal-difference learning &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t)+\alpha\left(G_t^{(n)}-V(S_t)\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;averaging-n-step-returns&quot;&gt;averaging &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step returns&lt;/h3&gt;
&lt;p&gt;e.g. average the 2-step and 4-step returns &lt;span class=&quot;math display&quot;&gt;\[
{1 \over 2}G^{(2)} + {1 \over 2}G^{(4)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;lambda-return&quot;&gt;&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;-return&lt;/h3&gt;
&lt;p&gt;The &lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;-return &lt;span class=&quot;math inline&quot;&gt;\(G_t^{\lambda}\)&lt;/span&gt; combines all &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step returns &lt;span class=&quot;math inline&quot;&gt;\(G_t^{(n)}\)&lt;/span&gt;. Using weight &lt;span class=&quot;math inline&quot;&gt;\((1-\lambda)\lambda^{n-1}\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)}
\]&lt;/span&gt; Forward-view TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;) &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha\left(G_t^\lambda-V(S_t)\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;eligibility-traces&quot;&gt;Eligibility Traces&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/46.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Credit assignment problem: did bell or light cause shock?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frequency heuristic&lt;/strong&gt;: assign credit to most frequent states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recency heuristic&lt;/strong&gt;: assign credit to most recent states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Elegibility traces&lt;/strong&gt; combine both heuristics: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
E_0(s) &amp;amp;= 0 \\
E_t(s) &amp;amp;= \gamma\lambda E_{t-1}(s) + 1(S_t=s)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/47.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;backward-view-tdlambda&quot;&gt;Backward view TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;Keep an eligibility trace for every state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;. Update value &lt;span class=&quot;math inline&quot;&gt;\(V(s)\)&lt;/span&gt; for every state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;. In proportion to TD-error &lt;span class=&quot;math inline&quot;&gt;\(\delta_t\)&lt;/span&gt; and eligibility trace &lt;span class=&quot;math inline&quot;&gt;\(E_t(s)\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\delta_t &amp;amp;= R_{t+1}+\gamma V(S_{t+1})-V(S_t) \\
V(s) &amp;amp;\leftarrow V(s)+\alpha\delta_t E_t(s)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 19 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/19/temporal-difference-learning.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/19/temporal-difference-learning.html</guid>
			</item>
		
			<item>
				<title>Monte-Carlo Reinforcement Learning</title>
				<description>&lt;ul&gt;
&lt;li&gt;MC methods learn directly from episodes of experience&lt;/li&gt;
&lt;li&gt;MC is &lt;em&gt;model-free&lt;/em&gt;: no knowledge of MDP transitions / rewards&lt;/li&gt;
&lt;li&gt;MC learns from &lt;em&gt;complete&lt;/em&gt; episodes: no bootstrapping&lt;/li&gt;
&lt;li&gt;MC uses the simplest possible idea: value = mean return&lt;/li&gt;
&lt;li&gt;Caveat: can only apply MC to &lt;em&gt;episodic&lt;/em&gt; MDPs. All episodes must terminate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;monte-carlo-policy-evaluation&quot;&gt;Monte-Carlo Policy Evaluation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: learn &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; from episodes of experience under policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Monte-Carlo policy evaluation uses &lt;code&gt;empirical mean return&lt;/code&gt; instead of &lt;em&gt;expected return&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&quot;incremental-mean&quot;&gt;Incremental Mean&lt;/h2&gt;
&lt;p&gt;The mean &lt;span class=&quot;math inline&quot;&gt;\(\mu_1,\mu_2,...\)&lt;/span&gt; of a sequence &lt;span class=&quot;math inline&quot;&gt;\(x_1,x_2,...\)&lt;/span&gt; can be computed incrementally, &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\mu_k &amp;amp;= {1 \over k}\sum_{j=1}^k x_j \\
&amp;amp;= {1 \over k}\left(x_k+\sum_{j=1}^{k-1} x_j\right) \\
&amp;amp;= {1 \over k}\left(x_k+(k-1)\mu_{k-1} \right) \\
&amp;amp;= \mu_{k-1} + {1 \over k}(x_k - \mu_{k-1})
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;incremental-monte-carlo-updates&quot;&gt;Incremental Monte-Carlo Updates&lt;/h2&gt;
&lt;p&gt;Update &lt;span class=&quot;math inline&quot;&gt;\(V(s)\)&lt;/span&gt; incrementally after episode &lt;span class=&quot;math inline&quot;&gt;\(S_1,A_1,R_2,...,S_T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For each state &lt;span class=&quot;math inline&quot;&gt;\(S_t\)&lt;/span&gt; with return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
N(S_t) &amp;amp;\leftarrow N(S_t) + 1 \\
V(S_t) &amp;amp;\leftarrow V(S_t) + {1 \over N(S_t)}(G_t-V(S_t))
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes. &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 19 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/19/monte-carlo-reinforcement-learning.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/19/monte-carlo-reinforcement-learning.html</guid>
			</item>
		
			<item>
				<title>Dynamic Programming</title>
				<description>&lt;p&gt;A method for solving complex problems by breaking them down into subproblems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slove the subproblems&lt;/li&gt;
&lt;li&gt;Combine solutions to subproblems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dynamic programming assumes full knowledge of the MDP. It is used for &lt;em&gt;planning&lt;/em&gt; in an MDP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For prediction:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Input: MDP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&amp;gt;\)&lt;/span&gt; and policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; or MRP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal P^\pi,\mathcal R^\pi,\gamma \right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Output: value function &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For control:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Input: MDP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Output: optimal value function &lt;span class=&quot;math inline&quot;&gt;\(v_*\)&lt;/span&gt; and optimal policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_*\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;iterative-policy-evaluation&quot;&gt;Iterative Policy Evaluation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: evaluate a given policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: iterative application of Bellman expectation backup &lt;span class=&quot;math display&quot;&gt;\[
v_1\to v_2 \to ... \to v_\pi
\]&lt;/span&gt; Using &lt;em&gt;synchronous&lt;/em&gt; backups,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At each iteration &lt;span class=&quot;math inline&quot;&gt;\(k+1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;For all state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s \in \mathcal S\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update &lt;span class=&quot;math inline&quot;&gt;\(v_{k+1}(\mathcal s)\)&lt;/span&gt; from &lt;span class=&quot;math inline&quot;&gt;\(v_k(\mathcal s&amp;#39;)\)&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s&amp;#39;\)&lt;/span&gt; is a successor state of &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}(\mathcal s)=\sum_{a\in\mathcal A} \pi(a|\mathcal s)\left(\mathcal R_{\mathcal s}^a+\gamma\sum_{\mathcal s&amp;#39;\in\mathcal S}\mathcal P^a_{\mathcal s\mathcal s&amp;#39;}v_k(\mathcal s&amp;#39;)\right)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\mathbf{v^{k+1}}=\mathbf{\mathcal R^\pi} + \gamma\mathbf{\mathcal P^\pi v^k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;how-to-improve-a-policy&quot;&gt;How to Improve a Policy&lt;/h2&gt;
&lt;p&gt;Given a policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;, &lt;strong&gt;evaluate&lt;/strong&gt; the policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
v_\pi(s)=\Bbb E[R_{t+1}+\gamma R_{t+2}+...|S_t=s]
\]&lt;/span&gt; &lt;strong&gt;Improve&lt;/strong&gt; the policy by acting greedily with respect to &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\pi &amp;#39;=greedy(v_\pi)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/41.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;principle-of-optimality&quot;&gt;Principle of Optimality&lt;/h2&gt;
&lt;p&gt;Any optimal policy can be subdivided into two components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An optimal first Action &lt;span class=&quot;math inline&quot;&gt;\(A_*\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Followed by an optimal policy from successor state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{S&amp;#39;}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A policy &lt;span class=&quot;math inline&quot;&gt;\(\pi(a|s)\)&lt;/span&gt; achieves the optimal value from state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)=v_*(s)\)&lt;/span&gt;, if and only if &lt;strong&gt;For any state &lt;span class=&quot;math inline&quot;&gt;\(s&amp;#39;\)&lt;/span&gt; reachable from &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; achieves the optimal value from state &lt;span class=&quot;math inline&quot;&gt;\(s&amp;#39;\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s&amp;#39;)=v_*(s&amp;#39;)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;deterministic-value-iteration&quot;&gt;Deterministic Value Iteration&lt;/h2&gt;
&lt;p&gt;If we know the solution to subproblems &lt;span class=&quot;math inline&quot;&gt;\(v_*(s&amp;#39;)\)&lt;/span&gt;. Then solution &lt;span class=&quot;math inline&quot;&gt;\(v_*(s)\)&lt;/span&gt; can be found by one-step lookahead: &lt;span class=&quot;math display&quot;&gt;\[
v_*(s)\leftarrow\max_{a\in\mathcal A}\mathcal R_s^a+\gamma\sum_{s&amp;#39;\in\mathcal S}v_*(s&amp;#39;)
\]&lt;/span&gt; The idea of value iteration is to apply these updates iteratively.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt;: start with final rewards and word backwards. &lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}(s)=\max_{a\in\mathcal A}\left(\mathcal R_s^a+\gamma\sum_{s&amp;#39;\in\mathcal S}\mathcal R_{ss&amp;#39;}^av_k(s&amp;#39;)\right)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}=\max_{a\in\mathcal A}\mathcal R^a+\gamma\mathcal P^av_k
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;synchronous-dynamic-programming-algorithms&quot;&gt;Synchronous Dynamic Programming Algorithms&lt;/h2&gt;
&lt;table style=&quot;width:50%;&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 11%&quot; /&gt;
&lt;col style=&quot;width: 23%&quot; /&gt;
&lt;col style=&quot;width: 15%&quot; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Problem&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Bellman Equation&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Algorithm&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Prediction&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Expectation Equation&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Iterative Policy Evaluation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Control&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Expectation Equation + Greedy Policy Improvement&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Policy Iteration&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Control&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Optimally Equation&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Value Iteration&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Algorithms are based on state-value function &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(v_*(s)\)&lt;/span&gt;. Complexity &lt;span class=&quot;math inline&quot;&gt;\(O(mn^2)\)&lt;/span&gt; per iteration for &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; actions and &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; states.&lt;/li&gt;
&lt;li&gt;Could also apply to action-value function &lt;span class=&quot;math inline&quot;&gt;\(q_\pi(s,a)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(q_*(s,a)\)&lt;/span&gt;. Complexity &lt;span class=&quot;math inline&quot;&gt;\(O(m^2n^2)\)&lt;/span&gt; per iteration.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;asynchronous-dynamic-programming&quot;&gt;Asynchronous Dynamic Programming&lt;/h2&gt;
&lt;p&gt;Asynchronous DP backs up states individually in any order. For each selected state, apply the appropriate backup. Can significantly reduce computation. Guaranteed to converge if all states continue to be selected.&lt;/p&gt;
&lt;h3 id=&quot;in-place-dynamic-programming&quot;&gt;In-Place Dynamic Programming&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Synchronous value iteration&lt;/strong&gt; stores two copies of value function for all &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; in &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{S}\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
v_{new}(s) &amp;amp;\leftarrow \max_{a\in{\mathcal{A}}}\left(\mathcal{R}_s^a+
\gamma\sum_{s&amp;#39;\in{\mathcal{S}}}\mathcal{P}^a_{ss&amp;#39;}v_{old}(s&amp;#39;)
\right) \\
v_{old} &amp;amp;\leftarrow v_{new}
\end{align}
\]&lt;/span&gt; &lt;strong&gt;In-place value iteration&lt;/strong&gt; only stores one copy of value function for all &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; in &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{S}\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v(s) \leftarrow \max_{a\in\mathcal{A}}\left(
\mathcal{R}_s^a+\gamma\sum_{s&amp;#39;\in\mathcal{S}}\mathcal{P}_{ss&amp;#39;}^av(s&amp;#39;)
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;prioritised-sweeping&quot;&gt;Prioritised Sweeping&lt;/h3&gt;
&lt;p&gt;Use magnitude of Bellman error to guide state selection, e.g. &lt;span class=&quot;math display&quot;&gt;\[
\left|
\max_{a\in\mathcal{A}}
\left(
\mathcal{R}_s^a + \gamma\sum_{s&amp;#39;\in\mathcal{S}}\mathcal{P}_{ss&amp;#39;}^av(s&amp;#39;)
\right) - v(s)
\right|
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backup the state with the largest remaining Bellman error&lt;/li&gt;
&lt;li&gt;Update Bellman error of affected states after each backup&lt;/li&gt;
&lt;li&gt;Requires knowledge of reverse dynamics (predecessor states)&lt;/li&gt;
&lt;li&gt;Can be implemented efficiently by maintaining a priority queue&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;real-time-dynamic-programming&quot;&gt;Real-Time Dynamic Programming&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;: only states that are relevant to agent. Use agent’s experience to guide the selection of states. After each time-step &lt;span class=&quot;math inline&quot;&gt;\(S_t, A_t, R_{t+1}\)&lt;/span&gt;, backup the state &lt;span class=&quot;math inline&quot;&gt;\(S_t\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
v(S_t) \leftarrow \max_{a\in\mathcal{A}} \left(
\mathcal{R}_{S_t}^a + \gamma\sum_{s&amp;#39;\in\mathcal{S}}
\mathcal{P}_{S_ts&amp;#39;}^av(s&amp;#39;)
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;full-width-and-sample-backups&quot;&gt;Full-width and sample backups&lt;/h2&gt;
&lt;h3 id=&quot;full-width-backups&quot;&gt;Full-Width Backups&lt;/h3&gt;
&lt;p&gt;DP uses &lt;em&gt;full-width&lt;/em&gt; backups. For each backup (sync or async):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Every successor state and action is considered&lt;/li&gt;
&lt;li&gt;Using knowledge of the MDP transitions and reward function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DP is effective for medium-sized problems (millions of states). For large problems DP suffers Bellman’s &lt;em&gt;curse of dimensionality&lt;/em&gt; (Number of states &lt;span class=&quot;math inline&quot;&gt;\(n=|\mathcal{S}|\)&lt;/span&gt; grows exponentially with number of state variables). Even one backup can be too expensive.&lt;/p&gt;
&lt;h3 id=&quot;sample-backups&quot;&gt;Sample Backups&lt;/h3&gt;
&lt;p&gt;Using sample rewards and sample transitions &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal{S,A,R,S&amp;#39;}\right&amp;gt;\)&lt;/span&gt; instead of reward function &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{R}\)&lt;/span&gt; and transition dynamics &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{P}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-free: no advance knowledge of MDP required&lt;/li&gt;
&lt;li&gt;Breaks the curse of dimensionality through sampling&lt;/li&gt;
&lt;li&gt;Cost of backup is constant, independent of &lt;span class=&quot;math inline&quot;&gt;\(n=|\mathcal{S}|\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 17 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/17/dynamic-programming.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/17/dynamic-programming.html</guid>
			</item>
		
			<item>
				<title>Recurrent Neural Networks with External Memory</title>
				<description>&lt;p&gt;&lt;a class=&quot;btn btn-default&quot; href=&quot;https://github.com/npow/RNN-EM&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;external-memory-read&quot;&gt;External memory read&lt;/h2&gt;
&lt;p&gt;RNN-EM has an external memory &lt;span class=&quot;math inline&quot;&gt;\(M_t \in R^{m\times n}\)&lt;/span&gt;. It can be considered as a memory with &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; slots and each slot is a vector with &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; elements. Similar to the external memory in computers, the memory capacity of RNN-EM may be increased if using a large &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The model generates a key vector &lt;span class=&quot;math inline&quot;&gt;\(k_t\)&lt;/span&gt; to search for content in the external memory. Though there are many possible ways to generate the key vector, we choose a simple linear function that relates hidden layer activity &lt;span class=&quot;math inline&quot;&gt;\(h_t\)&lt;/span&gt; as follows &lt;span class=&quot;math display&quot;&gt;\[
k_t=W_kh_t
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(W_k\in R^{m\times p}\)&lt;/span&gt; is a linear transformation matrix. Our intuition is that the memory should be in the same space of or affine to the hidden layer activity.&lt;/p&gt;
&lt;p&gt;We use cosine distance &lt;span class=&quot;math inline&quot;&gt;\(K(u,v)={u\cdot v \over ||u||||v||}\)&lt;/span&gt; to compare this key vector with contents in the external memory. The weight for the &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt;-th slot &lt;span class=&quot;math inline&quot;&gt;\(M_t(:,c)\)&lt;/span&gt; in memory &lt;span class=&quot;math inline&quot;&gt;\(M_t\)&lt;/span&gt; is computed as follows &lt;span class=&quot;math display&quot;&gt;\[
\hat w_t(c)=\frac{exp\beta_tK(k_t,M_t(:,c))}{\sum_qexp\beta_tK(k_t,M_t(:,q))}
\]&lt;/span&gt; where the above weight is normalized and sums to 1.0. &lt;span class=&quot;math inline&quot;&gt;\(\beta_t\)&lt;/span&gt; is a scalar larger than 0.0. It sharpens the weight vector when &lt;span class=&quot;math inline&quot;&gt;\(\beta_t\)&lt;/span&gt; is larger than 1.0. Conversely, it smooths or dampens the weight vector when &lt;span class=&quot;math inline&quot;&gt;\(\beta_t\)&lt;/span&gt; is between 0.0 and 1.0. We use the following function to obtain &lt;span class=&quot;math inline&quot;&gt;\(\beta_t\)&lt;/span&gt;; &lt;span class=&quot;math display&quot;&gt;\[
\beta_t=log(1+exp(W_\beta h_t))
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(W_\beta\in R^{1\times p}\)&lt;/span&gt; maps the hidden layer activity &lt;span class=&quot;math inline&quot;&gt;\(h_t\)&lt;/span&gt; to a scalar.&lt;/p&gt;
&lt;p&gt;Importantly, we also use a scalar coefficient &lt;span class=&quot;math inline&quot;&gt;\(g_t\)&lt;/span&gt; to interpolate the above weight estimate with the past weight as follows &lt;span class=&quot;math display&quot;&gt;\[
w_t=(1-g_t)w_{t-1}+g_t\hat w_t
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The memory content is retrieved from the external memory at time &lt;span class=&quot;math inline&quot;&gt;\(t-1\)&lt;/span&gt; using &lt;span class=&quot;math display&quot;&gt;\[
c_t=M_{t-1}w_{t-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;external-memory-update&quot;&gt;External memory update&lt;/h2&gt;
&lt;p&gt;RNN-EM generates a new content vector &lt;span class=&quot;math inline&quot;&gt;\(v_t\)&lt;/span&gt; to be added to its memory &lt;span class=&quot;math display&quot;&gt;\[
v_t=W_vh_t
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(W_v\in R^{m\times p}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;RNN-EM has a forget gate as follows &lt;span class=&quot;math display&quot;&gt;\[
f_t=1-w_t\odot e_t
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(e_t\in R^{n\times 1}\)&lt;/span&gt; is an erase vector, generated as &lt;span class=&quot;math inline&quot;&gt;\(e_t=\sigma(W_{he}h_t)\)&lt;/span&gt;. Notice that the &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt;-th element in the forget gate is zero only if both read weight &lt;span class=&quot;math inline&quot;&gt;\(w_t\)&lt;/span&gt; and erase vector &lt;span class=&quot;math inline&quot;&gt;\(e_t\)&lt;/span&gt; have their &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt;-th element set to one. Therefore, memory cannot be forgotten if it is not to be read.&lt;/p&gt;
&lt;p&gt;RNN-EM has an update gate &lt;span class=&quot;math inline&quot;&gt;\(u_t\)&lt;/span&gt;. It simply uses the weight &lt;span class=&quot;math inline&quot;&gt;\(w_t\)&lt;/span&gt; as follows &lt;span class=&quot;math display&quot;&gt;\[
u_t=w_t
\]&lt;/span&gt; Therefore, memory is only updated if it is to be read.&lt;/p&gt;
&lt;p&gt;With the above described two gates, the memory is updated as follows &lt;span class=&quot;math display&quot;&gt;\[
M_t=diag(f_t)M_{t-1}+diag(u_t)v_t
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(diag(\cdot)\)&lt;/span&gt; transforms a vector to a diagonal matrix with diagonal elements from the vector.&lt;/p&gt;
&lt;p&gt;Notice that when the number of memory slots is small, it may have similar performances as a geted RNN.&lt;/p&gt;
</description>
				<pubDate>Tue, 03 May 2016 00:00:00 +0800</pubDate>
				<link>/deep%20learning/rnn/2016/05/03/rnn-em.html</link>
				<guid isPermaLink="true">/deep%20learning/rnn/2016/05/03/rnn-em.html</guid>
			</item>
		
			<item>
				<title>Markov Decision Processes</title>
				<description>&lt;h2 id=&quot;rewards&quot;&gt;Rewards&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;reward&lt;/code&gt; &lt;span class=&quot;math inline&quot;&gt;\(R_t\)&lt;/span&gt; is a scalar feedback signal&lt;/li&gt;
&lt;li&gt;Indecates how well agent is doing at step &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The agent’s job is to maximise cumulative reward&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reinforcement learning is based on the &lt;code&gt;reward hypothesis&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All goals can be described by the maximisation of expected cumulative reward&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;information-statea.k.a-markov-state&quot;&gt;Information State(a.k.a Markov state)&lt;/h2&gt;
&lt;p&gt;A state &lt;span class=&quot;math inline&quot;&gt;\(S_t\)&lt;/span&gt; is &lt;code&gt;Markov&lt;/code&gt; if and only if &lt;span class=&quot;math display&quot;&gt;\[
\Bbb P[S_{t+1}|S_t]=\Bbb P[S_{t+1}|S_1,...,S_t]
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The future is independent of the past given the present&lt;/li&gt;
&lt;li&gt;Once the state is known, the history may be thrown away&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;markov-decision-processes&quot;&gt;Markov Decision Processes&lt;/h2&gt;
&lt;h3 id=&quot;markov-processes&quot;&gt;Markov Processes&lt;/h3&gt;
&lt;h4 id=&quot;state-transition-matrix&quot;&gt;State Transition Matrix&lt;/h4&gt;
&lt;p&gt;For a Markov state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt; and successor state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s&amp;#39;\)&lt;/span&gt;, the &lt;em&gt;state transition probability&lt;/em&gt; is defined by &lt;span class=&quot;math display&quot;&gt;\[
\mathcal P_{\mathcal s\mathcal s&amp;#39;}=\Bbb P[\mathcal S_{t+1}=\mathcal s&amp;#39;|\mathcal S_t=\mathcal s]
\]&lt;/span&gt; State transition matrix &lt;span class=&quot;math inline&quot;&gt;\(\mathcal P\)&lt;/span&gt; defines transition probabilities from all states &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; to all successor states &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s&amp;#39;\)&lt;/span&gt;, &lt;span class=&quot;math display&quot;&gt;\[
\mathcal P=\text{from}
\begin{array}{c}
\text{to} \\
\begin{bmatrix}
\mathcal P_{11} &amp;amp; \cdots &amp;amp; \mathcal P_{1n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\mathcal P_{n1} &amp;amp; \cdots &amp;amp; \mathcal P_{nn}
\end{bmatrix}
\end{array}
\]&lt;/span&gt; where each row of the matrix sums to 1.&lt;/p&gt;
&lt;h4 id=&quot;markov-process&quot;&gt;Markov Process&lt;/h4&gt;
&lt;p&gt;A Markov process is a memoryless random process, i.e. a sequence of random states &lt;span class=&quot;math inline&quot;&gt;\(\mathcal S_1,\mathcal S_2,...\)&lt;/span&gt; with the Markov property.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;Markov Process(or Markov Chain)&lt;/em&gt; is a tuple &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal P\right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal S\)&lt;/span&gt; is a (finite) set of states&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal P\)&lt;/span&gt; is state transition probability matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;markov-reward-process&quot;&gt;Markov Reward Process&lt;/h3&gt;
&lt;p&gt;A Markov reward process is a Markov chain with values.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;Markov Reward Process&lt;/em&gt; is a tuple &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal P,\mathcal R,\gamma\right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal S\)&lt;/span&gt; is a finite set of states&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal P\)&lt;/span&gt; is a state transition probability matrix&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal R\)&lt;/span&gt; is a reward function, &lt;span class=&quot;math inline&quot;&gt;\(\mathcal R_s=\Bbb E[R_{t+1}|\mathcal S_t=\mathcal s]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\gamma\)&lt;/span&gt; is a discount factor, &lt;span class=&quot;math inline&quot;&gt;\(\gamma \in [0,1]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;return&quot;&gt;Return&lt;/h4&gt;
&lt;p&gt;The return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt;(Goal) is the total discounted reward from time-step &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt;. &lt;span class=&quot;math display&quot;&gt;\[
G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^kR_{t+k+1}
\]&lt;/span&gt; The discount &lt;span class=&quot;math inline&quot;&gt;\(\gamma\in[0,1]\)&lt;/span&gt; is the present value of future rewords.&lt;/p&gt;
&lt;h4 id=&quot;value-function&quot;&gt;Value Function&lt;/h4&gt;
&lt;p&gt;The value function &lt;span class=&quot;math inline&quot;&gt;\(v(s)\)&lt;/span&gt; gives the long-term value of state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;state value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(v(s)\)&lt;/span&gt; of an MRP is the expected &lt;code&gt;Return&lt;/code&gt; starting from state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v(s)=\Bbb E[G_t|\mathcal S_t=\mathcal s]
\]&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/39.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/38.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;The reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;bellman-equation-for-mrps&quot;&gt;Bellman Equation for MRPs&lt;/h4&gt;
&lt;p&gt;The value function can be decomposed into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;immediate reward &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;discounted value of successor state &lt;span class=&quot;math inline&quot;&gt;\(\gamma v(S_{t+1})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
v(s) &amp;amp;= \Bbb E[G_t|S_t=s] \\
&amp;amp;= \Bbb E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...|S_t=s] \\
&amp;amp;= \Bbb E[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+...)|S_t=s] \\
&amp;amp;= \Bbb E[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
&amp;amp;= \Bbb E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]
\end{align}
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v(s)=\mathcal R_s+\gamma \sum_{s&amp;#39;\in\mathcal S} \mathcal P_{ss&amp;#39;}v(s&amp;#39;)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;bellman-equation-in-matrix-form&quot;&gt;Bellman Equation in Matrix Form&lt;/h4&gt;
&lt;p&gt;The Bellman equation can be expressed concisely using matrices, &lt;span class=&quot;math display&quot;&gt;\[
v=\mathcal R+\gamma \mathcal Pv
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(v\)&lt;/span&gt; is a column vector with one entry per state &lt;span class=&quot;math display&quot;&gt;\[
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
=
\begin{bmatrix}
\mathcal R_1 \\
\vdots \\
\mathcal R_n
\end{bmatrix}
+\gamma \begin{bmatrix}
\mathcal P_{11} &amp;amp; \cdots &amp;amp; \mathcal P_{1n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\mathcal P_{n1} &amp;amp; \cdots &amp;amp; \mathcal P_{nn}
\end{bmatrix}
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;sloving-the-bellman-equation&quot;&gt;Sloving the Bellman Equation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The Bellman equation is a linear equation&lt;/li&gt;
&lt;li&gt;It can be solved directly: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
v &amp;amp;= \mathcal R+\gamma\mathcal Pv \\
(I-\gamma\mathcal P)v &amp;amp;=\mathcal R \\
v &amp;amp;= (I-\gamma\mathcal P)^{-1}\mathcal R
\end{align}
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Computational complexity is &lt;span class=&quot;math inline&quot;&gt;\(O(n^3)\)&lt;/span&gt; for &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; states&lt;/li&gt;
&lt;li&gt;Direct solution only possible for small MRPs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;markov-decision-process&quot;&gt;Markov Decision Process&lt;/h3&gt;
&lt;p&gt;A Markov decision process(MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.&lt;/p&gt;
&lt;p&gt;A Markov Decision Process is a tuple &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma\right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal A\)&lt;/span&gt; is a finite set of actions&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/40.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h4 id=&quot;policies&quot;&gt;Policies&lt;/h4&gt;
&lt;p&gt;A policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; is a distribution over actions given states &lt;span class=&quot;math display&quot;&gt;\[
\pi(a|s)=\Bbb P[A_t=a|S_t=s]
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A policy fully defines the behaviour of an agent&lt;/li&gt;
&lt;li&gt;MDP policies depend on the current state (not the history)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;value-function-1&quot;&gt;Value Function&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;state-value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt; of an MDP is the expected return starting from state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, and then following policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v_\pi(s)=\Bbb E_\pi[G_t|S_t=s]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;action-value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(q_\pi(s,a)\)&lt;/span&gt; is the expected return starting from state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, taking action &lt;span class=&quot;math inline&quot;&gt;\(a\)&lt;/span&gt;, and then following policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
q_\pi(s,a)=\Bbb E_\pi[G_t|S_t=s,A_t=a]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;bellman-expectation-equation&quot;&gt;Bellman Expectation Equation&lt;/h4&gt;
&lt;p&gt;The state-value function can again be decomposed into immediate reward plus discounted value of successor state &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
v_\pi(s) &amp;amp;= \Bbb E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s] \\
&amp;amp;=\sum_{a\in\mathcal A} \pi(a|s)q_\pi(s,a)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The action-value function can similarly be decomposed, &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
q_\pi(s,a) &amp;amp;= \Bbb E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a] \\
&amp;amp;= \mathcal R_s^a + \gamma\sum_{s&amp;#39;\in\mathcal S} \mathcal P_{ss&amp;#39;}^a v_\pi(s&amp;#39;)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;bellman-expectation-equation-matrix-form&quot;&gt;Bellman Expectation Equation (Matrix Form)&lt;/h4&gt;
&lt;p&gt;The Bellman expectation equation can be expressed concisely using the induced MRP, &lt;span class=&quot;math display&quot;&gt;\[
v_\pi=\mathcal R^\pi + \gamma \mathcal P^\pi v_\pi
\]&lt;/span&gt; with direct solution &lt;span class=&quot;math display&quot;&gt;\[
v_\pi=(I-\gamma\mathcal P^\pi)^{-1}\mathcal R^\pi
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;optimal-value-function&quot;&gt;Optimal Value Function&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;optimal state-value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(v_*(s)\)&lt;/span&gt; is the maximum value function over all policies &lt;span class=&quot;math display&quot;&gt;\[
v_*(s)=\max_\pi v_\pi(s)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;optimal action-value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(q_*(s,a)\)&lt;/span&gt; is the maximum action-value function over all policies &lt;span class=&quot;math display&quot;&gt;\[
q_*(s,a)=\max_\pi q_\pi(s,a)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimal value function specifies the best possible performance in the MDP.&lt;/li&gt;
&lt;li&gt;An MDP is “solved” when we know the optimal value fn.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;partially-observable-mdps-pomdps&quot;&gt;Partially Observable MDPs (POMDPs)&lt;/h3&gt;
&lt;p&gt;A Partially Observable Markov Decision Process is an MDP with hidden states. It is a hidden Markov model with actions.&lt;/p&gt;
&lt;p&gt;A POMDP is a tuple &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal O,\mathcal P,\mathcal R,\mathcal Z,\gamma\right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal O\)&lt;/span&gt; is a finite set of observations&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal Z\)&lt;/span&gt; is an observation function, &lt;span class=&quot;math display&quot;&gt;\[
\mathcal Z_{s&amp;#39;o}^a=\Bbb P[O_{t+1}=o|S_{t+1}=s&amp;#39;,A_t=a]
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 03 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/03/markov-decision-processes.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/03/markov-decision-processes.html</guid>
			</item>
		
			<item>
				<title>A Full Hardware Guide to Deep Learning</title>
				<description>&lt;p&gt;&lt;a class=&quot;btn btn-default&quot; href=&quot;http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/&quot; target=&quot;_blank&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: GTX 680 or GTX 960 (no money); GTX 980 (best performance); GTX Titan (if you need memory); GTX 970 (no convolutional nets)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Two threads per GPU; full 40 PCIe lanes and correct PCIe spec (same as your motherboard); &amp;gt; 2GHz; cache does not matter;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RAM&lt;/strong&gt;: Use asynchronous mini-batch allocation; clock rate and timings do not matter; buy at least as much CPU RAM as you have GPU RAM;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hard drive/SSD&lt;/strong&gt;: Use asynchronous batch-file reads and compress your data if you have image or sound data; a hard drive will be fine unless you work with 32 bit floating point data sets with large input dimensions&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Power supply unit(PSU)&lt;/strong&gt;: Add up watts of GPUs + CPU + (100-300) for required power; get high efficiency rating if you use large conv nets; make sure it has enough PCIe connectors (6+8pins) and watts for your (future) GPUs&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cooling&lt;/strong&gt;: Set coolbits flag in your config if you run a single GPU; otherwise flashing BIOS for increased fan speeds is easiest and cheapest; use water cooling for multiple GPUs and/or when you need to keep down the noise (you work with other people in the same room)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: Get PCIe 3.0 and as many slots as you need for your (future) GPUs (one GPU takes two slots; max 4 GPUs per system)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Monitors&lt;/strong&gt;: If you want to upgrade your system to be more productive, it might make more sense to buy an additional monitor rather than upgrading your GPU&lt;/p&gt;
</description>
				<pubDate>Wed, 13 Apr 2016 00:00:00 +0800</pubDate>
				<link>/deep%20learning/tools/2016/04/13/a-full-hardware-guide-to-deep-learning.html</link>
				<guid isPermaLink="true">/deep%20learning/tools/2016/04/13/a-full-hardware-guide-to-deep-learning.html</guid>
			</item>
		
	</channel>
</rss>
