<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>Qiang Brother Notes.</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>BURLAP Tutorial</title>
				<description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;An MDP definition does not include a specific element for specifying terminal states because they can be implicitly defined in the transition dynamics and reward function. That is, a terminal state can be encoded in an MDP by being a state in which every action causes a deterministic transition back to itself with zero reward. However, for convenience and other practical reasons, terminal states in BURLAP are specified directly with a function.&lt;/p&gt;
&lt;h2 id=&quot;java-interfaces-for-mdp-definitions&quot;&gt;Java Interfaces for MDP Definitions&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/55.png&quot; alt=&quot;Figure: UML Digram of the Java interfaces/classes for an MDP definition.&quot; /&gt;&lt;figcaption&gt;Figure: UML Digram of the Java interfaces/classes for an MDP definition.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;SADomain&lt;/strong&gt; - A data structure that stands for “sigle agent domain”. This data structure stores information about an MDP that you will define and is typically passed to different planning or learning algorithms. &lt;code&gt;Simply could be regard as a territory over which rule or control is exercised.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State&lt;/strong&gt; - Implement this interface to define the state variables of your MDP state space. An instance of this object will specify a single state from the state space.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Action&lt;/strong&gt; - Implement this interface to define a possible action that the agent can select. If your MDP action set is discrete and unparameterized, you may consider using the provided concrete implementation &lt;em&gt;SimpleAction&lt;/em&gt;, which defines an aciton entirely by a single String name.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;ActionType&lt;/strong&gt; - Implement this interface to define a kind of Java factory for generating your Actions. In particular, this interface allows you define &lt;em&gt;preconditions&lt;/em&gt; for actions. Actions with preconditions are actions that the agent can only select/execute in some states, and not others. It also allows you to specify which kinds of parameterizations of your actions are allowable in a state, if your actions are parameterized. Often, MDPs have unparameterized actions that can be executed in any state (no precondtions). In such cases, you should consider the provided concrete implementation &lt;em&gt;UniversalActionType&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;SampleModel&lt;/strong&gt; - Implement this interface to define the model of your MDP. This inferface only requires you to implement methods that can sample a transition: spit back out a possible next state and reward given a prior state and action taken. Some planning algorithms, however, require more information; they may require being able to enumerate the set of possible transitions and their probability of occurring. If you wish to support these kinds of algorithms, then you will instead want to implement the FullModel interface that extends the SampleModel interface with a method for enumerating the transition probability distribution.
&lt;ul&gt;
&lt;li&gt;Note that if you are defining a learning problem in which an agent interacts with an external environment from BURLAP, it may not be possible to define even a SampleModel. For example, if you’re going to use BURLAP to control robots via reinforcement learning, it might not be possible for you to specify a model of reality in a meanginful way (or it might simply be unncessary). In these cases, the model can be omitted from the MDP description and instead you’ll want to implement a custom Environment instance, described next.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;/strong&gt; - An MDP defines the nature of an environment, but ultimately, an agent will want to interact with an actual environment, either through learning or to execute a policy it computed from planning for the MDP. An environment has a specific state of the world that the agent can only modify by using the MDP actions. Implement this interface to provide an environment with which BURLAP agents can interact. If you defined the MDP yourself, then you’ll probably don’t want to implement Environment yourself and instead use the provided concreate SimulatedEnvironment class, which takes an SADomain with a SampleModel, and simulates an environment for it.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;EnvironmentOutcmoe&lt;/strong&gt; - A tuple that contains a prior state/observation, an action taken in that state, a reward recieved, and a next state/observation to which the environment transitioned. This object is typically returned by an Environment instance when an action is taken, or from a SampleModel when you sample a transition.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;TransitionProb&lt;/strong&gt; - A tuple containing a double and an EnvironmentOutcome object, which specifies the probability of the transition specified by EnvironmentOutcome occurring. Typically, a list of these objects is returned by a FullModel instance when querying it for the transition probability distribution.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Thu, 07 Jul 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/07/07/burlap-tutorial.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/07/07/burlap-tutorial.html</guid>
			</item>
		
			<item>
				<title>Exploration and Exploitation</title>
				<description>&lt;h2 id=&quot;regret&quot;&gt;Regret&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;action-value&lt;/em&gt; is the mean reward for action &lt;span class=&quot;math inline&quot;&gt;\(a\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
Q(a)=\Bbb E[r|a]
\]&lt;/span&gt; The &lt;em&gt;optimal value&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(V^*\)&lt;/span&gt; is &lt;span class=&quot;math display&quot;&gt;\[
V^*=Q(a^*)=\mathop{max}\limits_{a\in\mathcal A} Q(a)
\]&lt;/span&gt; The &lt;em&gt;regret&lt;/em&gt; is the opportunity loss for one step &lt;span class=&quot;math display&quot;&gt;\[
I_t=\Bbb E[V^*-Q(a_t)]
\]&lt;/span&gt; The &lt;em&gt;total regret&lt;/em&gt; is the total opportunity loss &lt;span class=&quot;math display&quot;&gt;\[
L_t=\Bbb E\left[\sum_{\tau=1}^t V^*-Q(a_\tau)\right]
\]&lt;/span&gt; Maximise cumulative reward &lt;span class=&quot;math inline&quot;&gt;\(\equiv\)&lt;/span&gt; minimise total regret.&lt;/p&gt;
</description>
				<pubDate>Mon, 04 Jul 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/07/04/exploration-and-exploitation.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/07/04/exploration-and-exploitation.html</guid>
			</item>
		
			<item>
				<title>Model-Based Reinforcement Learning</title>
				<description>&lt;h2 id=&quot;model-based-and-model-free-rl&quot;&gt;Model-Based and Model-Free RL&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Model-Free RL
&lt;ul&gt;
&lt;li&gt;No model&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Learn&lt;/code&gt; value function (and/or policy) from experience&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Model-Based RL
&lt;ul&gt;
&lt;li&gt;Learn a model from experience&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Plan&lt;/code&gt; value function (and/or policy) from model&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Dyna
&lt;ul&gt;
&lt;li&gt;Learn a model from real experience&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Learn and plan&lt;/code&gt; value function (and/or policy) from real and simulated experience&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;model-based-reinforcement-learning&quot;&gt;Model-Based Reinforcement Learning&lt;/h2&gt;
&lt;h3 id=&quot;advantages-of-model-based-rl&quot;&gt;Advantages of Model-Based RL&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Advantages:
&lt;ul&gt;
&lt;li&gt;Can efficiently learn model by supervised learning methods&lt;/li&gt;
&lt;li&gt;Can reason about model uncertainty&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Disadvantages:
&lt;ul&gt;
&lt;li&gt;First learn a model, then construct a value function (two sources of approximation error)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;what-is-a-model&quot;&gt;What is a Model?&lt;/h3&gt;
&lt;p&gt;A model &lt;span class=&quot;math inline&quot;&gt;\(\mathcal M\)&lt;/span&gt; is a representation of and MDP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal{S,A,P,R}\right&amp;gt;\)&lt;/span&gt;, parametrized by &lt;span class=&quot;math inline&quot;&gt;\(\eta\)&lt;/span&gt;. We will assume state space &lt;span class=&quot;math inline&quot;&gt;\(\mathcal S\)&lt;/span&gt; and action space &lt;span class=&quot;math inline&quot;&gt;\(\mathcal A\)&lt;/span&gt; are known. So a model &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{M=\left&amp;lt;P_\eta,R_\eta\right&amp;gt;}\)&lt;/span&gt; represents state transitions &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{P_\eta\approx P}\)&lt;/span&gt; and rewards &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{R_\eta\approx R}\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
S_{t+1} &amp;amp;\sim \mathcal P_\eta(S_{t+1}|S_t,A_t) \\
R_{t+1} &amp;amp;= \mathcal R_\eta(R_{t+1}|S_t,A_t)
\end{align}
\]&lt;/span&gt; Typically assume conditional independence between state transitions and rewards. &lt;span class=&quot;math display&quot;&gt;\[
\Bbb P[S_{t+1},R_{t+1}|S_t,A_t]=\Bbb P[S_{t+1}|S_t,A_t]\Bbb P[R_{t+1}|S_t,A_t]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;model-learning&quot;&gt;Model Learning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: estimate model &lt;span class=&quot;math inline&quot;&gt;\(\mathcal M_\eta\)&lt;/span&gt; from experience &lt;span class=&quot;math inline&quot;&gt;\(\{S_1,A_1,R_2,...,S_T\}\)&lt;/span&gt;. This is a supervised learning problem &lt;span class=&quot;math display&quot;&gt;\[
S_1,A_1 \to R_2,S_2 \\
S_2,A_2 \to R_3,S_3
\]&lt;/span&gt; Learning &lt;span class=&quot;math inline&quot;&gt;\(s,a\to r\)&lt;/span&gt; is a &lt;em&gt;regression problem&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Learning &lt;span class=&quot;math inline&quot;&gt;\(s,a\to s&amp;#39;\)&lt;/span&gt; is a &lt;em&gt;density estimation problem&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&quot;examples-of-models&quot;&gt;Examples of Models&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Table Lookup Model&lt;/li&gt;
&lt;li&gt;Linear Expectation Model&lt;/li&gt;
&lt;li&gt;Linear Gaussian Model&lt;/li&gt;
&lt;li&gt;Gaussian Process Model&lt;/li&gt;
&lt;li&gt;Deep Belief Network Model&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;planning-with-a-model&quot;&gt;Planning with a Model&lt;/h3&gt;
&lt;p&gt;Given a model &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{M_\eta=\left&amp;lt;P_\eta,R_\eta\right&amp;gt;}\)&lt;/span&gt;. Solve the MDP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal{S,A,P_\eta,R_\eta}\right&amp;gt;\)&lt;/span&gt; using favourite planning algorithm&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Value iteration&lt;/li&gt;
&lt;li&gt;Policy iteration&lt;/li&gt;
&lt;li&gt;Tree search&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;integrated-architectures-dyna&quot;&gt;Integrated Architectures: Dyna&lt;/h2&gt;
&lt;h3 id=&quot;dyna-architecture&quot;&gt;Dyna Architecture&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/53.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;dyna-q-algorithm&quot;&gt;Dyna-Q Algorithm&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/54.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;monte-carlo-search&quot;&gt;Monte-Carlo Search&lt;/h2&gt;
&lt;h3 id=&quot;simple-monte-carlo-search&quot;&gt;Simple Monte-Carlo Search&lt;/h3&gt;
&lt;p&gt;Given a model &lt;span class=&quot;math inline&quot;&gt;\(\mathcal M_v\)&lt;/span&gt; and a &lt;code&gt;simulation policy&lt;/code&gt; &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;. For each action &lt;span class=&quot;math inline&quot;&gt;\(a\in\mathcal A\)&lt;/span&gt;, simulate &lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt; episodes from current (real) state &lt;span class=&quot;math inline&quot;&gt;\(s_t\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\{\color{red}{s_t,a,}R_{t+1}^k,S_{t+1}^k,A_{t+1}^k,...,S_T^k\}_{k=1}^K\sim \mathcal M_v,\pi
\]&lt;/span&gt; Evaluate actions by mean return (&lt;code&gt;Monte-Carlo evaluation&lt;/code&gt;) &lt;span class=&quot;math display&quot;&gt;\[
Q(\color{red}{s_t,a})={1\over K}\sum_{k=1}^K G_t 
\overset{P}\to
q_\pi(s_t,a)
\]&lt;/span&gt; Select current (real) action with maximum value &lt;span class=&quot;math display&quot;&gt;\[
a_t=\mathop{argmax}\limits_{a\in\mathcal A} Q(s_t,a) \tag{*}\label{*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;monte-carlo-tree-search-evaluation&quot;&gt;Monte-Carlo Tree Search (Evaluation)&lt;/h3&gt;
&lt;p&gt;Given a model &lt;span class=&quot;math inline&quot;&gt;\(\mathcal M_v\)&lt;/span&gt;. Simulate &lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt; episodes from current state &lt;span class=&quot;math inline&quot;&gt;\(s_t\)&lt;/span&gt; using current simulation policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;. Build a search tree containing visited states and actions. &lt;code&gt;Evaluate&lt;/code&gt; states &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt; by mean return of episodes from &lt;span class=&quot;math inline&quot;&gt;\(s,a\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
Q(\color{red}{s,a})={1\over N(s,a)}\sum_{k=1}^K\sum_{u=t}^T
1(S_u,A_u=s,a)G_u\overset P\to q_\pi(s,a)
\]&lt;/span&gt; After search is finished, select current (real) action with maximum value in search tree (&lt;span class=&quot;math inline&quot;&gt;\(\ref{*}\)&lt;/span&gt;).&lt;/p&gt;
&lt;h3 id=&quot;monte-carlo-tree-search-simulation&quot;&gt;Monte-Carlo Tree Search (Simulation)&lt;/h3&gt;
&lt;p&gt;In MCTS, the simulation policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; &lt;code&gt;improves&lt;/code&gt;. Each simulation consists of two phases (in-tree, out-of-tree)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tree policy&lt;/code&gt; (improves): pick actions to maximise &lt;span class=&quot;math inline&quot;&gt;\(Q(S,A)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Default policy&lt;/code&gt; (fixed): pick actions randomly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Repeat (each simulation)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Evaluate&lt;/code&gt; states &lt;span class=&quot;math inline&quot;&gt;\(Q(S,A)\)&lt;/span&gt; by Monte-Carlo evaluation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Improve&lt;/code&gt; tree policy, e.g. by &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;-greedy(Q)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;Monte-Carlo control&lt;/code&gt; applied to &lt;code&gt;simulated experience&lt;/code&gt;. Converges on the optimal search tree, &lt;span class=&quot;math inline&quot;&gt;\(Q(S,A)\to q_*(S,A)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;td-search&quot;&gt;TD Search&lt;/h2&gt;
&lt;p&gt;Simulate episodes from the current (real) state &lt;span class=&quot;math inline&quot;&gt;\(s_t\)&lt;/span&gt;. Estimate action-value function &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt;. For each step of simulation, update action-values by Sarsa &lt;span class=&quot;math display&quot;&gt;\[
\Delta Q(S,A)=\alpha(R+\gamma Q(S&amp;#39;,A&amp;#39;)-Q(S,A))
\]&lt;/span&gt; Select actions based on action-values &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt;, e.g. &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;-greedy. May also use function approximate for &lt;span class=&quot;math inline&quot;&gt;\(Q\)&lt;/span&gt;.&lt;/p&gt;
</description>
				<pubDate>Fri, 01 Jul 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/07/01/model-based-reinforcement-learning.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/07/01/model-based-reinforcement-learning.html</guid>
			</item>
		
			<item>
				<title>Policy Gradient</title>
				<description>&lt;p&gt;Directly parametrise the &lt;strong&gt;policy&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\pi_\theta(s,a)=\Bbb P[a|s,\theta]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;advantages-of-policy-based-rl&quot;&gt;Advantages of Policy-Based RL&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Better convergence properties&lt;/li&gt;
&lt;li&gt;Effective in high-dimensional or continuous action spaces&lt;/li&gt;
&lt;li&gt;Can learn stochastic policies&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Typically converge to a local rather than global optimum&lt;/li&gt;
&lt;li&gt;Evaluating a policy is typically inefficient and high variance&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;policy-objective-functions&quot;&gt;Policy Objective Functions&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: given policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_\theta(s,a)\)&lt;/span&gt; with parameters &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;, find best &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Policy based reinforcement learning is an &lt;code&gt;optimisation&lt;/code&gt; problem, find &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt; that &lt;strong&gt;maximises&lt;/strong&gt; &lt;span class=&quot;math inline&quot;&gt;\(J(\theta)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;policy-gradient&quot;&gt;Policy Gradient&lt;/h2&gt;
&lt;p&gt;Let &lt;span class=&quot;math inline&quot;&gt;\(J(\theta)\)&lt;/span&gt; be any policy objective function. Policy gradient algorithms search for a &lt;em&gt;local&lt;/em&gt; maximum in &lt;span class=&quot;math inline&quot;&gt;\(J(\theta)\)&lt;/span&gt; by ascending the gradient of the policy, w.r.t. parameters &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\theta=\alpha\nabla_\theta J(\theta)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;score-function&quot;&gt;Score Function&lt;/h2&gt;
&lt;p&gt;Assume policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_\theta\)&lt;/span&gt; is differentiable whenever it is non-zero and we know the gradient &lt;span class=&quot;math inline&quot;&gt;\(\nabla_\theta \pi_\theta(s,a)\)&lt;/span&gt;. &lt;code&gt;Likelihood ratios&lt;/code&gt; exploit the following identity &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\nabla_\theta\pi_\theta(s,a) &amp;amp;= \pi_\theta(s,a) \frac{\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)} \\
&amp;amp;= \pi_\theta(s,a) \nabla_\theta log \pi_\theta(s,a)
\end{align}
\]&lt;/span&gt; The &lt;code&gt;score function&lt;/code&gt; is &lt;span class=&quot;math inline&quot;&gt;\(\nabla_\theta log \pi_\theta(s,a)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;policy-gradient-theorem&quot;&gt;Policy Gradient Theorem&lt;/h2&gt;
&lt;h3 id=&quot;one-step-mdps&quot;&gt;One-Step MDPs&lt;/h3&gt;
&lt;p&gt;Consider a simple class of &lt;em&gt;one-step&lt;/em&gt; MDPs: Starting in state &lt;span class=&quot;math inline&quot;&gt;\(s\sim d(s)\)&lt;/span&gt;; Terminating after one time-step with reward &lt;span class=&quot;math inline&quot;&gt;\(r=\mathcal R_{s,a}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Use likelihood ratios to compute the policy gradient &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
J(\theta) &amp;amp;= \Bbb E_{\pi_\theta}[r] \\
&amp;amp;= \sum_{\mathcal{s\in S}} d(s) \sum_{a\in\mathcal A} \pi_\theta(s,a)\mathcal R_{s,a} \\
\nabla_\theta J(\theta) &amp;amp;= \sum_{s\in\mathcal S}d(s)\sum_{a\in\mathcal A}\pi_\theta(s,a)\nabla_\theta log \pi_\theta(s,a)\mathcal R_{s,a} \\
&amp;amp;=\Bbb E_{\pi_\theta}\left[\nabla_\theta log \pi_\theta(s,a)r\right]
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For any differentiable policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_\theta(s,a)\)&lt;/span&gt;, for any of the policy objective functions &lt;span class=&quot;math inline&quot;&gt;\(J\)&lt;/span&gt;, the policy gradient is &lt;span class=&quot;math display&quot;&gt;\[
\nabla_\theta J(\theta)=\Bbb E_{\pi_\theta}
\left[
\nabla_\theta log\pi_\theta(s,a)Q^{\pi_\theta}(s,a)
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;monte-carlo-policy-gradient&quot;&gt;Monte-Carlo Policy Gradient&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Update parameters by stochastic gradient ascent&lt;/li&gt;
&lt;li&gt;Using policy gradient theorem&lt;/li&gt;
&lt;li&gt;Using return &lt;span class=&quot;math inline&quot;&gt;\(v_t\)&lt;/span&gt; as an unbiased sample of &lt;span class=&quot;math inline&quot;&gt;\(Q^{\pi_\theta}(s_t,a_t)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\theta_t=\alpha\nabla_\theta log \pi_\theta(s_t,a_t)v_t
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;actor-critic-policy-gradient&quot;&gt;Actor-Critic Policy Gradient&lt;/h2&gt;
&lt;h3 id=&quot;reducing-variance-using-a-critic&quot;&gt;Reducing Variance Using a Critic&lt;/h3&gt;
&lt;p&gt;Monte-Carlo policy gradient still has high variance. We use a &lt;code&gt;critic&lt;/code&gt; to estimate the action-value function &lt;span class=&quot;math display&quot;&gt;\[
Q_w(s,a)\approx Q^{\pi_\theta}(s,a)
\]&lt;/span&gt; Actor-Critic algorithms maintain &lt;em&gt;two&lt;/em&gt; sets of parameters&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Critic&lt;/strong&gt; Updates action-value function parameters &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor&lt;/strong&gt; Updates policy parameters &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt;, in direction suggested by critic&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actor-critic algorithms follow an &lt;em&gt;approximate&lt;/em&gt; policy gradient &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\nabla_\theta J(\theta) &amp;amp;\approx \Bbb E_{\pi_\theta}\left[
\nabla_\theta log \pi_\theta(s,a) Q_w(s,a)
\right] \\
\Delta \theta &amp;amp;= \alpha\nabla_\theta log \pi_\theta(s,a) Q_w(s,a)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;action-value-actor-critic&quot;&gt;Action-Value Actor-Critic&lt;/h3&gt;
&lt;p&gt;Simple actor-critic algorithm based on action-critic. Using linear value fn approx. &lt;span class=&quot;math inline&quot;&gt;\(Q_w(s,a)=\phi (s,a)^Tw\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Critic&lt;/strong&gt; Updates &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt; by linear TD(0)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Actor&lt;/strong&gt; Updates &lt;span class=&quot;math inline&quot;&gt;\(\theta\)&lt;/span&gt; by policy gradient&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;compatible-function-approximation&quot;&gt;Compatible Function Approximation&lt;/h3&gt;
&lt;p&gt;If the following two conditions are satisfied:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Value function approximator is compatible to the policy &lt;span class=&quot;math display&quot;&gt;\[
 \nabla_w Q_w(s,a)=\nabla_\theta log \pi_\theta(s,a)
 \]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Value function parameters &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt; minimise the mean-squared error &lt;span class=&quot;math display&quot;&gt;\[
 \varepsilon=\Bbb E_{\pi_\theta}\left[
(Q^{\pi_\theta}(s,a)-Q_w(s,a))^2
 \right]
 \]&lt;/span&gt; Then the policy gradient is exist &lt;span class=&quot;math display&quot;&gt;\[
\nabla_\theta J(\theta)=\Bbb E_{\pi_\theta}
\left[
\nabla_\theta log \pi_\theta(s,a) Q_w(s,a)
\right]
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;reducing-variance-using-a-baseline&quot;&gt;Reducing Variance Using a Baseline&lt;/h3&gt;
&lt;p&gt;We subtract a baseline function &lt;span class=&quot;math inline&quot;&gt;\(B(s)\)&lt;/span&gt; from the policy gradient. This can reduce variance, without changing expectation &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\Bbb E_{\pi_\theta}[\nabla_\theta log \pi_\theta(s,a)B(s)] &amp;amp;= 
\sum_{s\in\mathcal S} d^{\pi_\theta}(s) \sum_a \nabla_\theta \pi_\theta(s,a)B(s) \\
&amp;amp;=\sum_{s\in\mathcal S} d^{\pi_\theta}B(s)\nabla_\theta \sum_{a\in\mathcal A} \pi_\theta(s,a) \\
&amp;amp;=0
\end{align}
\]&lt;/span&gt; A good baseline is the state value function &lt;span class=&quot;math inline&quot;&gt;\(B(s)=V^{\pi_\theta}(s)\)&lt;/span&gt;. So we can rewrite the policy gradient using the &lt;code&gt;advantage function&lt;/code&gt; &lt;span class=&quot;math inline&quot;&gt;\(A^{\pi_\theta}(s,a)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
A^{\pi_\theta}(s,a) &amp;amp;= Q^{\pi_\theta}(s,a)-V^{\pi_\theta}(s) \\
\nabla_\theta J(\theta) &amp;amp;= \color{red}{
    \Bbb E_{\pi_\theta}[\nabla_\theta log \pi_\theta(s,a)A^{\pi_\theta}(s,a)]
}
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;summary-of-policy-gradient-algorithms&quot;&gt;Summary of Policy Gradient Algorithms&lt;/h2&gt;
&lt;p&gt;The &lt;em&gt;policy gradient&lt;/em&gt; has many equivalent forms &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\nabla_\theta J(\theta) &amp;amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{v_t}] \tag{REINFORCE} \\
&amp;amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{Q^w(s,a)}] \tag{Q Actor-Critic} \\
&amp;amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{A^w(s,a)}] \tag{Advantage Actor-Critic} \\
&amp;amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{\delta}] \tag{TD Actor-Critic} \\
&amp;amp;= \Bbb E_{\pi\theta}[\nabla_\theta log \pi_\theta(s,a)\color{red}{\delta e}] \tag{TD($\lambda$) Actor-Critic} \\
G_\theta^{-1}\nabla_\theta J(\theta) &amp;amp;= w \tag{Natural Actor-Critic}
\end{align}
\]&lt;/span&gt; Each leads a stochastic gradient ascent algorithm. Critic uses &lt;code&gt;policy evaluation&lt;/code&gt; (e.g. MC or TD learning) to estimate &lt;span class=&quot;math inline&quot;&gt;\(Q^\pi(s,a)\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(A^\pi(s,a)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(V^\pi(s)\)&lt;/span&gt;.&lt;/p&gt;
</description>
				<pubDate>Thu, 23 Jun 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/06/23/policy-gradient.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/06/23/policy-gradient.html</guid>
			</item>
		
			<item>
				<title>Value Function Approximation</title>
				<description>&lt;h2 id=&quot;incremental-methods&quot;&gt;Incremental Methods&lt;/h2&gt;
&lt;h3 id=&quot;types-of-value-function-approximation&quot;&gt;Types of Value Function Approximation&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/48.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;value-function-approx.-by-stochastic-gradient-descent&quot;&gt;Value Function Approx. By Stochastic Gradient Descent&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: find parameter vector &lt;span class=&quot;math inline&quot;&gt;\(\mathbf w\)&lt;/span&gt; minimising mean-squared error between approximate value fn &lt;span class=&quot;math inline&quot;&gt;\(\hat v(s,\mathbf w)\)&lt;/span&gt; and true value fn &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
J(\mathbf w)=\mathbb E_\pi\left[
\left(
v_\pi(S)-\hat v(S,\mathbf w)
\right)^2
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;feature-vectors&quot;&gt;Feature Vectors&lt;/h3&gt;
&lt;p&gt;Represent state by a &lt;em&gt;feature vector&lt;/em&gt; &lt;span class=&quot;math display&quot;&gt;\[
\mathbf x(S)=\begin{bmatrix}
x_1(S) \\
\vdots \\
x_n(S)
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;linear-value-function-approximation&quot;&gt;Linear Value Function Approximation&lt;/h3&gt;
&lt;p&gt;Represent value function by a linear combination of features &lt;span class=&quot;math display&quot;&gt;\[
\hat v(S,\mathbf w)=\mathbf x(S)^T \mathbf w
=\sum_{j=1}^n \mathbf x_j(S)\mathbf w_j
\]&lt;/span&gt; Objective function is quadratic in parameters &lt;span class=&quot;math inline&quot;&gt;\(\mathbf w\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
J(\mathbf w)=\mathbb E_\pi\left[
\left(
v_\pi(S)- \mathbf x(S)^T \mathbf w
\right)^2
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;incremental-prediction-algorithms&quot;&gt;Incremental Prediction Algorithms&lt;/h3&gt;
&lt;p&gt;In practice, we substitute a &lt;em&gt;target&lt;/em&gt; for &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt;. For MC, the target is the return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\mathbf w=\alpha(\color{red}{G_t}-\hat v(S_t,\mathbf w))
\nabla_{\mathbf w}\hat v(S_t,\mathbf w)
\]&lt;/span&gt; For TD(0) the target is the TD target &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma\hat v(S_{t+1},\mathbf w)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\mathbf w=\alpha
(\color{red}{R_{t+1}+\gamma\hat v(S_{t+1},\mathbf w)}-\hat v(S_t,\mathbf w))
\nabla_{\mathbf w}\hat v(S_t,\mathbf w)
\]&lt;/span&gt; For TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;), the target is the &lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;-return &lt;span class=&quot;math inline&quot;&gt;\(G_t^\lambda\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\Delta\mathbf w=\alpha(\color{red}{G_t^\lambda}-\hat v(S_t,\mathbf w))
\nabla_{\mathbf w}\hat v(S_t,\mathbf w)
\]&lt;/span&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Monte-Carlo evaluation converges to a local optimum, even when using non-linear value function approximation. Linear TD(0) converges (close) to global optimum.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;action-value-function-approximation&quot;&gt;Action-Value Function Approximation&lt;/h3&gt;
&lt;p&gt;Approximate the action-value function &lt;span class=&quot;math display&quot;&gt;\[
\hat q(S,A,\mathbf w) \approx q_\pi(S,A)
\]&lt;/span&gt; Minimise mean-squared error between approximate action-value fn &lt;span class=&quot;math inline&quot;&gt;\(\hat q(S,A,\mathbf w)\)&lt;/span&gt; and true action-value fn &lt;span class=&quot;math inline&quot;&gt;\(q_\pi(S,A)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
J(\mathbf w)=\Bbb E_\pi\left[
\left(
q_\pi(S,A)-\hat q(S,A,\mathbf w)
\right)^2
\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;convergence-of-prediction-algorithms&quot;&gt;Convergence of Prediction Algorithms&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/49.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;gradient-temporal-difference-learning&quot;&gt;Gradient Temporal-Difference Learning&lt;/h3&gt;
&lt;p&gt;TD does not follow the gradient of &lt;em&gt;any&lt;/em&gt; objective function, this is why TD can diverge when off-policy or using non-linear function approximation.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Gradient TD&lt;/code&gt; follows true gradient of projected Bellman error&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/50.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;convergence-of-control-algorithms&quot;&gt;Convergence of Control Algorithms&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/51.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;batch-reinforcement-learning&quot;&gt;Batch Reinforcement Learning&lt;/h2&gt;
&lt;h3 id=&quot;least-squares-prediction&quot;&gt;Least Squares Prediction&lt;/h3&gt;
&lt;p&gt;Give value function approximation &lt;span class=&quot;math inline&quot;&gt;\(\hat v(s,\mathbf w)\approx v_\pi(s)\)&lt;/span&gt;, and experience &lt;span class=&quot;math inline&quot;&gt;\(\mathcal D\)&lt;/span&gt; consisting of &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;state, value \right&amp;gt;\)&lt;/span&gt; pairs &lt;span class=&quot;math display&quot;&gt;\[
\mathcal D=\left\{
\left&amp;lt;s_1,v_1^\pi \right&amp;gt;,
\left&amp;lt;s_2,v_2^\pi \right&amp;gt;,
...,
\left&amp;lt;s_T,v_T^\pi \right&amp;gt;,
\right\}
\]&lt;/span&gt; &lt;strong&gt;Least squared&lt;/strong&gt; algorithms find parameter vector &lt;span class=&quot;math inline&quot;&gt;\(\mathbf w\)&lt;/span&gt; minimising sum-squared error between &lt;span class=&quot;math inline&quot;&gt;\(\hat v(s_t,\mathbf w)\)&lt;/span&gt; and target values &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
LS(\mathbf w) &amp;amp;= \sum_{t=1}^T (v_t^\pi-\hat v(s_t,\mathbf w))^2 \\
&amp;amp;= \Bbb E_{\mathcal D} \left[(v_t^\pi-\hat v(s_t,\mathbf w))^2\right]
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;stochastic-gradient-descent-with-experience-replay&quot;&gt;Stochastic Gradient Descent with Experience Replay&lt;/h3&gt;
&lt;p&gt;Given experience consisting of &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;state, value\right&amp;gt;\)&lt;/span&gt; pairs &lt;span class=&quot;math display&quot;&gt;\[
\mathcal D=\left\{
\left&amp;lt;s_1,v_1^\pi \right&amp;gt;,
\left&amp;lt;s_2,v_2^\pi \right&amp;gt;,
...,
\left&amp;lt;s_T,v_T^\pi \right&amp;gt;,
\right\}
\]&lt;/span&gt; Repeat:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sample state, value from experience &lt;span class=&quot;math display&quot;&gt;\[
 \left&amp;lt;s, v^\pi\right&amp;gt;\sim \mathcal D
 \]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Apply stochastic gradient descent update &lt;span class=&quot;math display&quot;&gt;\[
 \Delta\mathbf w=\alpha(v^\pi-\hat v(s,\mathbf w)) \nabla_{\mathbf w}
 \hat v(s, \mathbf w)
 \]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Converges to least squares solution &lt;span class=&quot;math display&quot;&gt;\[
\mathbf w^\pi=\mathop{\text{argmin}}\limits_{\mathbf w} LS(\mathbf w)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;linear-least-squares-prediction&quot;&gt;Linear Least Squares Prediction&lt;/h3&gt;
&lt;p&gt;At minimum of &lt;span class=&quot;math inline&quot;&gt;\(LS(\mathbf w)\)&lt;/span&gt;, the expected update must be zero &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\Bbb E_{\mathcal D}[\Delta\mathbf w] &amp;amp;= 0 \\
\alpha\sum_{t=1}^T \mathbf x(s_t)(v_t^\pi-\mathbf x(s_t)^T\mathbf w) &amp;amp;= 0 \\
\sum_{t=1}^T\mathbf x(s_t)v_t^\pi &amp;amp;= \sum_{t=1}^T \mathbf x(s_t)\mathbf x(s_t)^T\mathbf w \\
\mathbf w &amp;amp;= \left(
\sum_{t=1}^T\mathbf x(s_t)\mathbf x(s_t)^T
\right)^{-1} \sum_{t=1}^T \mathbf x(s_t)v_t^\pi
\end{align}
\]&lt;/span&gt; For &lt;span class=&quot;math inline&quot;&gt;\(N\)&lt;/span&gt; features, direct solution time is &lt;span class=&quot;math inline&quot;&gt;\(O(N^3)\)&lt;/span&gt;. Incremental solution time is &lt;span class=&quot;math inline&quot;&gt;\(O(N^2)\)&lt;/span&gt; using Shermann-Morrison.&lt;/p&gt;
&lt;h3 id=&quot;linear-least-squares-prediction-algorithms&quot;&gt;Linear Least Squares Prediction Algorithms&lt;/h3&gt;
&lt;p&gt;In practice, we do not know true values &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\)&lt;/span&gt;, our “training data” muse use noisy or biased samples of &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LSMC (Least Squares Monte-Carlo) uses return &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\approx G_t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;LSTD (Least Squares Temporal-Difference) uses TD target &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\approx R_{t+1}+\gamma\hat v(S_{t+1},\mathbf w)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;LSTD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;) (Least Squares TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)) use &lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;-return &lt;span class=&quot;math inline&quot;&gt;\(v_t^\pi\approx G_t^\lambda\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;convergence-of-linear-least-squares-prediction-algorithms&quot;&gt;Convergence of Linear Least Squares Prediction Algorithms&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/52.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
</description>
				<pubDate>Sun, 19 Jun 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/06/19/value-function-approximation.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/06/19/value-function-approximation.html</guid>
			</item>
		
			<item>
				<title>Model free Control</title>
				<description>&lt;p&gt;&lt;strong&gt;Model-free prediction&lt;/strong&gt;: &lt;em&gt;Estimate&lt;/em&gt; the value function of an &lt;em&gt;unknown&lt;/em&gt; MDP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model-free Control&lt;/strong&gt;: &lt;em&gt;Optimise&lt;/em&gt; the value function of an &lt;em&gt;unknown&lt;/em&gt; MDP.&lt;/p&gt;
&lt;h2 id=&quot;on-and-off-policy-learning&quot;&gt;On and Off-Policy Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;On-policy learning
&lt;ul&gt;
&lt;li&gt;“Learn on the job”&lt;/li&gt;
&lt;li&gt;Learn about policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; from experience sampled from &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Off-policy learning
&lt;ul&gt;
&lt;li&gt;“Look over someone’s shoulder”&lt;/li&gt;
&lt;li&gt;Learn about policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; from experience sampled from &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;glie&quot;&gt;GLIE&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Greedy in the Limit with Infinite Exploration&lt;/em&gt; (GLIE).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All state-action pairs are explored infinitely many times, &lt;span class=&quot;math inline&quot;&gt;\(\lim\limits_{k\to\infty}N_k(s,a)=\infty\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The policy converges on a greedy policy, &lt;span class=&quot;math display&quot;&gt;\[
\lim\limits_{k\to\infty}\pi_k(a|s)=1(a=\mathop{\text{argmax}}\limits_{a&amp;#39;\in\mathcal{A}}Q_k(s,a&amp;#39;))
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For example, &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;-greedy is GLIE if &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt; reduces to zero at &lt;span class=&quot;math inline&quot;&gt;\(\epsilon_k={1\over k}\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;glie-monte-carlo-control&quot;&gt;GLIE Monte-Carlo Control&lt;/h2&gt;
&lt;p&gt;Sample &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt;th episode using &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;:&lt;span class=&quot;math inline&quot;&gt;\(\{\mathcal{S_1,A_1,R_2,...,S_T}\}\sim\pi\)&lt;/span&gt;, for each state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal S_t\)&lt;/span&gt; and action &lt;span class=&quot;math inline&quot;&gt;\(\mathcal A_t\)&lt;/span&gt; in the episode &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
N(\mathcal{S_t,A_t}) &amp;amp;\leftarrow N(\mathcal{S_t,A_t})+1 \\
Q(\mathcal{S_t,A_t}) &amp;amp;\leftarrow Q(\mathcal{S_t,A_t}) + {1\over N(\mathcal{S_t,A_t})}\left(G_t-Q(\mathcal{S_t,A_t})\right)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Improve policy based on new action-value function &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\epsilon &amp;amp;\leftarrow {1\over k} \\
\pi &amp;amp;\leftarrow \epsilon\text{-greedy}(Q)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;saralambda&quot;&gt;Sara(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
Q(\mathcal{S,A})\leftarrow Q(\mathcal{S,A})+\alpha(R+\gamma Q(\mathcal{S&amp;#39;,A&amp;#39;})-Q(\mathcal{S,A}))
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;n-step-sarsa&quot;&gt;&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-Step Sarsa&lt;/h3&gt;
&lt;p&gt;Define the &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step &lt;span class=&quot;math inline&quot;&gt;\(Q\)&lt;/span&gt;-return &lt;span class=&quot;math display&quot;&gt;\[
q_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n})
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step Sarsa updates &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt; towards the &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step &lt;span class=&quot;math inline&quot;&gt;\(Q\)&lt;/span&gt;-return &lt;span class=&quot;math display&quot;&gt;\[
Q(S_t, A_t)\leftarrow Q(S_t, A_t)+\alpha\left(q_t^{(n)}-Q(S_t,A_t)\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;forward-view-sarsalambda&quot;&gt;Forward View Sarsa(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;The &lt;span class=&quot;math inline&quot;&gt;\(q^\lambda\)&lt;/span&gt; return combines all &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step &lt;span class=&quot;math inline&quot;&gt;\(Q\)&lt;/span&gt;-returns &lt;span class=&quot;math inline&quot;&gt;\(q_t^{(n)}\)&lt;/span&gt;, using weight &lt;span class=&quot;math inline&quot;&gt;\((1-\lambda)\lambda^{n-1}\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
q_t^\lambda=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}q_t^{(n)}
\]&lt;/span&gt; Forward-view Sarsa(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;) &lt;span class=&quot;math display&quot;&gt;\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left(q_t^\lambda-Q(S_t,A_t)\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;backward-view-sarsalambda&quot;&gt;Backward View Sarsa(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;Just like TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;), we use &lt;code&gt;eligibility traces&lt;/code&gt; in an online algorithm. But Sarsa(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;) has one eligibility trace for each state-action pair &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
E_0(s,a) &amp;amp;= 0 \\
E_t(s,a) &amp;amp;= \gamma\lambda E_{t-1}(s,a)+1(S_t=s,A_t=a)
\end{align}
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt; is updated for every state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; and action &lt;span class=&quot;math inline&quot;&gt;\(a\)&lt;/span&gt;. Inproportion to TD-error &lt;span class=&quot;math inline&quot;&gt;\(\delta_t\)&lt;/span&gt; and eligibility trace &lt;span class=&quot;math inline&quot;&gt;\(E_t(s,a)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\delta_t = R_{t+1} + \gamma Q(S_{t+1}, A_{t+1})-Q(S_t,A_t)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
Q(s,a)\leftarrow Q(s,a)+\alpha\delta_t E_t(s,a)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;off-policy-learning&quot;&gt;Off-Policy Learning&lt;/h2&gt;
&lt;p&gt;Evaluate target policy &lt;span class=&quot;math inline&quot;&gt;\(\pi(a|s)\)&lt;/span&gt; to compute &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(q_\pi(s,a)\)&lt;/span&gt;, while following behaviour policy &lt;span class=&quot;math inline&quot;&gt;\(\mu(a|s)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\{\mathcal{S_1,A_1,R_2,...,S_t}\}\sim \mu
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;importance-sampling-for-off-policy-monte-carlo&quot;&gt;Importance Sampling for Off-Policy Monte-Carlo&lt;/h3&gt;
&lt;p&gt;Use returns generated from &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; to evaluate &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;. Weight return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; according to similarity between policies. Multiply importance sampling corrections along whole episode &lt;span class=&quot;math display&quot;&gt;\[
G_t^{\pi / \mu}=\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}
\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}
...
\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}
G_t
\]&lt;/span&gt; Update value towards &lt;em&gt;corrected&lt;/em&gt; return &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t)+\alpha\left(
\color{red}{G_t^{\pi / \mu}}
-V(S_t)\right)
\]&lt;/span&gt; Cannot use if &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; is zero when &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; is non-zero. Importance sampling can dramatically increase variance.&lt;/p&gt;
&lt;h3 id=&quot;importance-sampling-for-off-policy-td&quot;&gt;Importance Sampling for Off-Policy TD&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
V(S_t)\leftarrow V(S_t)+\alpha\left(
\color{red}{\frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}}
(R_{t+1}+\gamma V(S_{t+1}))-V(s_t)
\right)
\]&lt;/span&gt; Much lower variance than Monte-Carlo importance sampling. Policies only need to be similar over a single step.&lt;/p&gt;
&lt;h3 id=&quot;q-learning&quot;&gt;Q-Learning&lt;/h3&gt;
&lt;p&gt;Next action is chosen using behaviour policy &lt;span class=&quot;math inline&quot;&gt;\(A_{t+1}\sim\mu(\cdot |S_t)\)&lt;/span&gt;, but we consider alternative successor action &lt;span class=&quot;math inline&quot;&gt;\(A&amp;#39;\sim\pi(\cdot |S_t)\)&lt;/span&gt;, and update &lt;span class=&quot;math inline&quot;&gt;\(Q(S_t,A_t)\)&lt;/span&gt; towards value of alternative action &lt;span class=&quot;math display&quot;&gt;\[
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left(
\color{red}{R_{t+1}+\gamma Q(S_{t+1},A&amp;#39;)}
-Q(S_t,A_t)
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;off-policy-control-with-q-learning&quot;&gt;Off-Policy Control with Q-Learning&lt;/h3&gt;
&lt;p&gt;We now allow both behaviour and target policies to improve. The target policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; is greedy w.r.t. &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\pi(S_{t+1})=\mathop{\text{argmax}} \limits_{a&amp;#39;}Q(S_{t+1},a&amp;#39;)
\]&lt;/span&gt; The behaviour policy &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; is e.g. &lt;span class=&quot;math inline&quot;&gt;\(\epsilon\)&lt;/span&gt;-greedy w.r.t. &lt;span class=&quot;math inline&quot;&gt;\(Q(s,a)\)&lt;/span&gt;. The Q-learning target then simplifies: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
R_{t+1} &amp;amp;+ \gamma Q(S_{t+1},A&amp;#39;) \\
=R_{t+1} &amp;amp;+ \gamma Q(S_{t+1}, \mathop{\text{argmax}} \limits_{a&amp;#39;} Q(S_{t+1},a&amp;#39;)) \\
=R_{t+1} &amp;amp;+ \max_{a&amp;#39;} \gamma Q(S_{t+1},a&amp;#39;)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;q-learning-control-algorithm&quot;&gt;Q-Learning Control Algorithm&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
Q(S,A)\leftarrow Q(S,A)+\alpha\left(R+\gamma\max_{a&amp;#39;}Q(S&amp;#39;,a&amp;#39;)-Q(S,A)\right)
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 11 Jun 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/06/11/model-free-control.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/06/11/model-free-control.html</guid>
			</item>
		
			<item>
				<title>RL Concepts</title>
				<description>&lt;p&gt;&lt;strong&gt;backup&lt;/strong&gt;: We &lt;em&gt;back up&lt;/em&gt; the value of the state after each greedy move to the state before the move.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;experience&lt;/strong&gt;: sample sequences of &lt;em&gt;states&lt;/em&gt;, &lt;em&gt;actions&lt;/em&gt;, and &lt;em&gt;rewards&lt;/em&gt; from actual or simulated interaction with an environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nonstationary&lt;/strong&gt;: The return after taking an action in one state depends on the actions taken in later states in the same episode. Because all action selections are undergoing learning, the problem becomes nonstationary from the point view of the earlier state.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;prediction problem&lt;/strong&gt;: The computation of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(q_\pi\)&lt;/span&gt; for a fixed solution policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bootstrapping&lt;/strong&gt;: All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea &lt;em&gt;bootstrapping&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;exploring starts&lt;/strong&gt;: For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes &lt;em&gt;start in a state-action pair&lt;/em&gt;, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of &lt;em&gt;exploring starts&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-greedy&lt;/strong&gt;: most of the time they choose an action that maximal estimated action value, but with probability &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt; they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, &lt;span class=&quot;math inline&quot;&gt;\({\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt;, and the remaining bulk of the probability, &lt;span class=&quot;math inline&quot;&gt;\(1-\varepsilon +{\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt;, is given to the greedy action. The &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-greedy policies are examples of &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-soft policies, defined as policies for which &lt;span class=&quot;math inline&quot;&gt;\(\pi(a|s)\ge {\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt; for all states and actions, for some &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon &amp;gt;0\)&lt;/span&gt;. Among &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-soft policies, $-greedy policies are in some sense those that are closest to greedy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;target policy&lt;/strong&gt;: The policy being learned about is called the &lt;em&gt;target policy&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;behavior policy&lt;/strong&gt;: The policy used to generate behavior is called the &lt;em&gt;behavior policy&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;off-policy learning&lt;/strong&gt;: Learning is from data “off” the target policy, and the overall process is termed &lt;em&gt;off-policy learning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;importance sampling&lt;/strong&gt;: a general technique for estimating expected values under one distribution given samples from another.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;importance-sampling ratio&lt;/strong&gt;: We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the &lt;em&gt;importance-sampling ratio&lt;/em&gt;. &lt;span class=&quot;math display&quot;&gt;\[
\rho_t^T=\frac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1}\mu(A_k|S_k)p(S_{k+1}|S_k,A_k)}
=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ordinary importance sampling&lt;/strong&gt;: Now we are ready to give a Monte Carlo algorithm that uses a batch of observed episodes following policy &lt;span class=&quot;math inline&quot;&gt;\(\mu\)&lt;/span&gt; to estimate &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(\mathcal{s})\)&lt;/span&gt;. It is convenient here to number time steps in a way that increases across episode boundaries. That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time &lt;span class=&quot;math inline&quot;&gt;\(t=101\)&lt;/span&gt;. This enables us to use time-step numbers to refer to particular steps in particular episodes. In particular, we can define the set of all time steps in which state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{s}\)&lt;/span&gt; is visited, denoted &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{T}(\mathcal{s})\)&lt;/span&gt;. This is for an every-visit method; for a first-visit method, &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{T}(\mathcal{s})\)&lt;/span&gt; would only include time steps that were first visits to &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{s}\)&lt;/span&gt; within their episodes. Also, let &lt;span class=&quot;math inline&quot;&gt;\(T(t)\)&lt;/span&gt; denote the first time of termination following time &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; denote the return after &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt; up through &lt;span class=&quot;math inline&quot;&gt;\(T(t)\)&lt;/span&gt;. Then &lt;span class=&quot;math inline&quot;&gt;\(\{G_t\}_{t\in{\mathcal{T}(\mathcal{s})}}\)&lt;/span&gt; are the returns that pertain to state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{s}\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(\{\rho_t^{T(t)}\}_{t\in{\mathcal{T}(\mathcal{s})}}\)&lt;/span&gt; are the corresponding importance-sampling ratios. To estimate &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(\mathcal{s})\)&lt;/span&gt;, we simply scale the returns by the ratios and average the results: &lt;span class=&quot;math display&quot;&gt;\[
V(\mathcal{s})=\frac{\sum_{t\in{\mathcal{T}(\mathcal{s})}}\rho_t^{T(t)}G_t}{|\mathcal{T}(\mathcal{s})|}
\]&lt;/span&gt; When importance sampling is done as a simple average in this way it is called &lt;em&gt;ordinary importance sampling&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;weighted importance sampling&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
V(\mathcal{s})=\frac{\sum_{t\in\mathcal{T}(\mathcal{s})} \rho_t^{T(t)}G_t}{\sum_{t\in\mathcal{T}(\mathcal{s})}\rho_t^{T(t)}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Score Function&lt;/strong&gt;: The score function &lt;span class=&quot;math inline&quot;&gt;\(u(\theta)\)&lt;/span&gt; is the partial derivative of the log-likelihood function &lt;span class=&quot;math inline&quot;&gt;\(F(\theta)=\text{ln}L(\theta)\)&lt;/span&gt;, where &lt;span class=&quot;math inline&quot;&gt;\(L(\theta)\)&lt;/span&gt; is the standard likelihood function. &lt;span class=&quot;math display&quot;&gt;\[
u(\theta)=\frac{\partial}{\partial\theta}F(\theta)
\]&lt;/span&gt; Using formulatino of &lt;span class=&quot;math inline&quot;&gt;\(u\)&lt;/span&gt;, one can easily compute various statistical measurements associated with &lt;span class=&quot;math inline&quot;&gt;\(u\)&lt;/span&gt;. For example, the mean &lt;span class=&quot;math inline&quot;&gt;\(E(u(\theta))\)&lt;/span&gt; can be shown to equal zero while the variance is precisely the Fisher information matrix.&lt;a href=&quot;http://mathworld.wolfram.com/ScoreFunction.html&quot;&gt;[Link]&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Fri, 20 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/20/rl-concepts.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/20/rl-concepts.html</guid>
			</item>
		
			<item>
				<title>Temporal-Difference Learning</title>
				<description>&lt;ul&gt;
&lt;li&gt;TD methods learn directly from episodes of experience&lt;/li&gt;
&lt;li&gt;TD is &lt;em&gt;model-free&lt;/em&gt;: no knowledge of MDP transitions / rewards&lt;/li&gt;
&lt;li&gt;TD learns from &lt;em&gt;incomplete&lt;/em&gt; episodes, by &lt;em&gt;bootstrapping&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;TD updates a guess towards a guess&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;mc-and-td&quot;&gt;MC and TD&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: learn &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; online from experience under policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;incremental-every-visit-monte-carlo&quot;&gt;Incremental every-visit Monte-Carlo&lt;/h3&gt;
&lt;p&gt;Update value &lt;span class=&quot;math inline&quot;&gt;\(V(S_t)\)&lt;/span&gt; toward &lt;em&gt;actual&lt;/em&gt; return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;simplest-temporal-difference-learning-algorithm-td0&quot;&gt;Simplest temporal-difference learning algorithm: TD(0)&lt;/h3&gt;
&lt;p&gt;Update value &lt;span class=&quot;math inline&quot;&gt;\(V(S_t)\)&lt;/span&gt; toward &lt;em&gt;estimated&lt;/em&gt; return &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma V(S_{t+1})\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha\left(R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right)
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma V(S_{t+1})\)&lt;/span&gt; is called the &lt;em&gt;TD&lt;/em&gt; target.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\)&lt;/span&gt; is called the &lt;em&gt;TD&lt;/em&gt; error.&lt;/p&gt;
&lt;h2 id=&quot;biasvariance-trade-off&quot;&gt;Bias/Variance Trade-Off&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Return &lt;span class=&quot;math inline&quot;&gt;\(G_t=R_{t+1}+\gamma R_{t+2}+...+\gamma^{T-1}R_T\)&lt;/span&gt; is &lt;em&gt;unbiased&lt;/em&gt; estimate of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(S_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;True TD target &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma v_\pi(S_{t+1})\)&lt;/span&gt; is &lt;em&gt;unbiased&lt;/em&gt; estimate of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(S_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;TD target &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma V(S_{t+1})\)&lt;/span&gt; is &lt;em&gt;biased&lt;/em&gt; estimate of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(S_t)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;TD target is much lower variance than the return:
&lt;ul&gt;
&lt;li&gt;Return depends on &lt;em&gt;many&lt;/em&gt; random actions, transitions, rewards&lt;/li&gt;
&lt;li&gt;TD target depends on &lt;em&gt;one&lt;/em&gt; random action, transition, reward&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;advantages-and-disadvantages-of-mc-vs.td&quot;&gt;Advantages and Disadvantages of MC vs. TD&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;MC has high variance, zero bias
&lt;ul&gt;
&lt;li&gt;Good convergence properties&lt;/li&gt;
&lt;li&gt;(even with function approximation)&lt;/li&gt;
&lt;li&gt;Not very sensitive to initial value&lt;/li&gt;
&lt;li&gt;Very simple to understand and use&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;TD has low variance, some bias
&lt;ul&gt;
&lt;li&gt;Usually more efficient than MC&lt;/li&gt;
&lt;li&gt;TD(0) converges to &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;(but not always with function approximation)&lt;/li&gt;
&lt;li&gt;More sensitive to initial value&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;monte-carlo-backup&quot;&gt;Monte-Carlo Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/42.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;temporal-difference-backup&quot;&gt;Temporal-Difference Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/43.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;dynamic-programming-backup&quot;&gt;Dynamic Programming Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/44.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;bootstrapping-and-sampling&quot;&gt;Bootstrapping and Sampling&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt;: update involves an estimate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC does not bootstrap&lt;/li&gt;
&lt;li&gt;DP bootstraps&lt;/li&gt;
&lt;li&gt;TD bootstraps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sampling&lt;/strong&gt;: update samples an expectation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC samples&lt;/li&gt;
&lt;li&gt;DP does not sample&lt;/li&gt;
&lt;li&gt;TD samples&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;unified-view-of-reinforcement-learning&quot;&gt;Unified View of Reinforcement Learning&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/45.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;tdlambda&quot;&gt;TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h2&gt;
&lt;h3 id=&quot;n-step-return&quot;&gt;&lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step return&lt;/h3&gt;
&lt;p&gt;Define the &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step return: &lt;span class=&quot;math display&quot;&gt;\[
G_t^{(n)}=R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^nV(S_{t+n})
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step temporal-difference learning &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t)+\alpha\left(G_t^{(n)}-V(S_t)\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;averaging-n-step-returns&quot;&gt;averaging &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step returns&lt;/h3&gt;
&lt;p&gt;e.g. average the 2-step and 4-step returns &lt;span class=&quot;math display&quot;&gt;\[
{1 \over 2}G^{(2)} + {1 \over 2}G^{(4)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;lambda-return&quot;&gt;&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;-return&lt;/h3&gt;
&lt;p&gt;The &lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;-return &lt;span class=&quot;math inline&quot;&gt;\(G_t^{\lambda}\)&lt;/span&gt; combines all &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;-step returns &lt;span class=&quot;math inline&quot;&gt;\(G_t^{(n)}\)&lt;/span&gt;. Using weight &lt;span class=&quot;math inline&quot;&gt;\((1-\lambda)\lambda^{n-1}\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
G_t^\lambda=(1-\lambda)\sum_{n=1}^\infty \lambda^{n-1}G_t^{(n)}
\]&lt;/span&gt; Forward-view TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;) &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha\left(G_t^\lambda-V(S_t)\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;eligibility-traces&quot;&gt;Eligibility Traces&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/46.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Credit assignment problem: did bell or light cause shock?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frequency heuristic&lt;/strong&gt;: assign credit to most frequent states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recency heuristic&lt;/strong&gt;: assign credit to most recent states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Elegibility traces&lt;/strong&gt; combine both heuristics: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
E_0(s) &amp;amp;= 0 \\
E_t(s) &amp;amp;= \gamma\lambda E_{t-1}(s) + 1(S_t=s)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/47.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;backward-view-tdlambda&quot;&gt;Backward view TD(&lt;span class=&quot;math inline&quot;&gt;\(\lambda\)&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;Keep an eligibility trace for every state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;. Update value &lt;span class=&quot;math inline&quot;&gt;\(V(s)\)&lt;/span&gt; for every state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;. In proportion to TD-error &lt;span class=&quot;math inline&quot;&gt;\(\delta_t\)&lt;/span&gt; and eligibility trace &lt;span class=&quot;math inline&quot;&gt;\(E_t(s)\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\delta_t &amp;amp;= R_{t+1}+\gamma V(S_{t+1})-V(S_t) \\
V(s) &amp;amp;\leftarrow V(s)+\alpha\delta_t E_t(s)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 19 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/19/temporal-difference-learning.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/19/temporal-difference-learning.html</guid>
			</item>
		
			<item>
				<title>Monte-Carlo Reinforcement Learning</title>
				<description>&lt;ul&gt;
&lt;li&gt;MC methods learn directly from episodes of experience&lt;/li&gt;
&lt;li&gt;MC is &lt;em&gt;model-free&lt;/em&gt;: no knowledge of MDP transitions / rewards&lt;/li&gt;
&lt;li&gt;MC learns from &lt;em&gt;complete&lt;/em&gt; episodes: no bootstrapping&lt;/li&gt;
&lt;li&gt;MC uses the simplest possible idea: value = mean return&lt;/li&gt;
&lt;li&gt;Caveat: can only apply MC to &lt;em&gt;episodic&lt;/em&gt; MDPs. All episodes must terminate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;monte-carlo-policy-evaluation&quot;&gt;Monte-Carlo Policy Evaluation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: learn &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; from episodes of experience under policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Monte-Carlo policy evaluation uses &lt;code&gt;empirical mean return&lt;/code&gt; instead of &lt;em&gt;expected return&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&quot;incremental-mean&quot;&gt;Incremental Mean&lt;/h2&gt;
&lt;p&gt;The mean &lt;span class=&quot;math inline&quot;&gt;\(\mu_1,\mu_2,...\)&lt;/span&gt; of a sequence &lt;span class=&quot;math inline&quot;&gt;\(x_1,x_2,...\)&lt;/span&gt; can be computed incrementally, &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\mu_k &amp;amp;= {1 \over k}\sum_{j=1}^k x_j \\
&amp;amp;= {1 \over k}\left(x_k+\sum_{j=1}^{k-1} x_j\right) \\
&amp;amp;= {1 \over k}\left(x_k+(k-1)\mu_{k-1} \right) \\
&amp;amp;= \mu_{k-1} + {1 \over k}(x_k - \mu_{k-1})
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;incremental-monte-carlo-updates&quot;&gt;Incremental Monte-Carlo Updates&lt;/h2&gt;
&lt;p&gt;Update &lt;span class=&quot;math inline&quot;&gt;\(V(s)\)&lt;/span&gt; incrementally after episode &lt;span class=&quot;math inline&quot;&gt;\(S_1,A_1,R_2,...,S_T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For each state &lt;span class=&quot;math inline&quot;&gt;\(S_t\)&lt;/span&gt; with return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
N(S_t) &amp;amp;\leftarrow N(S_t) + 1 \\
V(S_t) &amp;amp;\leftarrow V(S_t) + {1 \over N(S_t)}(G_t-V(S_t))
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes. &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 19 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/19/monte-carlo-reinforcement-learning.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/19/monte-carlo-reinforcement-learning.html</guid>
			</item>
		
			<item>
				<title>Dynamic Programming</title>
				<description>&lt;p&gt;A method for solving complex problems by breaking them down into subproblems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slove the subproblems&lt;/li&gt;
&lt;li&gt;Combine solutions to subproblems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dynamic programming assumes full knowledge of the MDP. It is used for &lt;em&gt;planning&lt;/em&gt; in an MDP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For prediction:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Input: MDP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&amp;gt;\)&lt;/span&gt; and policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; or MRP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal P^\pi,\mathcal R^\pi,\gamma \right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Output: value function &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For control:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Input: MDP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Output: optimal value function &lt;span class=&quot;math inline&quot;&gt;\(v_*\)&lt;/span&gt; and optimal policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_*\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;iterative-policy-evaluation&quot;&gt;Iterative Policy Evaluation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: evaluate a given policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: iterative application of Bellman expectation backup &lt;span class=&quot;math display&quot;&gt;\[
v_1\to v_2 \to ... \to v_\pi
\]&lt;/span&gt; Using &lt;em&gt;synchronous&lt;/em&gt; backups,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At each iteration &lt;span class=&quot;math inline&quot;&gt;\(k+1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;For all state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s \in \mathcal S\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update &lt;span class=&quot;math inline&quot;&gt;\(v_{k+1}(\mathcal s)\)&lt;/span&gt; from &lt;span class=&quot;math inline&quot;&gt;\(v_k(\mathcal s&amp;#39;)\)&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s&amp;#39;\)&lt;/span&gt; is a successor state of &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}(\mathcal s)=\sum_{a\in\mathcal A} \pi(a|\mathcal s)\left(\mathcal R_{\mathcal s}^a+\gamma\sum_{\mathcal s&amp;#39;\in\mathcal S}\mathcal P^a_{\mathcal s\mathcal s&amp;#39;}v_k(\mathcal s&amp;#39;)\right)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\mathbf{v^{k+1}}=\mathbf{\mathcal R^\pi} + \gamma\mathbf{\mathcal P^\pi v^k}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;how-to-improve-a-policy&quot;&gt;How to Improve a Policy&lt;/h2&gt;
&lt;p&gt;Given a policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;, &lt;strong&gt;evaluate&lt;/strong&gt; the policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
v_\pi(s)=\Bbb E[R_{t+1}+\gamma R_{t+2}+...|S_t=s]
\]&lt;/span&gt; &lt;strong&gt;Improve&lt;/strong&gt; the policy by acting greedily with respect to &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\pi &amp;#39;=greedy(v_\pi)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/41.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;principle-of-optimality&quot;&gt;Principle of Optimality&lt;/h2&gt;
&lt;p&gt;Any optimal policy can be subdivided into two components:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;An optimal first Action &lt;span class=&quot;math inline&quot;&gt;\(A_*\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Followed by an optimal policy from successor state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{S&amp;#39;}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A policy &lt;span class=&quot;math inline&quot;&gt;\(\pi(a|s)\)&lt;/span&gt; achieves the optimal value from state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)=v_*(s)\)&lt;/span&gt;, if and only if &lt;strong&gt;For any state &lt;span class=&quot;math inline&quot;&gt;\(s&amp;#39;\)&lt;/span&gt; reachable from &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; achieves the optimal value from state &lt;span class=&quot;math inline&quot;&gt;\(s&amp;#39;\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s&amp;#39;)=v_*(s&amp;#39;)\)&lt;/span&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&quot;deterministic-value-iteration&quot;&gt;Deterministic Value Iteration&lt;/h2&gt;
&lt;p&gt;If we know the solution to subproblems &lt;span class=&quot;math inline&quot;&gt;\(v_*(s&amp;#39;)\)&lt;/span&gt;. Then solution &lt;span class=&quot;math inline&quot;&gt;\(v_*(s)\)&lt;/span&gt; can be found by one-step lookahead: &lt;span class=&quot;math display&quot;&gt;\[
v_*(s)\leftarrow\max_{a\in\mathcal A}\mathcal R_s^a+\gamma\sum_{s&amp;#39;\in\mathcal S}v_*(s&amp;#39;)
\]&lt;/span&gt; The idea of value iteration is to apply these updates iteratively.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt;: start with final rewards and word backwards. &lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}(s)=\max_{a\in\mathcal A}\left(\mathcal R_s^a+\gamma\sum_{s&amp;#39;\in\mathcal S}\mathcal R_{ss&amp;#39;}^av_k(s&amp;#39;)\right)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}=\max_{a\in\mathcal A}\mathcal R^a+\gamma\mathcal P^av_k
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;synchronous-dynamic-programming-algorithms&quot;&gt;Synchronous Dynamic Programming Algorithms&lt;/h2&gt;
&lt;table style=&quot;width:50%;&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 11%&quot; /&gt;
&lt;col style=&quot;width: 23%&quot; /&gt;
&lt;col style=&quot;width: 15%&quot; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Problem&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Bellman Equation&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Algorithm&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Prediction&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Expectation Equation&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Iterative Policy Evaluation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Control&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Expectation Equation + Greedy Policy Improvement&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Policy Iteration&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Control&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Optimally Equation&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Value Iteration&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;Algorithms are based on state-value function &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(v_*(s)\)&lt;/span&gt;. Complexity &lt;span class=&quot;math inline&quot;&gt;\(O(mn^2)\)&lt;/span&gt; per iteration for &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; actions and &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; states.&lt;/li&gt;
&lt;li&gt;Could also apply to action-value function &lt;span class=&quot;math inline&quot;&gt;\(q_\pi(s,a)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(q_*(s,a)\)&lt;/span&gt;. Complexity &lt;span class=&quot;math inline&quot;&gt;\(O(m^2n^2)\)&lt;/span&gt; per iteration.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;asynchronous-dynamic-programming&quot;&gt;Asynchronous Dynamic Programming&lt;/h2&gt;
&lt;p&gt;Asynchronous DP backs up states individually in any order. For each selected state, apply the appropriate backup. Can significantly reduce computation. Guaranteed to converge if all states continue to be selected.&lt;/p&gt;
&lt;h3 id=&quot;in-place-dynamic-programming&quot;&gt;In-Place Dynamic Programming&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Synchronous value iteration&lt;/strong&gt; stores two copies of value function for all &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; in &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{S}\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
v_{new}(s) &amp;amp;\leftarrow \max_{a\in{\mathcal{A}}}\left(\mathcal{R}_s^a+
\gamma\sum_{s&amp;#39;\in{\mathcal{S}}}\mathcal{P}^a_{ss&amp;#39;}v_{old}(s&amp;#39;)
\right) \\
v_{old} &amp;amp;\leftarrow v_{new}
\end{align}
\]&lt;/span&gt; &lt;strong&gt;In-place value iteration&lt;/strong&gt; only stores one copy of value function for all &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; in &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{S}\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v(s) \leftarrow \max_{a\in\mathcal{A}}\left(
\mathcal{R}_s^a+\gamma\sum_{s&amp;#39;\in\mathcal{S}}\mathcal{P}_{ss&amp;#39;}^av(s&amp;#39;)
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;prioritised-sweeping&quot;&gt;Prioritised Sweeping&lt;/h3&gt;
&lt;p&gt;Use magnitude of Bellman error to guide state selection, e.g. &lt;span class=&quot;math display&quot;&gt;\[
\left|
\max_{a\in\mathcal{A}}
\left(
\mathcal{R}_s^a + \gamma\sum_{s&amp;#39;\in\mathcal{S}}\mathcal{P}_{ss&amp;#39;}^av(s&amp;#39;)
\right) - v(s)
\right|
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Backup the state with the largest remaining Bellman error&lt;/li&gt;
&lt;li&gt;Update Bellman error of affected states after each backup&lt;/li&gt;
&lt;li&gt;Requires knowledge of reverse dynamics (predecessor states)&lt;/li&gt;
&lt;li&gt;Can be implemented efficiently by maintaining a priority queue&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;real-time-dynamic-programming&quot;&gt;Real-Time Dynamic Programming&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Idea&lt;/strong&gt;: only states that are relevant to agent. Use agent’s experience to guide the selection of states. After each time-step &lt;span class=&quot;math inline&quot;&gt;\(S_t, A_t, R_{t+1}\)&lt;/span&gt;, backup the state &lt;span class=&quot;math inline&quot;&gt;\(S_t\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
v(S_t) \leftarrow \max_{a\in\mathcal{A}} \left(
\mathcal{R}_{S_t}^a + \gamma\sum_{s&amp;#39;\in\mathcal{S}}
\mathcal{P}_{S_ts&amp;#39;}^av(s&amp;#39;)
\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;full-width-and-sample-backups&quot;&gt;Full-width and sample backups&lt;/h2&gt;
&lt;h3 id=&quot;full-width-backups&quot;&gt;Full-Width Backups&lt;/h3&gt;
&lt;p&gt;DP uses &lt;em&gt;full-width&lt;/em&gt; backups. For each backup (sync or async):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Every successor state and action is considered&lt;/li&gt;
&lt;li&gt;Using knowledge of the MDP transitions and reward function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DP is effective for medium-sized problems (millions of states). For large problems DP suffers Bellman’s &lt;em&gt;curse of dimensionality&lt;/em&gt; (Number of states &lt;span class=&quot;math inline&quot;&gt;\(n=|\mathcal{S}|\)&lt;/span&gt; grows exponentially with number of state variables). Even one backup can be too expensive.&lt;/p&gt;
&lt;h3 id=&quot;sample-backups&quot;&gt;Sample Backups&lt;/h3&gt;
&lt;p&gt;Using sample rewards and sample transitions &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal{S,A,R,S&amp;#39;}\right&amp;gt;\)&lt;/span&gt; instead of reward function &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{R}\)&lt;/span&gt; and transition dynamics &lt;span class=&quot;math inline&quot;&gt;\(\mathcal{P}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model-free: no advance knowledge of MDP required&lt;/li&gt;
&lt;li&gt;Breaks the curse of dimensionality through sampling&lt;/li&gt;
&lt;li&gt;Cost of backup is constant, independent of &lt;span class=&quot;math inline&quot;&gt;\(n=|\mathcal{S}|\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 17 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/17/dynamic-programming.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/17/dynamic-programming.html</guid>
			</item>
		
	</channel>
</rss>
