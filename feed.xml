<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>An elder's memo.</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Sentiment analysis</title>
				<description>&lt;p&gt;Sentiment analysis has many other names&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Opinion extraction&lt;/li&gt;
&lt;li&gt;Opinion mining&lt;/li&gt;
&lt;li&gt;Sentiment mining&lt;/li&gt;
&lt;li&gt;Subjectively analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;learning-sentiment-lexicons&quot;&gt;Learning Sentiment Lexicons&lt;/h2&gt;
&lt;h3 id=&quot;pointwise-mutual-information&quot;&gt;Pointwise Mutual Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mutual information&lt;/strong&gt; between 2 random variables &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
I(X,Y)=\sum_x \sum_y P(x,y)log_2 \frac{P(x,y)}{P(x)P(y)}
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pointwise mutual information&lt;/strong&gt;: How much more do events &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; co-occur than if they were independent? &lt;span class=&quot;math display&quot;&gt;\[
PMI(X,Y)=log_2 \frac{P(x,y)}{P(x)P(y)}
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PMI between two words&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
PMI(word_1, word_2)=log_2 \frac{P(word_1, word_2)}{P(word_1)P(word_2)}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;how-to-estimate-pointwise-mutual-information&quot;&gt;How to Estimate Pointwise Mutual Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Query search engine (Altavista)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(P(word)\)&lt;/span&gt; estimated by &lt;span class=&quot;math inline&quot;&gt;\(hits(word)/N\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(P(word_1, word_2)\)&lt;/span&gt; by &lt;span class=&quot;math inline&quot;&gt;\(hits(word_1 \text{ NEAR } word_2)/N^2\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
PMI(word_1, word_2)=log_2 \frac{hits(word_1\text{ NEAR } word_2)}{hits(word_1)hits(word_2)}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Does phrase appear more with “poor” or “excellent” &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
Polarity(phrase)&amp;amp;=PMI(phrase, \text{&amp;quot;excellent&amp;quot;})-PMI(phrase, \text{&amp;quot;poor&amp;quot;})\\
&amp;amp;=log_2 \frac{hits(phrase \text{ NEAR &amp;quot;excellent&amp;quot;})}{hits(phrase)hits(\text{&amp;quot;excellent&amp;quot;})} - log_2 \frac{hits(phrase \text{ NEAR &amp;quot;poor&amp;quot;})}{hits(phrase)hits(\text{&amp;quot;poor&amp;quot;})} \\
&amp;amp;=log_2 \frac{hits(phrase \text{ NEAR &amp;quot;excellent&amp;quot;})}{hits(phrase)hits(\text{&amp;quot;excellent&amp;quot;})} \frac{hits(phrase \text{ NEAR &amp;quot;poor&amp;quot;})}{hits(phrase)hits(\text{&amp;quot;poor&amp;quot;})} \\
&amp;amp;=log_2\left( \frac{hits(phrase \text{ NEAR &amp;quot;excellent&amp;quot;})hits(\text{&amp;quot;poor&amp;quot;})}{hits(phrase \text{ NEAR &amp;quot;poor&amp;quot;})hits(\text{&amp;quot;excellent&amp;quot;})} \right)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;other-sentiment-tasks&quot;&gt;Other Sentiment Tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Finding sentiment of a sentence
&lt;ul&gt;
&lt;li&gt;The food was great but the service was awful.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Finding aspect/attribute/target of sentiment&lt;/li&gt;
&lt;li&gt;Detection of Friendliness&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Wed, 23 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/nlp/2015/12/23/sentiment-analysis.html</link>
				<guid isPermaLink="true">/foundation/nlp/2015/12/23/sentiment-analysis.html</guid>
			</item>
		
			<item>
				<title>Neural Reasoner</title>
				<description>&lt;h2 id=&quot;overview-of-neural-reasoner&quot;&gt;Overview of Neural Reasoner&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Neural Reasoner&lt;/strong&gt; has a layered architecture to deal with the complicated logical relations in reasoning as illustrated in Figure 1:&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/24.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;It consists of &lt;code&gt;one encoding layer&lt;/code&gt; and &lt;code&gt;multiple reasoning layers&lt;/code&gt;. The encoder layers first converts the question and facts from natural language sentences to vectorial representations. More specifically, &lt;span class=&quot;math display&quot;&gt;\[
Q \quad \underrightarrow{encode} \quad q^{(0)} , F_k \quad \underrightarrow{encode} \quad f_k^{(0)},k=1,2,...,K.
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(q^{(0)} \in \Bbb R^{d_Q}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(f_k^{(0)} \in \Bbb R^{d_F}\)&lt;/span&gt;. With the representations obtained from the encoding layer, the reasoning layer recursively updates the representations of questions and facts, &lt;span class=&quot;math display&quot;&gt;\[
\{q^{(l)}f_1^{(l)}\dotsb f_K^{(l)}\}\quad \underrightarrow{reason} \quad\{q^{(l+1)}f_1^{(l+1)}\dotsb f_K^{(l+1)}\}
\]&lt;/span&gt; through the interaction between question representation and fact representations. Intuitively, this interaction models the reasoning, including examination of the facts and comparison of the facts and the questions. Finally at layer-&lt;span class=&quot;math inline&quot;&gt;\(L\)&lt;/span&gt;, the resulted question representation &lt;span class=&quot;math inline&quot;&gt;\(q^{(L)}\)&lt;/span&gt; is fed to an answerer, which layer can be a classifier for choosing between a number of pre-determined classes (e.g., {Yes, No}) or a text generator for create a sentence.&lt;/p&gt;
&lt;p&gt;Neural Reasoner has the following desired properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it can handle varying number of facts, including irrelevant ones, and reach the final conclusion through repeated processing of filtering and combining;&lt;/li&gt;
&lt;li&gt;it makes no assumption about the form of language, as long as enough training examples are given.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/25.png&quot; alt=&quot;A diagram of implementation of Neural Reasoner with L reasoning layers, operating on one question and K facts&quot; /&gt;&lt;figcaption&gt;A diagram of implementation of Neural Reasoner with L reasoning layers, operating on one question and K facts&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;auxiliary-training-for-questionfact-representation&quot;&gt;Auxiliary Training for Question/Fact Representation&lt;/h2&gt;
&lt;p&gt;Use auxiliary training to facilitate the learning of representations of question and facts. Basically, in addition to using the learned representations of question and facts in the reasoning process, also use those representations to reconstruct the original questions or their more abstract forms with variables.&lt;/p&gt;
&lt;p&gt;In the auxiliary training, intend to achieve the following two goals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to compensate the lack of supervision in the learning task. In experiments, the supervision can be fairly weak since for each instance it is merely a classification with no more than 12 classes, while the number of instances are 1K to 10K.&lt;/li&gt;
&lt;li&gt;to introduce beneficial bias for the representation learning task. Since the network is a complicated nonlinear function, the back-propagation from the answering layer to the encoding layer can easily fail to learn well.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/26.png&quot; alt=&quot;Auxiliary training for question representation. The training for fact representation is identical and therefore omitted&quot; /&gt;&lt;figcaption&gt;Auxiliary training for question representation. The training for fact representation is identical and therefore omitted&lt;/figcaption&gt;
&lt;/figure&gt;
</description>
				<pubDate>Mon, 21 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/paper/2015/12/21/neural-reasoner.html</link>
				<guid isPermaLink="true">/foundation/paper/2015/12/21/neural-reasoner.html</guid>
			</item>
		
			<item>
				<title>OXFORD Machine Learning</title>
				<description>&lt;p&gt;&lt;a href=&quot;https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/&quot;&gt;Course Page&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;lecture-2-linear-supervised-learning&quot;&gt;Lecture 2 Linear supervised learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Many real processes can be &lt;em&gt;approximated&lt;/em&gt; with linear models.&lt;/li&gt;
&lt;li&gt;Linear regression often appears as a &lt;em&gt;module&lt;/em&gt; of larger systems.&lt;/li&gt;
&lt;li&gt;Linear problems can be solved &lt;em&gt;analytically&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Linear prediction provides an introduction to many of the &lt;em&gt;core concepts&lt;/em&gt; of machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/19.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;lecture-3&quot;&gt;Lecture 3&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/20.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/images/21.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/images/22.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/images/23.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/19/oxford-machine-learning.html</link>
				<guid isPermaLink="true">/foundation/2015/12/19/oxford-machine-learning.html</guid>
			</item>
		
			<item>
				<title>How to Choose a Neural Network</title>
				<description>&lt;figure&gt;
&lt;img src=&quot;/assets/images/18.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
</description>
				<pubDate>Sat, 19 Dec 2015 00:00:00 +0800</pubDate>
				<link>/method/2015/12/19/how-to-choose-a-neural-network.html</link>
				<guid isPermaLink="true">/method/2015/12/19/how-to-choose-a-neural-network.html</guid>
			</item>
		
			<item>
				<title>Basic conception</title>
				<description>&lt;h2 id=&quot;standard-deviation&quot;&gt;Standard Deviation&lt;/h2&gt;
&lt;p&gt;In statistics and probability theory, the standard deviation (SD) (represented by the Greek letter sigma, &lt;span class=&quot;math inline&quot;&gt;\(\sigma\)&lt;/span&gt;) measures the amount of variation or dispersion from the average. A low standard deviation indicates that the data points tend to be very close to the mean (also called expected value); a high standard deviation indicates that the data points are spread out over a large range of values.&lt;/p&gt;
&lt;h2 id=&quot;derivative&quot;&gt;Derivative&lt;/h2&gt;
&lt;p&gt;The derivative of a function of a real variable measures the sensitivity to change of a quantity (a function or dependent variable) which is determined by another quantity (the independent variable). For example, the derivative of the position of a moving object with respect to time is the object’s velocity: this measures how quickly the position of the object changes when time is advanced.当x变化时，y的变化量。&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/19/basic-conception.html</link>
				<guid isPermaLink="true">/foundation/2015/12/19/basic-conception.html</guid>
			</item>
		
			<item>
				<title>Theoretical Motivations for Deep Learning</title>
				<description>&lt;p&gt;With distributed representations, it is possible to represent exponential number of regions with a linear number of parameters. The magic of distributed representation is that it can learn a very complicated function(with many ups and downs) with a low number of examples.&lt;/p&gt;
&lt;p&gt;Depth is not necessary to have a flexible family of functions. Deeper networks does not correspond to a higher capacity. Deeper doesn’t mean we can represent more functions. If the functions we trying to learn has a particular characteristic obtained through composition of many operations, then it is much better to approximate these functions with a deep neural network.&lt;/p&gt;
&lt;p&gt;There is new theoretical result that deeper nets with rectifier/maxout units are exponentially more expressive than shallow ones because they can split the input space in many more linear regions, with constraints.&lt;/p&gt;
</description>
				<pubDate>Fri, 18 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/18/theoretical-motivations-for-deep-learning.html</link>
				<guid isPermaLink="true">/foundation/2015/12/18/theoretical-motivations-for-deep-learning.html</guid>
			</item>
		
			<item>
				<title>Jekyll render with pandoc on windows</title>
				<description>&lt;h2 id=&quot;下载绿色版jekyll&quot;&gt;下载绿色版Jekyll&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/sohero/PortableJekyll&quot; title=&quot;下载地址&quot;&gt;下载地址&lt;/a&gt;，这个版本，在原Portable Jekyll的基础上，修改了一下setpath.cmd。这样在写的正文里有中文也不会报错了。&lt;/li&gt;
&lt;li&gt;解压到任一目录，直接运行setpath.cmd，就可以使用了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装pandoc&quot;&gt;安装pandoc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jekyll的两个markdown解析器，对于mathjax的支持都不完美。用pandoc没有问题，不过需要安装jekyll-pandoc插件，&lt;a href=&quot;https://github.com/sohero/jekyll-pandoc&quot;&gt;插件下载地址&lt;/a&gt;。这个版本修改了一下依赖的Jekyll版本号，这样就可以在Jekyll 3.0里使用了。&lt;/li&gt;
&lt;li&gt;安装pandoc：&lt;a href=&quot;http://pandoc.org/&quot;&gt;下载地址&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Thu, 17 Dec 2015 00:00:00 +0800</pubDate>
				<link>/tools/log/2015/12/17/jekyll-render-with-pandoc-on-windows.html</link>
				<guid isPermaLink="true">/tools/log/2015/12/17/jekyll-render-with-pandoc-on-windows.html</guid>
			</item>
		
			<item>
				<title>git笔记 and cheatsheet</title>
				<description>&lt;p&gt;&lt;a class=&quot;btn btn-default&quot; href=&quot;https://github.com/sohero/git&quot;&gt;Go.&lt;/a&gt; &lt;a class=&quot;btn btn-default&quot; href=&quot;http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 17 Dec 2015 00:00:00 +0800</pubDate>
				<link>/tools/log/2015/12/17/git-note.html</link>
				<guid isPermaLink="true">/tools/log/2015/12/17/git-note.html</guid>
			</item>
		
			<item>
				<title>Language Modeling</title>
				<description>&lt;ul&gt;
&lt;li&gt;Goal: compute the probability of a sentence or a sequence of words: &lt;span class=&quot;math display&quot;&gt;\[
P(W)=P(w_1,w_2,w_3,...,w_n)
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Related task: probability of an upcoming word: &lt;span class=&quot;math display&quot;&gt;\[
p(w_5|w_1,w_2,w_3,w_4)
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;A model that computes either of these &lt;span class=&quot;math inline&quot;&gt;\(P(W)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(P(w_n|w_1,w_2,...,w_{n-1})\)&lt;/span&gt; is called a &lt;code&gt;language model&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-chain-rule-of-probability&quot;&gt;The Chain Rule of Probability&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)
\]&lt;/span&gt; The chain rule in general: &lt;span class=&quot;math display&quot;&gt;\[
P(x_1,x_2,x_3,…,x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)…P(x_n|x_1,…,x_{n-1})
\]&lt;/span&gt; i.e. &lt;span class=&quot;math display&quot;&gt;\[
P(w_1w_2···w_n)=\prod_i P(w_i|w_1w_2···w_{i-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;markov-assumption&quot;&gt;Markov Assumption&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
P(w_1w_2···w_n)\approx \prod_i P(w_i|w_{i-k}···w_{i-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unigram model &lt;span class=&quot;math display&quot;&gt;\[
P(w_1w_2···w_n)\approx \prod_i P(w_i)
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Bigram model &lt;span class=&quot;math display&quot;&gt;\[
P(w_1w_2···w_n)\approx \prod_i P(w_i|w_{i-1})
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;N-gram model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;estimating-bigram-probabilities&quot;&gt;Estimating bigram probabilities&lt;/h2&gt;
&lt;p&gt;The Maximum Likelihood Estimate &lt;span class=&quot;math display&quot;&gt;\[
P(w_i|w_{i-1})=\frac{count(w_{i-1},w_i)}{count(w_{i-1})}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;practical-issues&quot;&gt;Practical Issues&lt;/h3&gt;
&lt;p&gt;do everything in log space&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoid underflow&lt;/li&gt;
&lt;li&gt;also adding is faster than multiplying &lt;span class=&quot;math display&quot;&gt;\[
p_1\times p_2\times p_3\times p_4=logp_1+logp_2+logp_3+logp_4
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;perplexity&quot;&gt;Perplexity&lt;/h2&gt;
&lt;p&gt;The best language model is the one that best predicts an unseen test set. Perplexity is the probability of the test set, normalized by the number of words: &lt;span class=&quot;math display&quot;&gt;\[
PP(W)=P(w_1w_2...w_N)^{-{1 \over N}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Chain rule:&lt;/strong&gt; &lt;span class=&quot;math display&quot;&gt;\[
PP(W)=\sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i|w_1...w_{i-1})}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For bigrams:&lt;/strong&gt; &lt;span class=&quot;math display&quot;&gt;\[
PP(W)=\sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i|w_{i-1})}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Minimizing perplexity is the same as maximizing probability.&lt;/p&gt;
</description>
				<pubDate>Thu, 17 Dec 2015 00:00:00 +0800</pubDate>
				<link>/nlp/2015/12/17/language-modeling.html</link>
				<guid isPermaLink="true">/nlp/2015/12/17/language-modeling.html</guid>
			</item>
		
			<item>
				<title>word2vec</title>
				<description>&lt;h2 id=&quot;main-idea-of-word2vec&quot;&gt;Main idea of word2vec&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Instead of capturing cooccurrence counts directly&lt;/li&gt;
&lt;li&gt;Predict surrounding words of every word&lt;/li&gt;
&lt;li&gt;Faster and can easily incorporate a new sentence/document or add a word to the vocabulary&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Predict surrounding words in a window of length &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt; of every word.&lt;/li&gt;
&lt;li&gt;Objective function: Maximize the &lt;span class=&quot;math inline&quot;&gt;\(log\)&lt;/span&gt; probability of any context word given the current center word: &lt;span class=&quot;math display&quot;&gt;\[
J(\theta)=\frac{1}{T} \sum_{t=1}^T \sum_{-c\leq j \leq c,j \neq 0} log p(w_{t+j}|w_t)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
p(w_{t+j}|w_t)=p(w_o|w_i)=\frac{exp(v_{w_o}&amp;#39;^T v_{w_i})}{\sum_{w=1}^W exp(v_w&amp;#39;^T v_{w_i})}
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(v\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(v&amp;#39;\)&lt;/span&gt; are &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt; vector representations of &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt; (so every word has two vectors)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;glove&quot;&gt;GloVe&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
J=\frac{1}{2} \sum_{ij} f(P_{ij})(w_i \cdot \tilde w_j-logP_{ij})^2
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 14 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/14/word2vec.html</link>
				<guid isPermaLink="true">/foundation/2015/12/14/word2vec.html</guid>
			</item>
		
	</channel>
</rss>
