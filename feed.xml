<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>An elder's memo.</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>LSTM A Search Space Odyssey</title>
				<description>&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This paper reports the results of a large scale study on variations of the LSTM architecture.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The most commonly used LSTM architecture (vanilla LSTM) performs reasonably well on various datasets and using any of eight possible modifications does not significantly improve the LSTM performance.&lt;/li&gt;
&lt;li&gt;Certain modifications such as coupling the input and forget gates or removing peephole connections simplify LSTM without significantly hurting performance.&lt;/li&gt;
&lt;li&gt;The forget gate and the output activation function are the critical components of the LSTM block. While the first is crucial for LSTM performance, the second is necessary whenever the cell state is unbounded.&lt;/li&gt;
&lt;li&gt;Learning rate and network size are the most crucial tunable LSTM hyperparameters. Suerprisingly, the use of momentum was found to be unimportant (in setting of online gradient descent). Gaussian noise on the inputs was found to be moderately helpful for TIMIT, but harmful for other datasets.&lt;/li&gt;
&lt;li&gt;The analysis of hyperparameter interactions revealed that even the highest measured interaction (between learning rate and network size) is quite small. This implies that the hyperparameters can be tuned independently. In particular, the learning rate can be calibrated first using a fairly small network, thus saving a lot of experimentation time.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Thu, 24 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/paper/2015/12/24/lstm-a-search-space-odyssey.html</link>
				<guid isPermaLink="true">/foundation/paper/2015/12/24/lstm-a-search-space-odyssey.html</guid>
			</item>
		
			<item>
				<title>Sentiment analysis</title>
				<description>&lt;p&gt;Sentiment analysis has many other names&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Opinion extraction&lt;/li&gt;
&lt;li&gt;Opinion mining&lt;/li&gt;
&lt;li&gt;Sentiment mining&lt;/li&gt;
&lt;li&gt;Subjectively analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;learning-sentiment-lexicons&quot;&gt;Learning Sentiment Lexicons&lt;/h2&gt;
&lt;h3 id=&quot;pointwise-mutual-information&quot;&gt;Pointwise Mutual Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Mutual information&lt;/strong&gt; between 2 random variables &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
I(X,Y)=\sum_x \sum_y P(x,y)log_2 \frac{P(x,y)}{P(x)P(y)}
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pointwise mutual information&lt;/strong&gt;: How much more do events &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(y\)&lt;/span&gt; co-occur than if they were independent? &lt;span class=&quot;math display&quot;&gt;\[
PMI(X,Y)=log_2 \frac{P(x,y)}{P(x)P(y)}
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;PMI between two words&lt;/strong&gt;: &lt;span class=&quot;math display&quot;&gt;\[
PMI(word_1, word_2)=log_2 \frac{P(word_1, word_2)}{P(word_1)P(word_2)}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;how-to-estimate-pointwise-mutual-information&quot;&gt;How to Estimate Pointwise Mutual Information&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Query search engine (Altavista)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(P(word)\)&lt;/span&gt; estimated by &lt;span class=&quot;math inline&quot;&gt;\(hits(word)/N\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(P(word_1, word_2)\)&lt;/span&gt; by &lt;span class=&quot;math inline&quot;&gt;\(hits(word_1 \text{ NEAR } word_2)/N^2\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
PMI(word_1, word_2)=log_2 \frac{hits(word_1\text{ NEAR } word_2)}{hits(word_1)hits(word_2)}
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Does phrase appear more with “poor” or “excellent” &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
Polarity(phrase)&amp;amp;=PMI(phrase, \text{&amp;quot;excellent&amp;quot;})-PMI(phrase, \text{&amp;quot;poor&amp;quot;})\\
&amp;amp;=log_2 \frac{hits(phrase \text{ NEAR &amp;quot;excellent&amp;quot;})}{hits(phrase)hits(\text{&amp;quot;excellent&amp;quot;})} - log_2 \frac{hits(phrase \text{ NEAR &amp;quot;poor&amp;quot;})}{hits(phrase)hits(\text{&amp;quot;poor&amp;quot;})} \\
&amp;amp;=log_2 \frac{hits(phrase \text{ NEAR &amp;quot;excellent&amp;quot;})}{hits(phrase)hits(\text{&amp;quot;excellent&amp;quot;})} \frac{hits(phrase \text{ NEAR &amp;quot;poor&amp;quot;})}{hits(phrase)hits(\text{&amp;quot;poor&amp;quot;})} \\
&amp;amp;=log_2\left( \frac{hits(phrase \text{ NEAR &amp;quot;excellent&amp;quot;})hits(\text{&amp;quot;poor&amp;quot;})}{hits(phrase \text{ NEAR &amp;quot;poor&amp;quot;})hits(\text{&amp;quot;excellent&amp;quot;})} \right)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;other-sentiment-tasks&quot;&gt;Other Sentiment Tasks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Finding sentiment of a sentence
&lt;ul&gt;
&lt;li&gt;The food was great but the service was awful.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Finding aspect/attribute/target of sentiment&lt;/li&gt;
&lt;li&gt;Detection of Friendliness&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Wed, 23 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/nlp/2015/12/23/sentiment-analysis.html</link>
				<guid isPermaLink="true">/foundation/nlp/2015/12/23/sentiment-analysis.html</guid>
			</item>
		
			<item>
				<title>Improving Word Representations via Global Context and Multiple Word Prototypes</title>
				<description>&lt;h2 id=&quot;global-context-aware-neural-language-model&quot;&gt;Global Context-Aware Neural Language Model&lt;/h2&gt;
&lt;h3 id=&quot;training-objective&quot;&gt;Training Objective&lt;/h3&gt;
&lt;p&gt;Given a word sequence &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; and document &lt;span class=&quot;math inline&quot;&gt;\(d\)&lt;/span&gt; in which the sequence occurs, our goal is to discriminate the correct last word in &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; from other random words. We compute scores &lt;span class=&quot;math inline&quot;&gt;\(g(s,d)\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(g(s^w,d)\)&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(s^w\)&lt;/span&gt; is &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; with the last word replaced by word &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(g(·,·)\)&lt;/span&gt; is the scoring function that represents the neural networks used. We want &lt;span class=&quot;math inline&quot;&gt;\(g(s,d)\)&lt;/span&gt; to be larger than &lt;span class=&quot;math inline&quot;&gt;\(g(s^w,d)\)&lt;/span&gt; by a margin of 1, for any other word &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt; in the vocabulary, which corresponds to the training objective of minimizing the ranking loss for each &lt;span class=&quot;math inline&quot;&gt;\((s,d)\)&lt;/span&gt; found in the corpus: &lt;span class=&quot;math display&quot;&gt;\[
C_{s,d}=\sum_{w\in V} max(0, 1-g(s,d)+g(s^w,d))
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;neural-network-architecture&quot;&gt;Neural Network Architecture&lt;/h3&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/27.png&quot; alt=&quot;An overview of the neural language model. The model makes use of both local and global context to compute a score that should be large for the actual next word (bank in the example), compare to the score for other words. When word meaning is still ambiguous given local context, information in global context can help disambiguation.&quot; /&gt;&lt;figcaption&gt;An overview of the neural language model. The model makes use of both local and global context to compute a score that should be large for the actual next word (bank in the example), compare to the score for other words. When word meaning is still ambiguous given local context, information in global context can help disambiguation.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The score of local context uses the local word sequence &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;. We first represent the word sequence &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; as an ordered list of vectors &lt;span class=&quot;math inline&quot;&gt;\(x=(x_1,x_2,...,x_m)\)&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; is the embedding of word &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; in the sequence, which is a column in the embedding matrix &lt;span class=&quot;math inline&quot;&gt;\(L\in \Bbb R^{n\times |V|}\)&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(|V|\)&lt;/span&gt; denotes the size of the vocabulary. &lt;strong&gt;The columns of this embedding matrix &lt;span class=&quot;math inline&quot;&gt;\(L\)&lt;/span&gt; are the word vectors and will be learned and updated during training.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To compute the score of local context, &lt;span class=&quot;math inline&quot;&gt;\(socre_l\)&lt;/span&gt;, use a neural network with one hidden layer: &lt;span class=&quot;math display&quot;&gt;\[
a_1=f(W_1[x_1;x_2;...;x_m]+b_1)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
score_l=W_2a_1+b_2
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\([x_1;x_2;...;x_m]\)&lt;/span&gt; is the concatenation of the &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; word embeddings representing sequence &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(f\)&lt;/span&gt; is an element-wise activation function such as &lt;span class=&quot;math inline&quot;&gt;\(tanh\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(a_1\in \Bbb R^{h\times 1}\)&lt;/span&gt; is the activation of the hidden layer with &lt;span class=&quot;math inline&quot;&gt;\(h\)&lt;/span&gt; hidden nodes, &lt;span class=&quot;math inline&quot;&gt;\(W_1\in \Bbb R^{h\times \\(mn\\)}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(W_2\in\Bbb R^{1\times h}\)&lt;/span&gt; are respectively the first and second layer weights of the neural network, and &lt;span class=&quot;math inline&quot;&gt;\(b_1\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(b_2\)&lt;/span&gt; are the biases of each layer.&lt;/p&gt;
&lt;p&gt;For the score of the global context, we represent the document also as an ordered list of word embedding, &lt;span class=&quot;math inline&quot;&gt;\(d=(d_1,d_2,...,d_k)\)&lt;/span&gt;. First compute the weighted average of all word vectors in the document: &lt;span class=&quot;math display&quot;&gt;\[
c=\frac{\sum_{i=1}^k w(t_i)d_i}{\sum_{i=1}^k w(t_i)}
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(w(·)\)&lt;/span&gt; can be any weighting function that captures the importance of word &lt;span class=&quot;math inline&quot;&gt;\(t_i\)&lt;/span&gt; in the document. We use idf-weighting as the weighting function. Use a two-layer neural network to compute the global context score, &lt;span class=&quot;math inline&quot;&gt;\(score_g\)&lt;/span&gt;, similar to the above: &lt;span class=&quot;math display&quot;&gt;\[
a_1^g=f(W_1^g[c;x_m]+b_1^g)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
score_g=W_2^ga_1^g+b_2^g
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\([c;x_m]\)&lt;/span&gt; is the concatenation of the weighted average document vector and the vector of the last word in &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note that instead of using the document where the sequence occurs, we can also specify a fixed &lt;span class=&quot;math inline&quot;&gt;\(k&amp;gt;m\)&lt;/span&gt; that captures larger context.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The final score is the sum of the two scores: &lt;span class=&quot;math display&quot;&gt;\[
score = score_l+score_g
\]&lt;/span&gt; The local score preserves word order and syntactic information, while the global score uses a weighted average which is similar to bag-of-words features, capturing more of the semantics and topics of the document.&lt;/p&gt;
&lt;h3 id=&quot;learning&quot;&gt;Learning&lt;/h3&gt;
&lt;p&gt;Word embeddings move to good positions in the vector space faster when using &lt;code&gt;mini-batch L-BFGS&lt;/code&gt; (Liu and Nocedal, 1989) with 1000 pairs of good and corrupt examples per batch for training, compared to stochastic gradient descent.&lt;/p&gt;
&lt;h2 id=&quot;multi-prototype-neural-language-model&quot;&gt;Multi-Prototype Neural Language Model&lt;/h2&gt;
&lt;p&gt;In order to learn multiple prototypes, we first gather the fixed-sized context windows of all occurrences of a word (we use 5 words before and after the word occurrence). Each context is represented by a weighted average of the context words’ vectors, where again, we use &lt;code&gt;idf-weighting&lt;/code&gt; as the weighting function. Then use &lt;code&gt;spherical k-means&lt;/code&gt; to cluster these context representations. Finally, each word occurrence in the corpus is re-labeled to its associated cluster and is used to train the word representation for that cluster.&lt;/p&gt;
&lt;p&gt;Similarity between a pair of words &lt;span class=&quot;math inline&quot;&gt;\((w, w&amp;#39;)\)&lt;/span&gt; using the multi-prototype approach can be computed with of without context, as defined by Reisinger and Mooney (2010b): &lt;span class=&quot;math display&quot;&gt;\[
AvgSimC(w,w&amp;#39;)={1 \over K^2}\sum_{i=1}^k \sum_{j=1}^k p(c,w,i)p(c&amp;#39;,w&amp;#39;,j)d(\mu_i(w), \mu_j(w&amp;#39;))
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(p(c,w,i)\)&lt;/span&gt; is the likelihood that word &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt; is in its cluser &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; given context &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(\mu_i(w)\)&lt;/span&gt; is the vector representing the &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;-th cluster centroid of &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt;, and &lt;span class=&quot;math inline&quot;&gt;\(d(v,v&amp;#39;)\)&lt;/span&gt; is a function computing similarity between two vectors, which can be any of the distance functions presented by Curran(2004). The similarity measure can be computed in absence of context by assuming uniform &lt;span class=&quot;math inline&quot;&gt;\(p(c,w,i)\)&lt;/span&gt; over &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
</description>
				<pubDate>Wed, 23 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/paper/2015/12/23/improving-word-representations-via-global-context-and-multiple-word-prototypes.html</link>
				<guid isPermaLink="true">/foundation/paper/2015/12/23/improving-word-representations-via-global-context-and-multiple-word-prototypes.html</guid>
			</item>
		
			<item>
				<title>Neural Reasoner</title>
				<description>&lt;h2 id=&quot;overview-of-neural-reasoner&quot;&gt;Overview of Neural Reasoner&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Neural Reasoner&lt;/strong&gt; has a layered architecture to deal with the complicated logical relations in reasoning as illustrated in Figure 1:&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/24.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;It consists of &lt;code&gt;one encoding layer&lt;/code&gt; and &lt;code&gt;multiple reasoning layers&lt;/code&gt;. The encoder layers first converts the question and facts from natural language sentences to vectorial representations. More specifically, &lt;span class=&quot;math display&quot;&gt;\[
Q \quad \underrightarrow{encode} \quad q^{(0)} , F_k \quad \underrightarrow{encode} \quad f_k^{(0)},k=1,2,...,K.
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(q^{(0)} \in \Bbb R^{d_Q}\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(f_k^{(0)} \in \Bbb R^{d_F}\)&lt;/span&gt;. With the representations obtained from the encoding layer, the reasoning layer recursively updates the representations of questions and facts, &lt;span class=&quot;math display&quot;&gt;\[
\{q^{(l)}f_1^{(l)}\dotsb f_K^{(l)}\}\quad \underrightarrow{reason} \quad\{q^{(l+1)}f_1^{(l+1)}\dotsb f_K^{(l+1)}\}
\]&lt;/span&gt; through the interaction between question representation and fact representations. Intuitively, this interaction models the reasoning, including examination of the facts and comparison of the facts and the questions. Finally at layer-&lt;span class=&quot;math inline&quot;&gt;\(L\)&lt;/span&gt;, the resulted question representation &lt;span class=&quot;math inline&quot;&gt;\(q^{(L)}\)&lt;/span&gt; is fed to an answerer, which layer can be a classifier for choosing between a number of pre-determined classes (e.g., {Yes, No}) or a text generator for create a sentence.&lt;/p&gt;
&lt;p&gt;Neural Reasoner has the following desired properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it can handle varying number of facts, including irrelevant ones, and reach the final conclusion through repeated processing of filtering and combining;&lt;/li&gt;
&lt;li&gt;it makes no assumption about the form of language, as long as enough training examples are given.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;model&quot;&gt;Model&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/25.png&quot; alt=&quot;A diagram of implementation of Neural Reasoner with L reasoning layers, operating on one question and K facts&quot; /&gt;&lt;figcaption&gt;A diagram of implementation of Neural Reasoner with L reasoning layers, operating on one question and K facts&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;auxiliary-training-for-questionfact-representation&quot;&gt;Auxiliary Training for Question/Fact Representation&lt;/h2&gt;
&lt;p&gt;Use auxiliary training to facilitate the learning of representations of question and facts. Basically, in addition to using the learned representations of question and facts in the reasoning process, also use those representations to reconstruct the original questions or their more abstract forms with variables.&lt;/p&gt;
&lt;p&gt;In the auxiliary training, intend to achieve the following two goals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;to compensate the lack of supervision in the learning task. In experiments, the supervision can be fairly weak since for each instance it is merely a classification with no more than 12 classes, while the number of instances are 1K to 10K.&lt;/li&gt;
&lt;li&gt;to introduce beneficial bias for the representation learning task. Since the network is a complicated nonlinear function, the back-propagation from the answering layer to the encoding layer can easily fail to learn well.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/26.png&quot; alt=&quot;Auxiliary training for question representation. The training for fact representation is identical and therefore omitted&quot; /&gt;&lt;figcaption&gt;Auxiliary training for question representation. The training for fact representation is identical and therefore omitted&lt;/figcaption&gt;
&lt;/figure&gt;
</description>
				<pubDate>Mon, 21 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/paper/2015/12/21/neural-reasoner.html</link>
				<guid isPermaLink="true">/foundation/paper/2015/12/21/neural-reasoner.html</guid>
			</item>
		
			<item>
				<title>OXFORD Machine Learning</title>
				<description>&lt;p&gt;&lt;a href=&quot;https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/&quot;&gt;Course Page&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;lecture-2-linear-supervised-learning&quot;&gt;Lecture 2 Linear supervised learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Many real processes can be &lt;em&gt;approximated&lt;/em&gt; with linear models.&lt;/li&gt;
&lt;li&gt;Linear regression often appears as a &lt;em&gt;module&lt;/em&gt; of larger systems.&lt;/li&gt;
&lt;li&gt;Linear problems can be solved &lt;em&gt;analytically&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Linear prediction provides an introduction to many of the &lt;em&gt;core concepts&lt;/em&gt; of machine learning.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/19.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;lecture-3&quot;&gt;Lecture 3&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/20.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/images/21.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/images/22.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/images/23.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/19/oxford-machine-learning.html</link>
				<guid isPermaLink="true">/foundation/2015/12/19/oxford-machine-learning.html</guid>
			</item>
		
			<item>
				<title>How to Choose a Neural Network</title>
				<description>&lt;figure&gt;
&lt;img src=&quot;/assets/images/18.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
</description>
				<pubDate>Sat, 19 Dec 2015 00:00:00 +0800</pubDate>
				<link>/method/2015/12/19/how-to-choose-a-neural-network.html</link>
				<guid isPermaLink="true">/method/2015/12/19/how-to-choose-a-neural-network.html</guid>
			</item>
		
			<item>
				<title>Basic conception</title>
				<description>&lt;h2 id=&quot;standard-deviation&quot;&gt;Standard Deviation&lt;/h2&gt;
&lt;p&gt;In statistics and probability theory, the standard deviation (SD) (represented by the Greek letter sigma, &lt;span class=&quot;math inline&quot;&gt;\(\sigma\)&lt;/span&gt;) measures the amount of variation or dispersion from the average. A low standard deviation indicates that the data points tend to be very close to the mean (also called expected value); a high standard deviation indicates that the data points are spread out over a large range of values.&lt;/p&gt;
&lt;h2 id=&quot;derivative&quot;&gt;Derivative&lt;/h2&gt;
&lt;p&gt;The derivative of a function of a real variable measures the sensitivity to change of a quantity (a function or dependent variable) which is determined by another quantity (the independent variable). For example, the derivative of the position of a moving object with respect to time is the object’s velocity: this measures how quickly the position of the object changes when time is advanced.当x变化时，y的变化量。&lt;/p&gt;
</description>
				<pubDate>Sat, 19 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/19/basic-conception.html</link>
				<guid isPermaLink="true">/foundation/2015/12/19/basic-conception.html</guid>
			</item>
		
			<item>
				<title>Theoretical Motivations for Deep Learning</title>
				<description>&lt;p&gt;With distributed representations, it is possible to represent exponential number of regions with a linear number of parameters. The magic of distributed representation is that it can learn a very complicated function(with many ups and downs) with a low number of examples.&lt;/p&gt;
&lt;p&gt;Depth is not necessary to have a flexible family of functions. Deeper networks does not correspond to a higher capacity. Deeper doesn’t mean we can represent more functions. If the functions we trying to learn has a particular characteristic obtained through composition of many operations, then it is much better to approximate these functions with a deep neural network.&lt;/p&gt;
&lt;p&gt;There is new theoretical result that deeper nets with rectifier/maxout units are exponentially more expressive than shallow ones because they can split the input space in many more linear regions, with constraints.&lt;/p&gt;
</description>
				<pubDate>Fri, 18 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/18/theoretical-motivations-for-deep-learning.html</link>
				<guid isPermaLink="true">/foundation/2015/12/18/theoretical-motivations-for-deep-learning.html</guid>
			</item>
		
			<item>
				<title>Jekyll render with pandoc on windows</title>
				<description>&lt;h2 id=&quot;下载绿色版jekyll&quot;&gt;下载绿色版Jekyll&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/sohero/PortableJekyll&quot; title=&quot;下载地址&quot;&gt;下载地址&lt;/a&gt;，这个版本，在原Portable Jekyll的基础上，修改了一下setpath.cmd。这样在写的正文里有中文也不会报错了。&lt;/li&gt;
&lt;li&gt;解压到任一目录，直接运行setpath.cmd，就可以使用了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装pandoc&quot;&gt;安装pandoc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jekyll的两个markdown解析器，对于mathjax的支持都不完美。用pandoc没有问题，不过需要安装jekyll-pandoc插件，&lt;a href=&quot;https://github.com/sohero/jekyll-pandoc&quot;&gt;插件下载地址&lt;/a&gt;。这个版本修改了一下依赖的Jekyll版本号，这样就可以在Jekyll 3.0里使用了。&lt;/li&gt;
&lt;li&gt;安装pandoc：&lt;a href=&quot;http://pandoc.org/&quot;&gt;下载地址&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Thu, 17 Dec 2015 00:00:00 +0800</pubDate>
				<link>/tools/log/2015/12/17/jekyll-render-with-pandoc-on-windows.html</link>
				<guid isPermaLink="true">/tools/log/2015/12/17/jekyll-render-with-pandoc-on-windows.html</guid>
			</item>
		
			<item>
				<title>git笔记 and cheatsheet</title>
				<description>&lt;p&gt;&lt;a class=&quot;btn btn-default&quot; href=&quot;https://github.com/sohero/git&quot; target=&quot;_blank&quot;&gt;Go.&lt;/a&gt; &lt;a class=&quot;btn btn-default&quot; href=&quot;http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000&quot; target=&quot;_blank&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 17 Dec 2015 00:00:00 +0800</pubDate>
				<link>/tools/log/2015/12/17/git-note.html</link>
				<guid isPermaLink="true">/tools/log/2015/12/17/git-note.html</guid>
			</item>
		
	</channel>
</rss>
