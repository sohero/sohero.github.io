<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>An elder's memo.</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>RL Concepts</title>
				<description>&lt;p&gt;&lt;strong&gt;experience&lt;/strong&gt;: sample sequences of &lt;em&gt;states&lt;/em&gt;, &lt;em&gt;actions&lt;/em&gt;, and &lt;em&gt;rewards&lt;/em&gt; from actual or simulated interaction with an environment.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nonstationary&lt;/strong&gt;: The return after taking an action in one state depends on the actions taken in later states in the same episode. Because all action selections are undergoing learning, the problem becomes nonstationary from the point view of the earlier state.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;prediction problem&lt;/strong&gt;: The computation of &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(q_\pi\)&lt;/span&gt; for a fixed solution policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;bootstrapping&lt;/strong&gt;: All of them update estimates of the values of states based on estimates of the values of successor states. That is, they update estimates on the basis of other estimates. We call this general idea &lt;em&gt;bootstrapping&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;exploring starts&lt;/strong&gt;: For policy evaluation to work for action values, we must assure continual exploration. One way to do this is by specifying that the episodes &lt;em&gt;start in a state-action pair&lt;/em&gt;, and that every pair has a nonzero probability of being selected as the start. This guarantees that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of &lt;em&gt;exploring starts&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-greedy&lt;/strong&gt;: most of the time they choose an action that maximal estimated action value, but with probability &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt; they instead select an action at random. That is, all nongreedy actions are given the minimal probability of selection, &lt;span class=&quot;math inline&quot;&gt;\({\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt;, and the remaining bulk of the probability, &lt;span class=&quot;math inline&quot;&gt;\(1-\varepsilon +{\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt;, is given to the greedy action. The &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-greedy policies are examples of &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-soft policies, defined as policies for which &lt;span class=&quot;math inline&quot;&gt;\(\pi(a|s)\ge {\epsilon \over |\mathcal A(\mathcal s)|}\)&lt;/span&gt; for all states and actions, for some &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon &amp;gt;0\)&lt;/span&gt;. Among &lt;span class=&quot;math inline&quot;&gt;\(\varepsilon\)&lt;/span&gt;-soft policies, $-greedy policies are in some sense those that are closest to greedy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;target policy&lt;/strong&gt;: The policy being learned about is called the &lt;em&gt;target policy&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;behavior policy&lt;/strong&gt;: The policy used to generate behavior is called the &lt;em&gt;behavior policy&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;off-policy learning&lt;/strong&gt;: Learning is from data “off” the target policy, and the overall process is termed &lt;em&gt;off-policy learning&lt;/em&gt;.&lt;/p&gt;
</description>
				<pubDate>Fri, 20 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/20/rl-concepts.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/20/rl-concepts.html</guid>
			</item>
		
			<item>
				<title>Temporal-Difference Learning</title>
				<description>&lt;ul&gt;
&lt;li&gt;TD methods learn directly from episodes of experience&lt;/li&gt;
&lt;li&gt;TD is &lt;em&gt;model-free&lt;/em&gt;: no knowledge of MDP transitions / rewards&lt;/li&gt;
&lt;li&gt;TD learns from &lt;em&gt;incomplete&lt;/em&gt; episodes, by &lt;em&gt;bootstrapping&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;TD updates a guess towards a guess&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;mc-and-td&quot;&gt;MC and TD&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: learn &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; online from experience under policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&quot;incremental-every-visit-monte-carlo&quot;&gt;Incremental every-visit Monte-Carlo&lt;/h3&gt;
&lt;p&gt;Update value &lt;span class=&quot;math inline&quot;&gt;\(V(S_t)\)&lt;/span&gt; toward &lt;em&gt;actual&lt;/em&gt; return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;simplest-temporal-difference-learning-algorithm-td0&quot;&gt;Simplest temporal-difference learning algorithm: TD(0)&lt;/h3&gt;
&lt;p&gt;Update value &lt;span class=&quot;math inline&quot;&gt;\(V(S_t)\)&lt;/span&gt; toward &lt;em&gt;estimated&lt;/em&gt; return &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma V(S_{t+1})\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))
\]&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}+\gamma V(S_{t+1})\)&lt;/span&gt; is called the &lt;em&gt;TD&lt;/em&gt; target.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)\)&lt;/span&gt; is called the &lt;em&gt;TD&lt;/em&gt; error.&lt;/p&gt;
&lt;h2 id=&quot;monte-carlo-backup&quot;&gt;Monte-Carlo Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/42.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;temporal-difference-backup&quot;&gt;Temporal-Difference Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/43.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;dynamic-programming-backup&quot;&gt;Dynamic Programming Backup&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/44.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;bootstrapping-and-sampling&quot;&gt;Bootstrapping and Sampling&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Bootstrapping&lt;/strong&gt;: update involves an estimate&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC does not bootstrap&lt;/li&gt;
&lt;li&gt;DP bootstraps&lt;/li&gt;
&lt;li&gt;TD bootstraps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Sampling&lt;/strong&gt;: update samples an expectation&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MC samples&lt;/li&gt;
&lt;li&gt;DP does not sample&lt;/li&gt;
&lt;li&gt;TD samples&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;unified-view-of-reinforcement-learning&quot;&gt;Unified View of Reinforcement Learning&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/45.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;eligibility-traces&quot;&gt;Eligibility Traces&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/46.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Credit assignment problem: did bell or light cause shock?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Frequency heuristic&lt;/strong&gt;: assign credit to most frequent states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Recency heuristic&lt;/strong&gt;: assign credit to most recent states.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Elegibility traces&lt;/strong&gt; combine both heuristics: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
E_0(s) &amp;amp;= 0 \\
E_t(s) &amp;amp;= \gamma\lambda E_{t-1}(s) + 1(S_t=s)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/47.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
</description>
				<pubDate>Thu, 19 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/19/temporal-difference-learning.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/19/temporal-difference-learning.html</guid>
			</item>
		
			<item>
				<title>Monte-Carlo Reinforcement Learning</title>
				<description>&lt;ul&gt;
&lt;li&gt;MC methods learn directly from episodes of experience&lt;/li&gt;
&lt;li&gt;MC is &lt;em&gt;model-free&lt;/em&gt;: no knowledge of MDP transitions / rewards&lt;/li&gt;
&lt;li&gt;MC learns from &lt;em&gt;complete&lt;/em&gt; episodes: no bootstrapping&lt;/li&gt;
&lt;li&gt;MC uses the simplest possible idea: value = mean return&lt;/li&gt;
&lt;li&gt;Caveat: can only apply MC to &lt;em&gt;episodic&lt;/em&gt; MDPs. All episodes must terminate.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;monte-carlo-policy-evaluation&quot;&gt;Monte-Carlo Policy Evaluation&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: learn &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt; from episodes of experience under policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Monte-Carlo policy evaluation uses &lt;code&gt;empirical mean return&lt;/code&gt; instead of &lt;em&gt;expected return&lt;/em&gt;.&lt;/p&gt;
&lt;h2 id=&quot;incremental-mean&quot;&gt;Incremental Mean&lt;/h2&gt;
&lt;p&gt;The mean &lt;span class=&quot;math inline&quot;&gt;\(\mu_1,\mu_2,...\)&lt;/span&gt; of a sequence &lt;span class=&quot;math inline&quot;&gt;\(x_1,x_2,...\)&lt;/span&gt; can be computed incrementally, &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
\mu_k &amp;amp;= {1 \over k}\sum_{j=1}^k x_j \\
&amp;amp;= {1 \over k}\left(x_k+\sum_{j=1}^{k-1} x_j\right) \\
&amp;amp;= {1 \over k}\left(x_k+(k-1)\mu_{k-1} \right) \\
&amp;amp;= \mu_{k-1} + {1 \over k}(x_k - \mu_{k-1})
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;incremental-monte-carlo-updates&quot;&gt;Incremental Monte-Carlo Updates&lt;/h2&gt;
&lt;p&gt;Update &lt;span class=&quot;math inline&quot;&gt;\(V(s)\)&lt;/span&gt; incrementally after episode &lt;span class=&quot;math inline&quot;&gt;\(S_1,A_1,R_2,...,S_T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;For each state &lt;span class=&quot;math inline&quot;&gt;\(S_t\)&lt;/span&gt; with return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
N(S_t) &amp;amp;\leftarrow N(S_t) + 1 \\
V(S_t) &amp;amp;\leftarrow V(S_t) + {1 \over N(S_t)}(G_t-V(S_t))
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In non-stationary problems, it can be useful to track a running mean, i.e. forget old episodes. &lt;span class=&quot;math display&quot;&gt;\[
V(S_t) \leftarrow V(S_t) + \alpha(G_t-V(S_t))
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 19 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/19/monte-carlo-reinforcement-learning.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/19/monte-carlo-reinforcement-learning.html</guid>
			</item>
		
			<item>
				<title>Dynamic Programming</title>
				<description>&lt;p&gt;Dynamic programming assumes full knowledge of the MDP. It is used for &lt;em&gt;planning&lt;/em&gt; in an MDP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For prediction:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Input: MDP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&amp;gt;\)&lt;/span&gt; and policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; or MRP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal P^\pi,\mathcal R^\pi,\gamma \right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Output: value function &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For control:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Input: MDP &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma \right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Output: optimal value function &lt;span class=&quot;math inline&quot;&gt;\(v_*\)&lt;/span&gt; and optimal policy &lt;span class=&quot;math inline&quot;&gt;\(\pi_*\)&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;iterative-policy-evaluation-more-precisely-policy-value-evaluation&quot;&gt;Iterative Policy Evaluation (more precisely Policy-Value Evaluation)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: evaluate a given policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;: iterative application of Bellman expectation backup &lt;span class=&quot;math display&quot;&gt;\[
v_1\to v_2 \to ... \to v_\pi
\]&lt;/span&gt; Using &lt;em&gt;synchronous&lt;/em&gt; backups,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;At each iteration &lt;span class=&quot;math inline&quot;&gt;\(k+1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;For all state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s \in \mathcal S\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Update &lt;span class=&quot;math inline&quot;&gt;\(v_{k+1}(\mathcal s)\)&lt;/span&gt; from &lt;span class=&quot;math inline&quot;&gt;\(v_k(\mathcal s&amp;#39;)\)&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s&amp;#39;\)&lt;/span&gt; is a successor state of &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}(\mathcal s)=\sum_{a\in\mathcal A} \pi(a|\mathcal s)\left(\mathcal R_{\mathcal s}^a+\gamma\sum_{\mathcal s&amp;#39;\in\mathcal S}\mathcal P^a_{\mathcal s\mathcal s&amp;#39;}v_k(\mathcal s&amp;#39;)\right)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v^{k+1}=\mathcal R^\pi + \gamma\mathcal P^\pi v^k
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;how-to-improve-a-policy&quot;&gt;How to Improve a Policy&lt;/h2&gt;
&lt;p&gt;Given a policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;, &lt;strong&gt;evaluate&lt;/strong&gt; the policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
v_\pi(s)=\Bbb E[R_{t+1}+\gamma R_{t+2}+...|S_t=s]
\]&lt;/span&gt; &lt;strong&gt;Improve&lt;/strong&gt; the policy by acting greedily with respect to &lt;span class=&quot;math inline&quot;&gt;\(v_\pi\)&lt;/span&gt;: &lt;span class=&quot;math display&quot;&gt;\[
\pi &amp;#39;=greedy(v_\pi)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/41.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;h2 id=&quot;deterministic-value-iteration&quot;&gt;Deterministic Value Iteration&lt;/h2&gt;
&lt;p&gt;If we know the solution to subproblems &lt;span class=&quot;math inline&quot;&gt;\(v_*(s&amp;#39;)\)&lt;/span&gt;. Then solution &lt;span class=&quot;math inline&quot;&gt;\(v_*(s)\)&lt;/span&gt; can be found by one-step lookahead: &lt;span class=&quot;math display&quot;&gt;\[
v_*(s)\leftarrow\max_{a\in\mathcal A}\mathcal R_s^a+\gamma\sum_{s&amp;#39;\in\mathcal S}v_*(s&amp;#39;)
\]&lt;/span&gt; The idea of value iteration is to apply these updates iteratively.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt;: start with final rewards and word backwards. &lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}(s)=\max_{a\in\mathcal A}\left(\mathcal R_s^a+\gamma\sum_{s&amp;#39;\in\mathcal S}\mathcal R_{ss&amp;#39;}^av_k(s&amp;#39;)\right)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v_{k+1}=\max_{a\in\mathcal A}\mathcal R^a+\gamma\mathcal P^av_k
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;synchronous-dynamic-programming-algorithms&quot;&gt;Synchronous Dynamic Programming Algorithms&lt;/h2&gt;
&lt;table style=&quot;width:50%;&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 11%&quot; /&gt;
&lt;col style=&quot;width: 23%&quot; /&gt;
&lt;col style=&quot;width: 15%&quot; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&quot;header&quot;&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Problem&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Bellman Equation&lt;/th&gt;
&lt;th style=&quot;text-align: left;&quot;&gt;Algorithm&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Prediction&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Expectation Equation&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Iterative Policy Evaluation&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;even&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Control&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Expectation Equation + Greedy Policy Improvement&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Policy Iteration&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&quot;odd&quot;&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Control&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Bellman Optimally Equation&lt;/td&gt;
&lt;td style=&quot;text-align: left;&quot;&gt;Value Iteration&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
				<pubDate>Tue, 17 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/17/dynamic-programming.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/17/dynamic-programming.html</guid>
			</item>
		
			<item>
				<title>Recurrent Neural Networks with External Memory</title>
				<description>&lt;p&gt;&lt;a class=&quot;btn btn-default&quot; href=&quot;https://github.com/npow/RNN-EM&quot; target=&quot;_blank&quot;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;external-memory-read&quot;&gt;External memory read&lt;/h2&gt;
&lt;p&gt;RNN-EM has an external memory &lt;span class=&quot;math inline&quot;&gt;\(M_t \in R^{m\times n}\)&lt;/span&gt;. It can be considered as a memory with &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; slots and each slot is a vector with &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt; elements. Similar to the external memory in computers, the memory capacity of RNN-EM may be increased if using a large &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The model generates a key vector &lt;span class=&quot;math inline&quot;&gt;\(k_t\)&lt;/span&gt; to search for content in the external memory. Though there are many possible ways to generate the key vector, we choose a simple linear function that relates hidden layer activity &lt;span class=&quot;math inline&quot;&gt;\(h_t\)&lt;/span&gt; as follows &lt;span class=&quot;math display&quot;&gt;\[
k_t=W_kh_t
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(W_k\in R^{m\times p}\)&lt;/span&gt; is a linear transformation matrix. Our intuition is that the memory should be in the same space of or affine to the hidden layer activity.&lt;/p&gt;
&lt;p&gt;We use cosine distance &lt;span class=&quot;math inline&quot;&gt;\(K(u,v)={u\cdot v \over ||u||||v||}\)&lt;/span&gt; to compare this key vector with contents in the external memory. The weight for the &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt;-th slot &lt;span class=&quot;math inline&quot;&gt;\(M_t(:,c)\)&lt;/span&gt; in memory &lt;span class=&quot;math inline&quot;&gt;\(M_t\)&lt;/span&gt; is computed as follows &lt;span class=&quot;math display&quot;&gt;\[
\hat w_t(c)=\frac{exp\beta_tK(k_t,M_t(:,c))}{\sum_qexp\beta_tK(k_t,M_t(:,q))}
\]&lt;/span&gt; where the above weight is normalized and sums to 1.0. &lt;span class=&quot;math inline&quot;&gt;\(\beta_t\)&lt;/span&gt; is a scalar larger than 0.0. It sharpens the weight vector when &lt;span class=&quot;math inline&quot;&gt;\(\beta_t\)&lt;/span&gt; is larger than 1.0. Conversely, it smooths or dampens the weight vector when &lt;span class=&quot;math inline&quot;&gt;\(\beta_t\)&lt;/span&gt; is between 0.0 and 1.0. We use the following function to obtain &lt;span class=&quot;math inline&quot;&gt;\(\beta_t\)&lt;/span&gt;; &lt;span class=&quot;math display&quot;&gt;\[
\beta_t=log(1+exp(W_\beta h_t))
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(W_\beta\in R^{1\times p}\)&lt;/span&gt; maps the hidden layer activity &lt;span class=&quot;math inline&quot;&gt;\(h_t\)&lt;/span&gt; to a scalar.&lt;/p&gt;
&lt;p&gt;Importantly, we also use a scalar coefficient &lt;span class=&quot;math inline&quot;&gt;\(g_t\)&lt;/span&gt; to interpolate the above weight estimate with the past weight as follows &lt;span class=&quot;math display&quot;&gt;\[
w_t=(1-g_t)w_{t-1}+g_t\hat w_t
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The memory content is retrieved from the external memory at time &lt;span class=&quot;math inline&quot;&gt;\(t-1\)&lt;/span&gt; using &lt;span class=&quot;math display&quot;&gt;\[
c_t=M_{t-1}w_{t-1}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;external-memory-update&quot;&gt;External memory update&lt;/h2&gt;
&lt;p&gt;RNN-EM generates a new content vector &lt;span class=&quot;math inline&quot;&gt;\(v_t\)&lt;/span&gt; to be added to its memory &lt;span class=&quot;math display&quot;&gt;\[
v_t=W_vh_t
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(W_v\in R^{m\times p}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;RNN-EM has a forget gate as follows &lt;span class=&quot;math display&quot;&gt;\[
f_t=1-w_t\odot e_t
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(e_t\in R^{n\times 1}\)&lt;/span&gt; is an erase vector, generated as &lt;span class=&quot;math inline&quot;&gt;\(e_t=\sigma(W_{he}h_t)\)&lt;/span&gt;. Notice that the &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt;-th element in the forget gate is zero only if both read weight &lt;span class=&quot;math inline&quot;&gt;\(w_t\)&lt;/span&gt; and erase vector &lt;span class=&quot;math inline&quot;&gt;\(e_t\)&lt;/span&gt; have their &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt;-th element set to one. Therefore, memory cannot be forgotten if it is not to be read.&lt;/p&gt;
&lt;p&gt;RNN-EM has an update gate &lt;span class=&quot;math inline&quot;&gt;\(u_t\)&lt;/span&gt;. It simply uses the weight &lt;span class=&quot;math inline&quot;&gt;\(w_t\)&lt;/span&gt; as follows &lt;span class=&quot;math display&quot;&gt;\[
u_t=w_t
\]&lt;/span&gt; Therefore, memory is only updated if it is to be read.&lt;/p&gt;
&lt;p&gt;With the above described two gates, the memory is updated as follows &lt;span class=&quot;math display&quot;&gt;\[
M_t=diag(f_t)M_{t-1}+diag(u_t)v_t
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(diag(\cdot)\)&lt;/span&gt; transforms a vector to a diagonal matrix with diagonal elements from the vector.&lt;/p&gt;
&lt;p&gt;Notice that when the number of memory slots is small, it may have similar performances as a geted RNN.&lt;/p&gt;
</description>
				<pubDate>Tue, 03 May 2016 00:00:00 +0800</pubDate>
				<link>/deep%20learning/rnn/2016/05/03/rnn-em.html</link>
				<guid isPermaLink="true">/deep%20learning/rnn/2016/05/03/rnn-em.html</guid>
			</item>
		
			<item>
				<title>Markov Decision Processes</title>
				<description>&lt;h2 id=&quot;rewards&quot;&gt;Rewards&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;code&gt;reward&lt;/code&gt; &lt;span class=&quot;math inline&quot;&gt;\(R_t\)&lt;/span&gt; is a scalar feedback signal&lt;/li&gt;
&lt;li&gt;Indecates how well agent is doing at step &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The agent’s job is to maximise cumulative reward&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reinforcement learning is based on the &lt;code&gt;reward hypothesis&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;All goals can be described by the maximisation of expected cumulative reward&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;information-statea.k.a-markov-state&quot;&gt;Information State(a.k.a Markov state)&lt;/h2&gt;
&lt;p&gt;A state &lt;span class=&quot;math inline&quot;&gt;\(S_t\)&lt;/span&gt; is &lt;code&gt;Markov&lt;/code&gt; if and only if &lt;span class=&quot;math display&quot;&gt;\[
\Bbb P[S_{t+1}|S_t]=\Bbb P[S_{t+1}|S_1,...,S_t]
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The future is independent of the past given the present&lt;/li&gt;
&lt;li&gt;Once the state is known, the history may be thrown away&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;markov-decision-processes&quot;&gt;Markov Decision Processes&lt;/h2&gt;
&lt;h3 id=&quot;markov-processes&quot;&gt;Markov Processes&lt;/h3&gt;
&lt;h4 id=&quot;state-transition-matrix&quot;&gt;State Transition Matrix&lt;/h4&gt;
&lt;p&gt;For a Markov state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt; and successor state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s&amp;#39;\)&lt;/span&gt;, the &lt;em&gt;state transition probability&lt;/em&gt; is defined by &lt;span class=&quot;math display&quot;&gt;\[
\mathcal P_{\mathcal s\mathcal s&amp;#39;}=\Bbb P[\mathcal S_{t+1}=\mathcal s&amp;#39;|\mathcal S_t=\mathcal s]
\]&lt;/span&gt; State transition matrix &lt;span class=&quot;math inline&quot;&gt;\(\mathcal P\)&lt;/span&gt; defines transition probabilities from all states &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt; to all successor states &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s&amp;#39;\)&lt;/span&gt;, &lt;span class=&quot;math display&quot;&gt;\[
\mathcal P=\text{from}
\begin{array}{c}
\text{to} \\
\begin{bmatrix}
\mathcal P_{11} &amp;amp; \cdots &amp;amp; \mathcal P_{1n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\mathcal P_{n1} &amp;amp; \cdots &amp;amp; \mathcal P_{nn}
\end{bmatrix}
\end{array}
\]&lt;/span&gt; where each row of the matrix sums to 1.&lt;/p&gt;
&lt;h4 id=&quot;markov-process&quot;&gt;Markov Process&lt;/h4&gt;
&lt;p&gt;A Markov process is a memoryless random process, i.e. a sequence of random states &lt;span class=&quot;math inline&quot;&gt;\(\mathcal S_1,\mathcal S_2,...\)&lt;/span&gt; with the Markov property.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;Markov Process(or Markov Chain)&lt;/em&gt; is a tuple &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal P\right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal S\)&lt;/span&gt; is a (finite) set of states&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal P\)&lt;/span&gt; is state transition probability matrix&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;markov-reward-process&quot;&gt;Markov Reward Process&lt;/h3&gt;
&lt;p&gt;A Markov reward process is a Markov chain with values.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;Markov Reward Process&lt;/em&gt; is a tuple &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal P,\mathcal R,\gamma\right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal S\)&lt;/span&gt; is a finite set of states&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal P\)&lt;/span&gt; is a state transition probability matrix&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal R\)&lt;/span&gt; is a reward function, &lt;span class=&quot;math inline&quot;&gt;\(\mathcal R_s=\Bbb E[R_{t+1}|\mathcal S_t=\mathcal s]\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\gamma\)&lt;/span&gt; is a discount factor, &lt;span class=&quot;math inline&quot;&gt;\(\gamma \in [0,1]\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;return&quot;&gt;Return&lt;/h4&gt;
&lt;p&gt;The return &lt;span class=&quot;math inline&quot;&gt;\(G_t\)&lt;/span&gt;(Goal) is the total discounted reward from time-step &lt;span class=&quot;math inline&quot;&gt;\(t\)&lt;/span&gt;. &lt;span class=&quot;math display&quot;&gt;\[
G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^\infty \gamma^kR_{t+k+1}
\]&lt;/span&gt; The discount &lt;span class=&quot;math inline&quot;&gt;\(\gamma\in[0,1]\)&lt;/span&gt; is the present value of future rewords.&lt;/p&gt;
&lt;h4 id=&quot;value-function&quot;&gt;Value Function&lt;/h4&gt;
&lt;p&gt;The value function &lt;span class=&quot;math inline&quot;&gt;\(v(s)\)&lt;/span&gt; gives the long-term value of state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;state value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(v(s)\)&lt;/span&gt; of an MRP is the expected &lt;code&gt;Return&lt;/code&gt; starting from state &lt;span class=&quot;math inline&quot;&gt;\(\mathcal s\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v(s)=\Bbb E[G_t|\mathcal S_t=\mathcal s]
\]&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/39.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/38.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/40.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;The reward signal indicates what is good in an immediate sense, a value function specifies what is good in the long run. Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;bellman-equation-for-mrps&quot;&gt;Bellman Equation for MRPs&lt;/h4&gt;
&lt;p&gt;The value function can be decomposed into two parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;immediate reward &lt;span class=&quot;math inline&quot;&gt;\(R_{t+1}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;discounted value of successor state &lt;span class=&quot;math inline&quot;&gt;\(\gamma v(S_{t+1})\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
v(s) &amp;amp;= \Bbb E[G_t|S_t=s] \\
&amp;amp;= \Bbb E[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...|S_t=s] \\
&amp;amp;= \Bbb E[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+...)|S_t=s] \\
&amp;amp;= \Bbb E[R_{t+1}+\gamma G_{t+1}|S_t=s] \\
&amp;amp;= \Bbb E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]
\end{align}
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v(s)=\mathcal R_s+\gamma \sum_{s&amp;#39;\in\mathcal S} \mathcal P_{ss&amp;#39;}v(s&amp;#39;)
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;bellman-equation-in-matrix-form&quot;&gt;Bellman Equation in Matrix Form&lt;/h4&gt;
&lt;p&gt;The Bellman equation can be expressed concisely using matrices, &lt;span class=&quot;math display&quot;&gt;\[
v=\mathcal R+\gamma \mathcal Pv
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(v\)&lt;/span&gt; is a column vector with one entry per state &lt;span class=&quot;math display&quot;&gt;\[
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
=
\begin{bmatrix}
\mathcal R_1 \\
\vdots \\
\mathcal R_n
\end{bmatrix}
+\gamma \begin{bmatrix}
\mathcal P_{11} &amp;amp; \cdots &amp;amp; \mathcal P_{1n} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\mathcal P_{n1} &amp;amp; \cdots &amp;amp; \mathcal P_{nn}
\end{bmatrix}
\begin{bmatrix}
v(1) \\
\vdots \\
v(n)
\end{bmatrix}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;sloving-the-bellman-equation&quot;&gt;Sloving the Bellman Equation&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;The Bellman equation is a linear equation&lt;/li&gt;
&lt;li&gt;It can be solved directly: &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
v &amp;amp;= \mathcal R+\gamma\mathcal Pv \\
(I-\gamma\mathcal P)v &amp;amp;=\mathcal R \\
v &amp;amp;= (I-\gamma\mathcal P)^{-1}\mathcal R
\end{align}
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Computational complexity is &lt;span class=&quot;math inline&quot;&gt;\(O(n^3)\)&lt;/span&gt; for &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt; states&lt;/li&gt;
&lt;li&gt;Direct solution only possible for small MRPs&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;markov-decision-process&quot;&gt;Markov Decision Process&lt;/h3&gt;
&lt;p&gt;A Markov decision process(MDP) is a Markov reward process with decisions. It is an environment in which all states are Markov.&lt;/p&gt;
&lt;p&gt;A Markov Decision Process is a tuple &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal P,\mathcal R,\gamma\right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal A\)&lt;/span&gt; is a finite set of actions&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;policies&quot;&gt;Policies&lt;/h4&gt;
&lt;p&gt;A policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; is a distribution over actions given states &lt;span class=&quot;math display&quot;&gt;\[
\pi(a|s)=\Bbb P[A_t=a|S_t=s]
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A policy fully defines the behaviour of an agent&lt;/li&gt;
&lt;li&gt;MDP policies depend on the current state (not the history)&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&quot;value-function-1&quot;&gt;Value Function&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;state-value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(v_\pi(s)\)&lt;/span&gt; of an MDP is the expected return starting from state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, and then following policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
v_\pi(s)=\Bbb E_\pi[G_t|S_t=s]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;action-value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(q_\pi(s,a)\)&lt;/span&gt; is the expected return starting from state &lt;span class=&quot;math inline&quot;&gt;\(s\)&lt;/span&gt;, taking action &lt;span class=&quot;math inline&quot;&gt;\(a\)&lt;/span&gt;, and then following policy &lt;span class=&quot;math inline&quot;&gt;\(\pi\)&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
q_\pi(s,a)=\Bbb E_\pi[G_t|S_t=s,A_t=a]
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;bellman-expectation-equation&quot;&gt;Bellman Expectation Equation&lt;/h4&gt;
&lt;p&gt;The state-value function can again be decomposed into immediate reward plus discounted value of successor state &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
v_\pi(s) &amp;amp;= \Bbb E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s] \\
&amp;amp;=\sum_{a\in\mathcal A} \pi(a|s)q_\pi(s,a)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The action-value function can similarly be decomposed, &lt;span class=&quot;math display&quot;&gt;\[
\begin{align}
q_\pi(s,a) &amp;amp;= \Bbb E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1},A_{t+1})|S_t=s,A_t=a] \\
&amp;amp;= \mathcal R_s^a + \gamma\sum_{s&amp;#39;\in\mathcal S} \mathcal P_{ss&amp;#39;}^a v_\pi(s&amp;#39;)
\end{align}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;bellman-expectation-equation-matrix-form&quot;&gt;Bellman Expectation Equation (Matrix Form)&lt;/h4&gt;
&lt;p&gt;The Bellman expectation equation can be expressed concisely using the induced MRP, &lt;span class=&quot;math display&quot;&gt;\[
v_\pi=\mathcal R^\pi + \gamma \mathcal P^\pi v_\pi
\]&lt;/span&gt; with direct solution &lt;span class=&quot;math display&quot;&gt;\[
v_\pi=(I-\gamma\mathcal P^\pi)^{-1}\mathcal R^\pi
\]&lt;/span&gt;&lt;/p&gt;
&lt;h4 id=&quot;optimal-value-function&quot;&gt;Optimal Value Function&lt;/h4&gt;
&lt;p&gt;The &lt;em&gt;optimal state-value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(v_*(s)\)&lt;/span&gt; is the maximum value function over all policies &lt;span class=&quot;math display&quot;&gt;\[
v_*(s)=max_\pi v_\pi(s)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;optimal action-value function&lt;/em&gt; &lt;span class=&quot;math inline&quot;&gt;\(q_*(s,a)\)&lt;/span&gt; is the maximum action-value function over all policies &lt;span class=&quot;math display&quot;&gt;\[
q_*(s,a)=max_\pi q_\pi(s,a)
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The optimal value function specifies the best possible performance in the MDP.&lt;/li&gt;
&lt;li&gt;An MDP is “solved” when we know the optimal value fn.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;partially-observable-mdps-pomdps&quot;&gt;Partially Observable MDPs (POMDPs)&lt;/h3&gt;
&lt;p&gt;A Partially Observable Markov Decision Process is an MDP with hidden states. It is a hidden Markov model with actions.&lt;/p&gt;
&lt;p&gt;A POMDP is a tuple &lt;span class=&quot;math inline&quot;&gt;\(\left&amp;lt;\mathcal S,\mathcal A,\mathcal O,\mathcal P,\mathcal R,\mathcal Z,\gamma\right&amp;gt;\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal O\)&lt;/span&gt; is a finite set of observations&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(\mathcal Z\)&lt;/span&gt; is an observation function, &lt;span class=&quot;math display&quot;&gt;\[
\mathcal Z_{s&amp;#39;o}^a=\Bbb P[O_{t+1}=o|S_{t+1}=s&amp;#39;,A_t=a]
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Tue, 03 May 2016 00:00:00 +0800</pubDate>
				<link>/reinforcement%20learning/2016/05/03/markov-decision-processes.html</link>
				<guid isPermaLink="true">/reinforcement%20learning/2016/05/03/markov-decision-processes.html</guid>
			</item>
		
			<item>
				<title>A Full Hardware Guide to Deep Learning</title>
				<description>&lt;p&gt;&lt;a class=&quot;btn btn-default&quot; href=&quot;http://timdettmers.com/2015/03/09/deep-learning-hardware-guide/&quot; target=&quot;_blank&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt;: GTX 680 or GTX 960 (no money); GTX 980 (best performance); GTX Titan (if you need memory); GTX 970 (no convolutional nets)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CPU&lt;/strong&gt;: Two threads per GPU; full 40 PCIe lanes and correct PCIe spec (same as your motherboard); &amp;gt; 2GHz; cache does not matter;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RAM&lt;/strong&gt;: Use asynchronous mini-batch allocation; clock rate and timings do not matter; buy at least as much CPU RAM as you have GPU RAM;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hard drive/SSD&lt;/strong&gt;: Use asynchronous batch-file reads and compress your data if you have image or sound data; a hard drive will be fine unless you work with 32 bit floating point data sets with large input dimensions&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Power supply unit(PSU)&lt;/strong&gt;: Add up watts of GPUs + CPU + (100-300) for required power; get high efficiency rating if you use large conv nets; make sure it has enough PCIe connectors (6+8pins) and watts for your (future) GPUs&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Cooling&lt;/strong&gt;: Set coolbits flag in your config if you run a single GPU; otherwise flashing BIOS for increased fan speeds is easiest and cheapest; use water cooling for multiple GPUs and/or when you need to keep down the noise (you work with other people in the same room)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;: Get PCIe 3.0 and as many slots as you need for your (future) GPUs (one GPU takes two slots; max 4 GPUs per system)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Monitors&lt;/strong&gt;: If you want to upgrade your system to be more productive, it might make more sense to buy an additional monitor rather than upgrading your GPU&lt;/p&gt;
</description>
				<pubDate>Wed, 13 Apr 2016 00:00:00 +0800</pubDate>
				<link>/deep%20learning/tools/2016/04/13/a-full-hardware-guide-to-deep-learning.html</link>
				<guid isPermaLink="true">/deep%20learning/tools/2016/04/13/a-full-hardware-guide-to-deep-learning.html</guid>
			</item>
		
			<item>
				<title>Introduction to debugging neural networks</title>
				<description>&lt;p&gt;&lt;a class=&quot;btn btn-default&quot; href=&quot;http://russellsstewart.com/notes/0.html&quot; target=&quot;_blank&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The following advice is targeted at beginners to neural networks, and is based on my experience giving advice to neural net newcomers in industry and at Stanford. Neural nets are fundamentally harder to debug than most programs, because most neural net bugs don’t result in type errors or runtime errors. They just cause poor convergence. Especially when you’re new, this can be very frustrating! But an experienced neural net trainer will be able to systematically overcome the difficulty in spite of the ubiquitous and seemingly ambiguous error message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Performance Error: your neural net did not train well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To the uninitiated, the message is daunting. But to the experienced, this is a great error. It means the boilerplate coding is out of the way, and it’s time to dig in!&lt;/p&gt;
&lt;h2 id=&quot;how-to-deal-with-nans&quot;&gt;How to deal with NaNs&lt;/h2&gt;
&lt;p&gt;By far the most common first question I get from students is, “Why am I getting NaNs.” Occasionally, this has a complicated answer. But most often, the NaNs come in the first 100 iterations, and the answer is simple: your learning rate is too high. When the learning rate is very high, you will get NaNs in the first 100 iterations of training. Try reducing the learning rate by a factor of 3 until you no longer get NaNs in the first 100 iterations. As soon as this works, you’ll have a pretty good learning rate to get started with. In my experience, the best heavily validated learning rates are 1-10x below the range where you get NaNs.&lt;/p&gt;
&lt;p&gt;If you are getting NaNs beyond the first 100 iterations, there are 2 further common causes. 1) If you are using RNNs, make sure that you are using “gradient clipping”, which caps the global L2 norm of the gradients. RNNs tend to produce gradients early in training where 10% or fewer of the batches have learning spikes, where the gradient magnitude is very high. Without clipping, these spikes can cause NaNs. 2) If you have written any custom layers yourself, there is a good chance your own custom layer is causing the problems in a division by zero scenario. Another notoriously NaN producing layer is the softmax layer. The softmax computation involves an exp(x) term in both the numerator and denominator, which can divide Inf by Inf and produce NaNs. Make sure you are using a stabilized softmax implementation.&lt;/p&gt;
&lt;h2 id=&quot;what-to-do-when-your-neural-net-isnt-learning-anything&quot;&gt;What to do when your neural net isn’t learning anything&lt;/h2&gt;
&lt;p&gt;Once you stop getting NaNs, you are often rewarded with a neural net that runs smoothly for many thousand iterations, but never reduces the training loss after the initial fidgeting of the first few hundred iterations. When you’re first constructing your code base, waiting for more than 2000 iterations is rarely the answer. This is not because all networks can start learning in under 2000 iterations. Rather, the chance you’ve introduced a bug when coding up a network from scratch is so high that you’ll want to go into a special early debugging mode before waiting on high iteration counts. The name of the game here is to reduce the scope of the problem over and over again until you have a network that trains in less than 2000 iterations. Fortunately, there are always 2 good dimensions to reduce complexity.&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;Reduce the size of the training set to 10 instances. Working neural nets can usually overfit to 10 instances within just a few hundred iterations. Many coding bugs will prevent this from happening. If you’re network is not able to overfit to 10 instances of the training set, make sure your data and labels are hooked up correctly. Try reducing the batch size to 1 to check for batch computation errors. Add print statements throughout the code to make sure things look like you expect. Usually, you’ll be able to find these bugs through sheer brute force. Once you can train on 10 instances, try training on&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&quot;100&quot; type=&quot;1&quot;&gt;
&lt;li&gt;If this works okay, but not great, you’re ready for the next step.&lt;/li&gt;
&lt;/ol&gt;
&lt;ol start=&quot;2&quot; type=&quot;1&quot;&gt;
&lt;li&gt;Solve the simplest version of the problem that you’re interested in. If you’re translating sentences, try to build a language model for the target language first. Once that works, try to predict the first word of the translation given only the first 3 words of the source. If you’re trying to detect objects in images, try classifying the number of objects in each image before training a regression network. There is a trade-off between getting a good sub-problem you’re sure the network can solve, and spending the least amount of time plumbing the code to hook up the appropriate data. Creativity will help here.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The trick to scaling up a neural net for a new idea is to slowly relax the simplifications made in the above two steps. This is a form of coordinate ascent, and it works great. First, you show that the neural net can at least memorize a few examples. Then you show that it’s able to really generalize to the validation set on a dumbed down version of the problem. You slowly up the difficulty while making steady progress. It’s not as fun as hotshotting it the first time Karpathy style, but at least it works. At some point, you’ll find the problem is difficult enough that it can no longer be learned in 2000 iterations. That’s great! But it should rarely take more than 10 times the iterations of the previous complexity level of the problem. If you’re finding that to be the case, try to search for an intermediate level of complexity.&lt;/p&gt;
&lt;h2 id=&quot;tuning-hyperparameters&quot;&gt;Tuning hyperparameters&lt;/h2&gt;
&lt;p&gt;Now that your networks is learning things, you’re probably in pretty good shape. But you may find that your network is just not capable of solving the most difficult versions of your problem. Hyperparameter tuning will be key here. Some people who just download a CNN package and ran it on their dataset will tell you hyperparameter tuning didn’t make a difference. Realize that they’re solving an existing problem with an existing architecture. If you’re solving a new problem that demands a new architecture, hyperparameter tuning to get within the ballpark of a good setting is a must. You’re best bet is to read a hyperparameter tutorial for your specific problem, but I’ll list a few basic ideas here for completeness.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualization is key. Don’t be afraid to take the time to write yourself nice visualization tools throughout training. If your method of visualization is watching the loss bump around from the terminal, consider an upgrade.&lt;/li&gt;
&lt;li&gt;Weight initializations are important. Generally, larger magnitude initial weights are a good idea, but too large will get you NaNs. Thus, weight initialization will need to be simultaneously tuned with the learning rate.&lt;/li&gt;
&lt;li&gt;Make sure the weights look “healthy”. To learn what this means, I recommend opening weights from existing networks in an ipython notebook. Take some time to get used to what weight histograms should look like for your components in mature nets trained on standard datasets like ImageNet or the Penn Tree Bank.&lt;/li&gt;
&lt;li&gt;Neural nets are not scale invariant w.r.t. inputs, especially when trained with SGD rather than second order methods, as SGD is not a scale-invariant method. Take the time to scale your input data and output labels in the same way that others before you have scaled them.&lt;/li&gt;
&lt;li&gt;Decreasing your learning rate towards the end of training will almost always give you a boost. The best decay schedules usually take the form: after k epochs, divide the learning rate by 1.5 every n epochs, where &lt;span class=&quot;math inline&quot;&gt;\(k &amp;gt; n\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Use hyperparameter config files, although it’s okay to put hyperparameters in the code until you start trying out different values. I use json files that I load in with a command line argument as in &lt;a href=&quot;https://github.com/Russell91/tensorbox&quot; class=&quot;uri&quot;&gt;https://github.com/Russell91/tensorbox&lt;/a&gt;, but the exact format is not important. Avoid the urge to refactor your code as it becomes a hyperparameter loading mess! Refactors introduce bugs that cost you training cycles, and can be avoided until after you have a network you like.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Randomize your hyperparameter search if you can afford it. Random search generates hyperparmeter combinations you wouldn’t have thought of and removes a great deal of effort once your intuition is already trained on how to think about the impact of a given hyperparameter.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Debugging neural nets can be more laborious than traditional programs because almost all errors get projected onto the single dimension of overall network performance. Nonetheless, binary search is still your friend. By alternately 1) changing the difficulty of your problem, and 2) using a small number of training examples, you can quickly work through the initial bugs. Hyperparameter tuning and long periods of diligent waiting will get you the rest of the way.&lt;/p&gt;
</description>
				<pubDate>Mon, 11 Apr 2016 00:00:00 +0800</pubDate>
				<link>/deep%20learning/tricks/2016/04/11/introduction-to-debugging-neural-networks.html</link>
				<guid isPermaLink="true">/deep%20learning/tricks/2016/04/11/introduction-to-debugging-neural-networks.html</guid>
			</item>
		
			<item>
				<title>Iterables, Iterators and Generators</title>
				<description>&lt;h2 id=&quot;iterators&quot;&gt;Iterators&lt;/h2&gt;
&lt;p&gt;Iterators are iterables with some kind of ‘position’ state and a &lt;code&gt;.__next__()&lt;/code&gt;method. The &lt;code&gt;.__next__()&lt;/code&gt; method may be called to produce the next item and update the internal state.&lt;/p&gt;
&lt;p&gt;Iterables are objects that produce an iterator when they are passed to the &lt;code&gt;iter()&lt;/code&gt; builtin.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/36.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Calling &lt;code&gt;iter()&lt;/code&gt; on our “classic” iterable object produces a plain iterator instance&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;i &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;iter&lt;/span&gt;(lucky)
&lt;span class=&quot;op&quot;&gt;&amp;lt;&lt;/span&gt;iterator at &lt;span class=&quot;bn&quot;&gt;0x288cc10&lt;/span&gt;&lt;span class=&quot;op&quot;&gt;&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;a-better-iterable-class&quot;&gt;A better iterable class&lt;/h3&gt;
&lt;p&gt;You can choose the iterator that will be returned by &lt;code&gt;iter()&lt;/code&gt; by defining your own &lt;code&gt;.__iter__()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;span class=&quot;kw&quot;&gt;class&lt;/span&gt; Countdown(&lt;span class=&quot;bu&quot;&gt;object&lt;/span&gt;):
    &lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;__iter__&lt;/span&gt;(&lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;): &lt;span class=&quot;co&quot;&gt;# must return an iterator!&lt;/span&gt;
        &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bu&quot;&gt;iter&lt;/span&gt;([&lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;#39;launch&amp;#39;&lt;/span&gt;])&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&quot;iterators-the-hard-way&quot;&gt;Iterators the hard way&lt;/h3&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;span class=&quot;kw&quot;&gt;class&lt;/span&gt; CountdownIterator(&lt;span class=&quot;bu&quot;&gt;object&lt;/span&gt;):
    &lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;__init__&lt;/span&gt;(&lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;):
        &lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;._remaining &lt;span class=&quot;op&quot;&gt;=&lt;/span&gt; [&lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;#39;launch&amp;#39;&lt;/span&gt;]

    &lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;__iter__&lt;/span&gt;(&lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;):
        &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;

    &lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;__next__&lt;/span&gt;(&lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;):
        &lt;span class=&quot;cf&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;op&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;._remaining:
            &lt;span class=&quot;cf&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;pp&quot;&gt;StopIteration&lt;/span&gt;
        &lt;span class=&quot;cf&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;._remaining.pop(&lt;span class=&quot;dv&quot;&gt;0&lt;/span&gt;)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id=&quot;generators&quot;&gt;Generators&lt;/h2&gt;
&lt;p&gt;A generator function is a simpler way to create an iterator.&lt;/p&gt;
&lt;p&gt;Generator functions let you use local variables and the position of the program counter as state for a generator object. A new generator object is created and returned each time you call a generator function. The generator object is an iterator.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/37.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; countdown_generator():
    &lt;span class=&quot;cf&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;cf&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;st&quot;&gt;&amp;#39;launch&amp;#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The &lt;code&gt;.__iter__()&lt;/code&gt; method of our class can be written as a generator function:&lt;/p&gt;
&lt;div class=&quot;sourceCode&quot;&gt;&lt;pre class=&quot;sourceCode python&quot;&gt;&lt;code class=&quot;sourceCode python&quot;&gt;&lt;span class=&quot;kw&quot;&gt;class&lt;/span&gt; Countdown(&lt;span class=&quot;bu&quot;&gt;object&lt;/span&gt;):
    &lt;span class=&quot;kw&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;fu&quot;&gt;__iter__&lt;/span&gt;(&lt;span class=&quot;va&quot;&gt;self&lt;/span&gt;):
        &lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; n &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; [&lt;span class=&quot;dv&quot;&gt;5&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;4&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;3&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;2&lt;/span&gt;, &lt;span class=&quot;dv&quot;&gt;1&lt;/span&gt;, &lt;span class=&quot;st&quot;&gt;&amp;#39;launch&amp;#39;&lt;/span&gt;]:
            &lt;span class=&quot;cf&quot;&gt;yield&lt;/span&gt; n

&lt;span class=&quot;cf&quot;&gt;for&lt;/span&gt; n &lt;span class=&quot;op&quot;&gt;in&lt;/span&gt; Countdown():
    &lt;span class=&quot;bu&quot;&gt;print&lt;/span&gt;(n)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</description>
				<pubDate>Fri, 08 Apr 2016 00:00:00 +0800</pubDate>
				<link>/python/2016/04/08/iterables-iterators-and-generators.html</link>
				<guid isPermaLink="true">/python/2016/04/08/iterables-iterators-and-generators.html</guid>
			</item>
		
			<item>
				<title>GloVe</title>
				<description>&lt;blockquote&gt;
&lt;p&gt;Glove uses the matrix of word-word co-occurrence to produce the word vector, while word2vec using the local window context.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;strong&gt;statistics of word occurrences&lt;/strong&gt; in a corpus is the &lt;strong&gt;primary source&lt;/strong&gt; of information available to all unsupervised methods for &lt;strong&gt;learning word representations&lt;/strong&gt;, and although many such methods now exist, the question still remains as to &lt;strong&gt;how meaning is generated from these statistics&lt;/strong&gt;, and &lt;strong&gt;how the resulting word vectors might represent that meaning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; denote the matrix of word-word co-occurrence counts, the entries &lt;span class=&quot;math inline&quot;&gt;\(X_{ij}\)&lt;/span&gt; tabulate the number of times word &lt;span class=&quot;math inline&quot;&gt;\(j\)&lt;/span&gt; occurs in the context of word &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;. Let &lt;span class=&quot;math inline&quot;&gt;\(X_i=\sum_k X_{ik}\)&lt;/span&gt; be the nubmer of times any word appears in the context of word &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;. Let &lt;span class=&quot;math inline&quot;&gt;\(P_{ij}=P(j|i)={X_{ij} \over X_i}\)&lt;/span&gt; be the probability that word &lt;span class=&quot;math inline&quot;&gt;\(j\)&lt;/span&gt; appear in the context of word &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We take &lt;span class=&quot;math inline&quot;&gt;\(i=ice\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(j=steam\)&lt;/span&gt;, for words &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; related to &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; but not &lt;span class=&quot;math inline&quot;&gt;\(j\)&lt;/span&gt;, say &lt;span class=&quot;math inline&quot;&gt;\(k=solid\)&lt;/span&gt;, we expect the ratio &lt;span class=&quot;math inline&quot;&gt;\(P_{ik} \over P_{jk}\)&lt;/span&gt; will be large. Similarly, say &lt;span class=&quot;math inline&quot;&gt;\(k=gas\)&lt;/span&gt;, the ratio should be small. For words &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; like &lt;span class=&quot;math inline&quot;&gt;\(water\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(fashion\)&lt;/span&gt;, that are either related to both &lt;span class=&quot;math inline&quot;&gt;\(ice\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(steam\)&lt;/span&gt;, or to neither, the ratio should be close to &lt;strong&gt;one&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Noting that the ratio &lt;span class=&quot;math inline&quot;&gt;\(P_{ik} \over P_{jk}\)&lt;/span&gt; depends on three words &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(j\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt;, the most general model takes the form: &lt;span class=&quot;math display&quot;&gt;\[
F(w_i,w_j,\tilde w_k)={P_{ik} \over P_{jk}}\tag{1}
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(w\in\Bbb R^d\)&lt;/span&gt; are word vectors and &lt;span class=&quot;math inline&quot;&gt;\(\tilde w\in\Bbb R^d\)&lt;/span&gt; are separate context word vectors. &lt;span class=&quot;math inline&quot;&gt;\(F\)&lt;/span&gt; may depend on some as-of-yet unspecified parameters, and the number of possibilites for &lt;span class=&quot;math inline&quot;&gt;\(F\)&lt;/span&gt; is vast.&lt;/p&gt;
&lt;p&gt;Since &lt;strong&gt;vector spaces are inherently linear structures&lt;/strong&gt;, the most natural way to do this is with vector differences. With this aim, we can restrict our consideration to those functions &lt;span class=&quot;math inline&quot;&gt;\(F\)&lt;/span&gt; that depend only on the difference of the two target words, modifying Eqn.(1) to: &lt;span class=&quot;math display&quot;&gt;\[
F(w_i-w_j,\tilde w_k)={P_{ik}\over P_{jk}}\tag{2}
\]&lt;/span&gt; note that the arguments of &lt;span class=&quot;math inline&quot;&gt;\(F\)&lt;/span&gt; are vectors while the right-hand side is a scalar. While &lt;span class=&quot;math inline&quot;&gt;\(F\)&lt;/span&gt; could be taken to be a complicated function parameterized by, e.g., a neural network, doing so would obfuscate the linear structure we are trying to capture. To avoid this issue, we can first take the dot product of the arguments: &lt;span class=&quot;math display&quot;&gt;\[
F((w_i-w_j)^T\tilde w_k)={P_{ik}\over P_{jk}}\tag{3}
\]&lt;/span&gt; which prevents &lt;span class=&quot;math inline&quot;&gt;\(F\)&lt;/span&gt; from mixing the vector dimensions in undesirable ways.&lt;/p&gt;
&lt;p&gt;Note that for word-word co-occurrence matrices, the distinction between a word a context word is arbitrary and that we are free to exchange the two roles. To do so consistently, we must not only exchange &lt;span class=&quot;math inline&quot;&gt;\(w\leftrightarrow \tilde w\)&lt;/span&gt; but also &lt;span class=&quot;math inline&quot;&gt;\(X\leftrightarrow X^T\)&lt;/span&gt;. Our final model should be invariant under this relabeling, but Eqn.(3) is not. However, the &lt;strong&gt;symmetry&lt;/strong&gt; can be restored in two steps. First, we require that &lt;span class=&quot;math inline&quot;&gt;\(F\)&lt;/span&gt; be a homomorphism between the group &lt;span class=&quot;math inline&quot;&gt;\((\Bbb R, +)\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\((\Bbb R_{&amp;gt;0}, \times)\)&lt;/span&gt;, i.e., &lt;span class=&quot;math display&quot;&gt;\[
F((w_i-w_j)^T\tilde w_k)={F(w^T_i\tilde w_k)\over F(w^T_j\tilde w_k)}\tag{4}
\]&lt;/span&gt; which, by Eqn.(3), is solved by: &lt;span class=&quot;math display&quot;&gt;\[
F(w^T_i\tilde w_k)=P_{ik}={X_{ik}\over X_i}\tag{5}
\]&lt;/span&gt; The solution to Eqn.(4) is &lt;span class=&quot;math inline&quot;&gt;\(F=exp\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
e^{(w_i-w_j)^T\tilde w_k}={e^{w^T_i\tilde w_k}\over e^{w^T_j\tilde w_k}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or:&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
w^T_i\tilde w_k=log(P_{ik})=log(X_{ik})-log(X_i)\tag{6}
\]&lt;/span&gt; Note that Eqn.(6) would exhibit the exchange symmetry if not for the &lt;span class=&quot;math inline&quot;&gt;\(log(X_i)\)&lt;/span&gt; on the right-hand side. However, this term is independent for &lt;span class=&quot;math inline&quot;&gt;\(k\)&lt;/span&gt; so it can be absorbed into a bias &lt;span class=&quot;math inline&quot;&gt;\(b_i\)&lt;/span&gt; for &lt;span class=&quot;math inline&quot;&gt;\(w_i\)&lt;/span&gt;. Finally, adding an additional bias &lt;span class=&quot;math inline&quot;&gt;\(\tilde b_k\)&lt;/span&gt; for &lt;span class=&quot;math inline&quot;&gt;\(\tilde w_k\)&lt;/span&gt; restores the symmetry: &lt;span class=&quot;math display&quot;&gt;\[
w^T_i\tilde w_k+b_i+\tilde b_k=log(X_{ik})\tag{7}
\]&lt;/span&gt; Eqn.(7) is a drastic simplification over Eqn.(1), but it is actually ill-defined since the logarithm diverges whenever its argument is zero. One resolution to this issue is to include an additive shift in the logarithm, &lt;span class=&quot;math inline&quot;&gt;\(log(X_{ik})\rightarrow log(1+X_{ik})\)&lt;/span&gt;, which maintains the sparsity of &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; while avoiding the divergences.&lt;/p&gt;
&lt;p&gt;A main drawback to this model is that it weighs all co-occurrences equally, even those that happen rarely or never. Such rare co-occurrences are noisy and carry less information than the more frequent ones —— yet even just the zero entries account for 75-95% of the data in &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt;, depending on the vocabulary size and corpus.&lt;/p&gt;
&lt;p&gt;They propose a new weighted least squares regression model that addresses these problems. Casting Eqn.(7) as a &lt;strong&gt;least squares problem&lt;/strong&gt; and introducing a weighting function &lt;span class=&quot;math inline&quot;&gt;\(f(X_{ij})\)&lt;/span&gt; into the cost function gives us the model: &lt;span class=&quot;math display&quot;&gt;\[
J=\sum^V_{i,j=1} f(X_{ij})(w^T_i\tilde w_j+b_i+\tilde b_j-logX_{ij})^2\tag{8}
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(V\)&lt;/span&gt; is the size of the vocabulary. The weighting function should obey the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(f(0)=0\)&lt;/span&gt;. If &lt;span class=&quot;math inline&quot;&gt;\(f\)&lt;/span&gt; is viewed as a continuous function, it should vanish as &lt;span class=&quot;math inline&quot;&gt;\(x\rightarrow 0\)&lt;/span&gt; fast enough that the &lt;span class=&quot;math inline&quot;&gt;\(\lim_{x\to 0}f(x)log^2x\)&lt;/span&gt; is finite.&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(f(x)\)&lt;/span&gt; should be non-decreasing so that rare co-occurrences are not overweighted.&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(f(x)\)&lt;/span&gt; should be relatively small for large values of &lt;span class=&quot;math inline&quot;&gt;\(x\)&lt;/span&gt;, so that frequent co-occurrences are not overweighted.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
f(x)=\begin{cases}
(x/x_{max})^\alpha &amp;amp; \text{if $x&amp;lt;x_{max}$} \\\\
1 &amp;amp; \text{otherwise}
\end{cases}\tag{9}
\]&lt;/span&gt;&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/35.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;The performance of the model depends weakly on the cutoff, which we fix to &lt;span class=&quot;math inline&quot;&gt;\(x_{max}=100\)&lt;/span&gt; for all our experiments. We found that &lt;span class=&quot;math inline&quot;&gt;\(\alpha=3/4\)&lt;/span&gt; gives a modest improvement over a linear version with &lt;span class=&quot;math inline&quot;&gt;\(\alpha=1\)&lt;/span&gt;. Although we offer only empirical motivation for choosing the value 3/4, it is interesting that a similar fractional power scaling was found to give the best performance in (Mikolove et al., 2013a).&lt;/p&gt;
</description>
				<pubDate>Mon, 28 Mar 2016 00:00:00 +0800</pubDate>
				<link>/foundation/2016/03/28/glove.html</link>
				<guid isPermaLink="true">/foundation/2016/03/28/glove.html</guid>
			</item>
		
	</channel>
</rss>
