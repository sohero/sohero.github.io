<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
	<channel>
		<title></title>
		<description>An elder's memo.</description>
		<link>/</link>
		<atom:link href="/feed.xml" rel="self" type="application/rss+xml" />
		
			<item>
				<title>Theoretical Motivations for Deep Learning</title>
				<description>&lt;p&gt;With distributed representations, it is possible to represent exponential number of regions with a linear number of parameters. The magic of distributed representation is that it can learn a very complicated function(with many ups and downs) with a low number of examples.&lt;/p&gt;
&lt;p&gt;Depth is not necessary to have a flexible family of functions. Deeper networks does not correspond to a higher capacity. Deeper doesn’t mean we can represent more functions. If the functions we trying to learn has a particular characteristic obtained through composition of many operations, then it is much better to approximate these functions with a deep neural network.&lt;/p&gt;
&lt;p&gt;There is new theoretical result that deeper nets with rectifier/maxout units are exponentially more expressive than shallow ones because they can split the input space in many more linear regions, with constraints.&lt;/p&gt;
</description>
				<pubDate>Fri, 18 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/18/theoretical-motivations-for-deep-learning.html</link>
				<guid isPermaLink="true">/foundation/2015/12/18/theoretical-motivations-for-deep-learning.html</guid>
			</item>
		
			<item>
				<title>Jekyll render with pandoc on windows</title>
				<description>&lt;h2 id=&quot;下载绿色版jekyll&quot;&gt;下载绿色版Jekyll&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/sohero/PortableJekyll&quot; title=&quot;下载地址&quot;&gt;下载地址&lt;/a&gt;，这个版本，在原Portable Jekyll的基础上，修改了一下setpath.cmd。这样在写的正文里有中文也不会报错了。&lt;/li&gt;
&lt;li&gt;解压到任一目录，直接运行setpath.cmd，就可以使用了。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;安装pandoc&quot;&gt;安装pandoc&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jekyll的两个markdown解析器，对于mathjax的支持都不完美。用pandoc没有问题，不过需要安装jekyll-pandoc插件，&lt;a href=&quot;https://github.com/sohero/jekyll-pandoc&quot;&gt;插件下载地址&lt;/a&gt;。这个版本修改了一下依赖的Jekyll版本号，这样就可以在Jekyll 3.0里使用了。&lt;/li&gt;
&lt;li&gt;安装pandoc：&lt;a href=&quot;http://pandoc.org/&quot;&gt;下载地址&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Thu, 17 Dec 2015 00:00:00 +0800</pubDate>
				<link>/tools/log/2015/12/17/jekyll-render-with-pandoc-on-windows.html</link>
				<guid isPermaLink="true">/tools/log/2015/12/17/jekyll-render-with-pandoc-on-windows.html</guid>
			</item>
		
			<item>
				<title>git笔记 and cheatsheet</title>
				<description>&lt;p&gt;&lt;a class=&quot;btn btn-default&quot; href=&quot;https://github.com/sohero/git&quot;&gt;Go.&lt;/a&gt; &lt;a class=&quot;btn btn-default&quot; href=&quot;http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000&quot;&gt;原文&lt;/a&gt;&lt;/p&gt;
</description>
				<pubDate>Thu, 17 Dec 2015 00:00:00 +0800</pubDate>
				<link>/tools/log/2015/12/17/git-note.html</link>
				<guid isPermaLink="true">/tools/log/2015/12/17/git-note.html</guid>
			</item>
		
			<item>
				<title>Language Modeling</title>
				<description>&lt;ul&gt;
&lt;li&gt;Goal: compute the probability of a sentence or a sequence of words: &lt;span class=&quot;math display&quot;&gt;\[
P(W)=P(w_1,w_2,w_3,...,w_n)
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Related task: probability of an upcoming word: &lt;span class=&quot;math display&quot;&gt;\[
p(w_5|w_1,w_2,w_3,w_4)
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;A model that computes either of these &lt;span class=&quot;math inline&quot;&gt;\(P(W)\)&lt;/span&gt; or &lt;span class=&quot;math inline&quot;&gt;\(P(w_n|w_1,w_2,...,w_{n-1})\)&lt;/span&gt; is called a &lt;code&gt;language model&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;the-chain-rule-of-probability&quot;&gt;The Chain Rule of Probability&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
P(A,B,C,D)=P(A)P(B|A)P(C|A,B)P(D|A,B,C)
\]&lt;/span&gt; The chain rule in general: &lt;span class=&quot;math display&quot;&gt;\[
P(x_1,x_2,x_3,…,x_n) = P(x_1)P(x_2|x_1)P(x_3|x_1,x_2)…P(x_n|x_1,…,x_{n-1})
\]&lt;/span&gt; i.e. &lt;span class=&quot;math display&quot;&gt;\[
P(w_1w_2···w_n)=\prod_i P(w_i|w_1w_2···w_{i-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;markov-assumption&quot;&gt;Markov Assumption&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
P(w_1w_2···w_n)\approx \prod_i P(w_i|w_{i-k}···w_{i-1})
\]&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Unigram model &lt;span class=&quot;math display&quot;&gt;\[
P(w_1w_2···w_n)\approx \prod_i P(w_i)
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Bigram model &lt;span class=&quot;math display&quot;&gt;\[
P(w_1w_2···w_n)\approx \prod_i P(w_i|w_{i-1})
\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;N-gram model&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;estimating-bigram-probabilities&quot;&gt;Estimating bigram probabilities&lt;/h2&gt;
&lt;p&gt;The Maximum Likelihood Estimate &lt;span class=&quot;math display&quot;&gt;\[
P(w_i|w_{i-1})=\frac{count(w_{i-1},w_i)}{count(w_{i-1})}
\]&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&quot;practical-issues&quot;&gt;Practical Issues&lt;/h3&gt;
&lt;p&gt;do everything in log space&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Avoid underflow&lt;/li&gt;
&lt;li&gt;also adding is faster than multiplying &lt;span class=&quot;math display&quot;&gt;\[
p_1\times p_2\times p_3\times p_4=logp_1+logp_2+logp_3+logp_4
\]&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;perplexity&quot;&gt;Perplexity&lt;/h2&gt;
&lt;p&gt;The best language model is the one that best predicts an unseen test set. Perplexity is the probability of the test set, normalized by the number of words: &lt;span class=&quot;math display&quot;&gt;\[
PP(W)=P(w_1w_2...w_N)^{-{1 \over N}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Chain rule:&lt;/strong&gt; &lt;span class=&quot;math display&quot;&gt;\[
PP(W)=\sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i|w_1...w_{i-1})}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For bigrams:&lt;/strong&gt; &lt;span class=&quot;math display&quot;&gt;\[
PP(W)=\sqrt[N]{\prod_{i=1}^N \frac{1}{P(w_i|w_{i-1})}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Minimizing perplexity is the same as maximizing probability.&lt;/p&gt;
</description>
				<pubDate>Thu, 17 Dec 2015 00:00:00 +0800</pubDate>
				<link>/nlp/2015/12/17/language-modeling.html</link>
				<guid isPermaLink="true">/nlp/2015/12/17/language-modeling.html</guid>
			</item>
		
			<item>
				<title>word2vec</title>
				<description>&lt;h2 id=&quot;main-idea-of-word2vec&quot;&gt;Main idea of word2vec&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Instead of capturing cooccurrence counts directly&lt;/li&gt;
&lt;li&gt;Predict surrounding words of every word&lt;/li&gt;
&lt;li&gt;Faster and can easily incorporate a new sentence/document or add a word to the vocabulary&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;details&quot;&gt;Details&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Predict surrounding words in a window of length &lt;span class=&quot;math inline&quot;&gt;\(c\)&lt;/span&gt; of every word.&lt;/li&gt;
&lt;li&gt;Objective function: Maximize the &lt;span class=&quot;math inline&quot;&gt;\(log\)&lt;/span&gt; probability of any context word given the current center word: &lt;span class=&quot;math display&quot;&gt;\[
J(\theta)=\frac{1}{T} \sum_{t=1}^T \sum_{-c\leq j \leq c,j \neq 0} log p(w_{t+j}|w_t)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
p(w_{t+j}|w_t)=p(w_o|w_i)=\frac{exp(v_{w_o}&amp;#39;^T v_{w_i})}{\sum_{w=1}^W exp(v_w&amp;#39;^T v_{w_i})}
\]&lt;/span&gt; where &lt;span class=&quot;math inline&quot;&gt;\(v\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(v&amp;#39;\)&lt;/span&gt; are &lt;code&gt;input&lt;/code&gt; and &lt;code&gt;output&lt;/code&gt; vector representations of &lt;span class=&quot;math inline&quot;&gt;\(w\)&lt;/span&gt; (so every word has two vectors)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;glove&quot;&gt;GloVe&lt;/h2&gt;
&lt;p&gt;&lt;span class=&quot;math display&quot;&gt;\[
J=\frac{1}{2} \sum_{ij} f(P_{ij})(w_i \cdot \tilde w_j-logP_{ij})^2
\]&lt;/span&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 14 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/14/word2vec.html</link>
				<guid isPermaLink="true">/foundation/2015/12/14/word2vec.html</guid>
			</item>
		
			<item>
				<title>sense2vec - a fast and accurate method for word sense disambiguation in neural word embeddings</title>
				<description>&lt;p&gt;Despite these advancements, most word embedding techniques share a common problem in that each word must encode all of its potential meanings into a single vector.&lt;/p&gt;
&lt;p&gt;This technique is inspired by the work of Huang et al. (2012), which uses a multi-prototype neural vector-space model that clusters contexts to generate prototypes. Given a pre-trained word embedding model, each context embedding is generated by computing a weighted sum of the words in the context (weighted by tf-idf). Then, for each term, the associated context embeddings are clustered. The clusters are used to re-label each occurrence of each word in the corpus. Once these terms have been re-labeled with the cluster’s number, a new word model is trained on the labeled embeddings (with a different vector for each) generating the word-sense embeddings.&lt;/p&gt;
&lt;p&gt;We expand on the work of Huang et al. (2012) by leveraging supervised NLP labels instead of unsupervised clusters to determine a particular word nstance’s sense. This eliminates the need to train embeddings multiple times, eliminates the need for a clustering step, and creates an efficient method by which a supervised classifier may consume the appropriate word-sense embedding.&lt;/p&gt;
&lt;p&gt;Given a labeled corpus (either by hand or by a model) with one or more labels per word, the sense2vec model first counts the number of uses (where a unique word maps set of one or more labels/uses) of each word and generates a random ”sense embedding” for each use. A model is then trained using either the CBOW, Skip-gram, or Structured Skip-gram model onfigurations. Instead of predicting a token given surrounding tokens, this model predicts a word sense given surrounding senses. &lt;img src=&quot;/assets/images/16.jpg&quot; alt=&quot;sense2vec&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 14 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/paper/2015/12/14/sense2vec-a-fast-and-accurate-method-for-word-sense-disambiguation-in-neural-word-embeddings.html</link>
				<guid isPermaLink="true">/foundation/paper/2015/12/14/sense2vec-a-fast-and-accurate-method-for-word-sense-disambiguation-in-neural-word-embeddings.html</guid>
			</item>
		
			<item>
				<title>End-To-End Memory Networks</title>
				<description>&lt;h2 id=&quot;single-layer&quot;&gt;Single Layer&lt;/h2&gt;
&lt;p&gt;A layer has two memroy: &lt;code&gt;input memory&lt;/code&gt;,&lt;code&gt;output memory&lt;/code&gt;. Parameters are &lt;span class=&quot;math inline&quot;&gt;\(A \in \Bbb R^{d \times |V|}\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(B \in \Bbb R^{d \times |V|}\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(C \in \Bbb R^{d \times |V|}\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(W \in \Bbb R^{|V| \times d}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Input set &lt;span class=&quot;math inline&quot;&gt;\(x_1,...,x_i\)&lt;/span&gt; (one hot encoding or distribute encoding? probably the one hot.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Input memory&lt;/strong&gt; The input memory represented by &lt;span class=&quot;math inline&quot;&gt;\(\\{m_i\\}\)&lt;/span&gt;, the &lt;span class=&quot;math inline&quot;&gt;\(m_i\)&lt;/span&gt; is computed by &lt;span class=&quot;math inline&quot;&gt;\(Ax_i\)&lt;/span&gt;, i.e. each &lt;span class=&quot;math inline&quot;&gt;\(m_i\)&lt;/span&gt; is transformed from &lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; using &lt;span class=&quot;math inline&quot;&gt;\(A\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Query&lt;/strong&gt; The query &lt;span class=&quot;math inline&quot;&gt;\(q\)&lt;/span&gt; is transformed to the internal state &lt;span class=&quot;math inline&quot;&gt;\(u\)&lt;/span&gt; using &lt;span class=&quot;math inline&quot;&gt;\(Bq\)&lt;/span&gt;. Then, compute the match probability &lt;span class=&quot;math inline&quot;&gt;\(p_i\)&lt;/span&gt; between &lt;span class=&quot;math inline&quot;&gt;\(u\)&lt;/span&gt; and each memory &lt;span class=&quot;math inline&quot;&gt;\(m_i\)&lt;/span&gt; using softmax: &lt;span class=&quot;math display&quot;&gt;\[
p_i=softmax(u^Tm_i)
\]&lt;/span&gt; so &lt;span class=&quot;math inline&quot;&gt;\(p\)&lt;/span&gt; is a probability vecotr over the inputs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Output memory&lt;/strong&gt; Each &lt;span class=&quot;math inline&quot;&gt;\(x_i\)&lt;/span&gt; is transformed to a output vecotr &lt;span class=&quot;math inline&quot;&gt;\(c_i\)&lt;/span&gt; by &lt;span class=&quot;math inline&quot;&gt;\(Cx_i\)&lt;/span&gt;. The response vector &lt;span class=&quot;math inline&quot;&gt;\(o\)&lt;/span&gt; from the memory computed by: &lt;span class=&quot;math display&quot;&gt;\[
o=\sum_{i} p_ic_i
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Generating final prediction&lt;/strong&gt; The predicted label formula: &lt;span class=&quot;math display&quot;&gt;\[
\hat a=softmax(W(o+u))
\]&lt;/span&gt; &lt;img src=&quot;/assets/images/15.jpg&quot; alt=&quot;Figure 1&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;mutltiple-layers&quot;&gt;Mutltiple Layers&lt;/h2&gt;
&lt;p&gt;With &lt;span class=&quot;math inline&quot;&gt;\(K\)&lt;/span&gt; hop operations, the memory layers are stacked in the following way:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;The input tot layers above the first is the sum of the output &lt;span class=&quot;math inline&quot;&gt;\(o^k\)&lt;/span&gt; and the input &lt;span class=&quot;math inline&quot;&gt;\(u^k\)&lt;/span&gt; from layers &lt;span class=&quot;math inline&quot;&gt;\(k-1\)&lt;/span&gt;(different ways to combine &lt;span class=&quot;math inline&quot;&gt;\(o^k\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(u^k\)&lt;/span&gt; are proposed later): &lt;span class=&quot;math display&quot;&gt;\[
u^{k+1}=u^k+o^k
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each layer has its own embedding matrices &lt;span class=&quot;math inline&quot;&gt;\(A^k\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(C^k\)&lt;/span&gt;, used to embed the inputs &lt;span class=&quot;math inline&quot;&gt;\(\\{x_i\\}\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;At the top of network, the input &lt;span class=&quot;math inline&quot;&gt;\(W\)&lt;/span&gt; also combines the input and the output of the top memory layer:&lt;span class=&quot;math inline&quot;&gt;\(\hat a=softmax(Wu^{k+1})=softmax(W(o^K+u^K))\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Two types of weights tying:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Adjacent&lt;/strong&gt;: the output embedding for one layer is the input embedding for the one above, i.e. &lt;span class=&quot;math inline&quot;&gt;\(A^{k+1}=C^k\)&lt;/span&gt;. also constrain (a) the answer prediction matrix to be the same as the final output embedding, i.e. &lt;span class=&quot;math inline&quot;&gt;\(W^T=C^K\)&lt;/span&gt;, and (b) the question embedding to match the input embedding of the first layer, i.e. &lt;span class=&quot;math inline&quot;&gt;\(B=A^1\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Layer-wise (RNN-like)&lt;/strong&gt;: the input and output embeddings are the same across different layers, i.e. &lt;span class=&quot;math inline&quot;&gt;\(A^1=A^2=...=A^K\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(C^1=C^2=...=C^K\)&lt;/span&gt;. found it useful to add a linear mapping &lt;span class=&quot;math inline&quot;&gt;\(H\)&lt;/span&gt; to the update of &lt;span class=&quot;math inline&quot;&gt;\(u\)&lt;/span&gt; between hops; that is, &lt;span class=&quot;math inline&quot;&gt;\(u^{k+1}=Hu^k+o^k\)&lt;/span&gt;. This mapping is learned from data and used throughout experiments for layer-wise weight tying.&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Mon, 14 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/paper/2015/12/14/end-to-end-memory-networks.html</link>
				<guid isPermaLink="true">/foundation/paper/2015/12/14/end-to-end-memory-networks.html</guid>
			</item>
		
			<item>
				<title>Edit Distance</title>
				<description>&lt;p&gt;The minimum edit distance between two strings is the minimum number of editing operations &lt;code&gt;Insertion&lt;/code&gt; &lt;code&gt;Deletion&lt;/code&gt; &lt;code&gt;Substitution&lt;/code&gt; needed to transform one into other.&lt;/p&gt;
&lt;h2 id=&quot;defining-min-edit-distance&quot;&gt;Defining Min Edit Distance&lt;/h2&gt;
&lt;p&gt;For two Strings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; of length &lt;span class=&quot;math inline&quot;&gt;\(n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt; of length &lt;span class=&quot;math inline&quot;&gt;\(m\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We define &lt;span class=&quot;math inline&quot;&gt;\(D(i,j)\)&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the edit distance between &lt;span class=&quot;math inline&quot;&gt;\(X[1..i]\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(Y[1..j]\)&lt;/span&gt;, i.e. the first &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt; characters of &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; and the first &lt;span class=&quot;math inline&quot;&gt;\(j\)&lt;/span&gt; characters of &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;the edit distance between &lt;span class=&quot;math inline&quot;&gt;\(X\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(Y\)&lt;/span&gt; is thus &lt;span class=&quot;math inline&quot;&gt;\(D(n,m)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;computing-min-edit-distance-levenshtein&quot;&gt;Computing Min Edit Distance (Levenshtein)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Initialization &lt;span class=&quot;math inline&quot;&gt;\(D(i,0)=i\)&lt;/span&gt;, &lt;span class=&quot;math inline&quot;&gt;\(D(0,j)=j\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Recurrence Relation: for each i=i…M&lt;/p&gt;
&lt;p&gt;for each j=1…N &lt;span class=&quot;math display&quot;&gt;\[
D(i,j)=
\begin{cases}
D(i-1,j)+1 \\
D(i, j-1)+1 \\
D(i-1, j-1) + 
\begin{cases}
2 &amp;amp; \text{if $X(i) \neq Y(j)$}\\
0 &amp;amp;\text{if $X(i) = Y(j)$}
\end{cases} 
\end{cases}
\]&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Termination: &lt;span class=&quot;math inline&quot;&gt;\(D(N,M)\)&lt;/span&gt; is the distance. &lt;img src=&quot;/assets/images/17.jpg&quot; alt=&quot;edit distance table&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
				<pubDate>Mon, 14 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/nlp/2015/12/14/edit-distance.html</link>
				<guid isPermaLink="true">/foundation/nlp/2015/12/14/edit-distance.html</guid>
			</item>
		
			<item>
				<title>the equations of backpropagation</title>
				<description>&lt;p&gt;Typically, the last layer error vector: &lt;span class=&quot;math display&quot;&gt;\[
\delta ^L = \nabla C \odot \sigma &amp;#39;(z^L)
= a-y
\]&lt;/span&gt; The middle &lt;span class=&quot;math inline&quot;&gt;\(l\)&lt;/span&gt; hidden layer error vector: &lt;span class=&quot;math display&quot;&gt;\[
\delta ^l = (W^{l+1})^T\delta ^{l+1}\odot \sigma &amp;#39;(z^l)
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\frac{\partial C}{\partial b^l}=\delta ^l
\]&lt;/span&gt; &lt;span class=&quot;math display&quot;&gt;\[
\frac{\partial C}{\partial W^l}=\delta ^l (a^{l-1})^T
\]&lt;/span&gt; Here, &lt;span class=&quot;math inline&quot;&gt;\(W^l \in \Bbb R^{|l|\times|l-1|}\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(b^l\in \Bbb R^{|l|\times 1}\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(z^l\in \Bbb R^{|l|\times 1}\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(a^l\in \Bbb R^{|l|\times 1}\)&lt;/span&gt;,&lt;span class=&quot;math inline&quot;&gt;\(\delta^l\in \Bbb R^{|l|\times 1}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/13.jpg&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;/assets/images/14.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
				<pubDate>Mon, 07 Dec 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/12/07/the-equations-of-backpropagation.html</link>
				<guid isPermaLink="true">/foundation/2015/12/07/the-equations-of-backpropagation.html</guid>
			</item>
		
			<item>
				<title>Rectified Linear Unit (ReLU)</title>
				<description>&lt;figure&gt;
&lt;img src=&quot;/assets/images/10.jpg&quot; alt=&quot;ReLU&quot; /&gt;&lt;figcaption&gt;ReLU&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The Rectified Linear Unit (ReLU) computes the function &lt;span class=&quot;math inline&quot;&gt;\(f(x)=max(0,x)\)&lt;/span&gt;, which is simply thresholded at zero.&lt;/p&gt;
&lt;p&gt;There are several pros and cons to using the ReLUs:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;(Pros) Compared to sigmoid/tanh neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero. Meanwhile, ReLUs does not suffer from saturating.&lt;/li&gt;
&lt;li&gt;(Pros) It was found to greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.&lt;/li&gt;
&lt;li&gt;(Cons) Unfortunately, ReLU units can be fragile during training and can “die”. For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e., neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;leaky-relu&quot;&gt;Leaky ReLU&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/11.jpg&quot; alt=&quot;Leaky ReLU&quot; /&gt;&lt;figcaption&gt;Leaky ReLU&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when &lt;span class=&quot;math inline&quot;&gt;\(x&amp;lt;0\)&lt;/span&gt;, a leaky ReLU will instead have a small negative slope(of 0.01, or so). That is, the function computes &lt;span class=&quot;math inline&quot;&gt;\(f(x)=ax\)&lt;/span&gt; if &lt;span class=&quot;math inline&quot;&gt;\(x&amp;lt;0\)&lt;/span&gt; and &lt;span class=&quot;math inline&quot;&gt;\(f(x)=x\)&lt;/span&gt; if &lt;span class=&quot;math inline&quot;&gt;\(x\geqslant 0\)&lt;/span&gt;, where &lt;span class=&quot;math inline&quot;&gt;\(a\)&lt;/span&gt; is a small constant. Some people report success with this form of activation function, but the results are not always consistent.&lt;/p&gt;
&lt;h2 id=&quot;parametric-relu&quot;&gt;Parametric ReLU&lt;/h2&gt;
&lt;figure&gt;
&lt;img src=&quot;/assets/images/12.jpg&quot; alt=&quot;rectified unit family&quot; /&gt;&lt;figcaption&gt;rectified unit family&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;The first variant is called parametric rectified linear unit (PReLU). In PReLU, the slopes of negative part are learned from data rather than pre-defined.&lt;/p&gt;
&lt;h2 id=&quot;randomized-relu&quot;&gt;Randomized ReLU&lt;/h2&gt;
&lt;p&gt;In RReLU, the slopes of negative parts are randomized in a given range in the training, and then fixed in the testing. As mentioned in [B. Xu, N. Wang, T. Chen, and M. Li. Empirical Evaluation of Rectified Activations in Convolution Network. In ICML Deep Learning Workshop, 2015.], in a recent Kaggle National Data Science Bowl (NDSB) competition, it is reported that RReLU could reduce overfitting due to its randomized nature. Moreover, suggested by the NDSB competition winner, the random &lt;span class=&quot;math inline&quot;&gt;\(a_i\)&lt;/span&gt; in training is sampled from &lt;span class=&quot;math inline&quot;&gt;\(1/U(3,8)\)&lt;/span&gt; and in test time it is fixed as its expectation, i.e., &lt;span class=&quot;math inline&quot;&gt;\(2/(l+u)=2/11\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In conclusion, three types of ReLU variants all consistently outperform the original ReLU in these three data sets. And PReLU and RReLU seem better choices.&lt;/strong&gt;&lt;/p&gt;
</description>
				<pubDate>Wed, 18 Nov 2015 00:00:00 +0800</pubDate>
				<link>/foundation/2015/11/18/rectified-linear-unit-relu.html</link>
				<guid isPermaLink="true">/foundation/2015/11/18/rectified-linear-unit-relu.html</guid>
			</item>
		
	</channel>
</rss>
